{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MGT-python: Musical Gestures Toolbox","text":"<p>The Musical Gestures Toolbox for Python is a comprehensive collection of tools for visualization and analysis of audio and video, with a focus on motion capture and musical gesture analysis.</p> <p></p>"},{"location":"#what-is-mgt-python","title":"What is MGT-python?","text":"<p>MGT-python provides researchers, artists, and developers with powerful tools to:</p> <ul> <li>Analyze motion in video recordings</li> <li>Extract audio features from multimedia files  </li> <li>Generate visualizations like motiongrams, videograms, and motion history images</li> <li>Process and manipulate video content with computer vision techniques</li> <li>Integrate seamlessly with scientific Python ecosystem (NumPy, SciPy, Matplotlib)</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#video-analysis","title":"\ud83c\udfa5 Video Analysis","text":"<ul> <li>Motion detection and tracking</li> <li>Optical flow analysis</li> <li>Frame differencing and motion history</li> <li>Video preprocessing (cropping, filtering, rotation)</li> </ul>"},{"location":"#audio-processing","title":"\ud83c\udfb5 Audio Processing","text":"<ul> <li>Waveform analysis and visualization</li> <li>Spectrograms and chromagrams</li> <li>Tempo and beat tracking</li> <li>Audio feature extraction</li> </ul>"},{"location":"#visualization-tools","title":"\ud83d\udcca Visualization Tools","text":"<ul> <li>Motiongrams (motion over time)</li> <li>Videograms (pixel intensity over time)</li> <li>Average images and motion plots</li> <li>Interactive plotting with Matplotlib</li> </ul>"},{"location":"#utilities","title":"\ud83d\udd27 Utilities","text":"<ul> <li>Video format conversion</li> <li>Batch processing capabilities</li> <li>Integration with OpenPose for pose estimation</li> <li>Export functionality for further analysis</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install musicalgestures\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import musicalgestures as mg\n\n# Load a video\nmv = mg.MgVideo('dance.avi')\n\n# Generate motion analysis\nmotion = mv.motion()\n\n# Create visualizations\nmotiongram = mv.motiongrams()\naverage_image = mv.average()\n\n# Audio analysis\naudio = mg.MgAudio('music.wav')\nspectrogram = audio.spectrogram()\n</code></pre>"},{"location":"#try-it-online","title":"Try it Online","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide - Detailed setup instructions</li> <li>Quick Start Tutorial - Get up and running in minutes</li> <li>Examples - Sample code and use cases</li> <li>User Guide - Comprehensive documentation</li> </ul>"},{"location":"#academic-background","title":"Academic Background","text":"<p>This toolbox builds upon years of research in musical gesture analysis:</p> <ul> <li>Musical Gestures Toolbox for Max (Original)</li> <li>Musical Gestures Toolbox for Matlab (Previous version)</li> <li>MGT-python (Current version)</li> </ul>"},{"location":"#support-and-community","title":"Support and Community","text":"<ul> <li>Documentation: You're reading it! \ud83d\udcda</li> <li>Issues: GitHub Issues</li> <li>Source Code: GitHub Repository</li> <li>Research Group: fourMs Lab at RITMO</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use MGT-python in your research, please cite:</p> <pre><code>@software{mgt_python,\n  title={Musical Gestures Toolbox for Python},\n  author={University of Oslo fourMs Lab},\n  url={https://fourms.github.io/MGT-python/},\n  version={1.3.2},\n  year={2024}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MGT-python is released under the GNU General Public License v3 (GPLv3).</p> <p>Ready to explore musical gestures? Start with our Quick Start Guide or jump into the examples!</p>"},{"location":"MODULES/","title":"Mgt-python Modules","text":"<p>Auto-generated documentation modules index.</p> <p>Full list of Mgt-python project modules.</p> <ul> <li>MGT-python<ul> <li>360video</li> </ul> </li> <li>Musicalgestures<ul> <li>Audio</li> <li>Blend</li> <li>Blurfaces</li> <li>CenterFace</li> <li>Colored</li> <li>Cropping Window</li> <li>Cropvideo</li> <li>Directograms</li> <li>Filter</li> <li>Flow</li> <li>Frameaverage</li> <li>Grid</li> <li>History</li> <li>Impacts</li> <li>Info</li> <li>Input Test</li> <li>MgList</li> <li>Motionanalysis</li> <li>Motionvideo</li> <li>Motionvideo Mp Render</li> <li>Motionvideo Mp Run</li> <li>Pose</li> <li>Show</li> <li>Show Window</li> <li>Ssm</li> <li>Subtract</li> <li>Utils</li> <li>Video</li> <li>Videoadjust</li> <li>Videograms</li> <li>Videoreader</li> <li>Warp</li> </ul> </li> </ul>"},{"location":"contributing/","title":"Contributing to MGT-python","text":"<p>We welcome contributions to the Musical Gestures Toolbox for Python! This guide will help you get started with contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#fork-and-clone","title":"Fork and Clone","text":"<ol> <li> <p>Fork the repository on GitHub: https://github.com/fourMs/MGT-python</p> </li> <li> <p>Clone your fork locally:    <pre><code>git clone https://github.com/YOUR_USERNAME/MGT-python.git\ncd MGT-python\n</code></pre></p> </li> <li> <p>Add upstream remote:    <pre><code>git remote add upstream https://github.com/fourMs/MGT-python.git\n</code></pre></p> </li> </ol>"},{"location":"contributing/#development-environment","title":"Development Environment","text":""},{"location":"contributing/#set-up-virtual-environment","title":"Set up virtual environment","text":"<pre><code># Create virtual environment\npython -m venv mgt-dev\nsource mgt-dev/bin/activate  # Linux/macOS\n# mgt-dev\\Scripts\\activate   # Windows\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"contributing/#development-dependencies","title":"Development Dependencies","text":"<pre><code># Testing\npip install pytest pytest-cov\n\n# Code quality\npip install black flake8 isort mypy\n\n# Documentation\npip install mkdocs mkdocs-material mkdocstrings[python]\n\n# Optional: Jupyter for notebook development\npip install jupyter notebook\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<p>Always work on a feature branch, never directly on master:</p> <pre><code>git checkout master\ngit pull upstream master\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<p>Follow the coding standards and patterns established in the codebase:</p>"},{"location":"contributing/#code-style-guidelines","title":"Code Style Guidelines","text":"<ul> <li>PEP 8 compliance for Python code</li> <li>4 spaces for indentation (no tabs)</li> <li>Line length: Maximum 88 characters (Black default)</li> <li>Imports: Group and sort imports using isort</li> <li>Docstrings: Use Google-style docstrings</li> </ul>"},{"location":"contributing/#example-function-documentation","title":"Example Function Documentation","text":"<pre><code>def mg_example_function(video_path, param1=None, param2=False):\n    \"\"\"\n    Brief description of the function.\n\n    Longer description explaining what the function does,\n    how it works, and any important details.\n\n    Args:\n        video_path (str): Path to the input video file.\n        param1 (int, optional): Description of param1. Defaults to None.\n        param2 (bool, optional): Description of param2. Defaults to False.\n\n    Returns:\n        dict: Dictionary containing:\n            - 'output_path': Path to generated output file\n            - 'data': Processed data array\n\n    Raises:\n        FileNotFoundError: If video_path does not exist.\n        ValueError: If param1 is negative.\n\n    Example:\n        &gt;&gt;&gt; result = mg_example_function('video.mp4', param1=10)\n        &gt;&gt;&gt; print(result['output_path'])\n        'video_processed.mp4'\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"contributing/#3-code-quality-checks","title":"3. Code Quality Checks","text":"<p>Before committing, run these checks:</p>"},{"location":"contributing/#formatting","title":"Formatting","text":"<pre><code># Format code with Black\nblack musicalgestures/\n\n# Sort imports\nisort musicalgestures/\n\n# Check formatting\nblack --check musicalgestures/\nisort --check-only musicalgestures/\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<pre><code># Check code style\nflake8 musicalgestures/\n\n# Type checking (optional but recommended)\nmypy musicalgestures/\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=musicalgestures tests/\n\n# Run specific test file\npytest tests/test_video.py\n\n# Run specific test\npytest tests/test_video.py::test_video_loading\n</code></pre>"},{"location":"contributing/#4-add-tests","title":"4. Add Tests","text":"<p>All new functionality should include tests. Add tests to the appropriate file in the <code>tests/</code> directory:</p>"},{"location":"contributing/#test-structure","title":"Test Structure","text":"<pre><code>import pytest\nimport musicalgestures as mg\nfrom pathlib import Path\n\n\nclass TestNewFeature:\n    \"\"\"Test suite for new feature.\"\"\"\n\n    def test_basic_functionality(self):\n        \"\"\"Test basic functionality works as expected.\"\"\"\n        # Arrange\n        video_path = mg.examples.dance\n\n        # Act\n        result = mg.new_feature(video_path)\n\n        # Assert\n        assert result is not None\n        assert Path(result['output_path']).exists()\n\n    def test_error_handling(self):\n        \"\"\"Test that errors are handled appropriately.\"\"\"\n        with pytest.raises(FileNotFoundError):\n            mg.new_feature('nonexistent_file.mp4')\n\n    def test_parameter_validation(self):\n        \"\"\"Test parameter validation.\"\"\"\n        video_path = mg.examples.dance\n\n        # Test invalid parameter\n        with pytest.raises(ValueError):\n            mg.new_feature(video_path, invalid_param=-1)\n</code></pre>"},{"location":"contributing/#5-update-documentation","title":"5. Update Documentation","text":""},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Ensure all new functions and classes have proper docstrings following the Google style.</p>"},{"location":"contributing/#user-documentation","title":"User Documentation","text":"<p>If adding user-facing features, update the relevant documentation files:</p> <ul> <li><code>docs/user-guide/</code> - User guide sections</li> <li><code>docs/examples.md</code> - Add usage examples</li> <li><code>docs/api-reference/</code> - API documentation (auto-generated from docstrings)</li> </ul>"},{"location":"contributing/#changelog","title":"Changelog","text":"<p>Add entry to <code>CHANGELOG.md</code> (if it exists) or prepare release notes.</p>"},{"location":"contributing/#6-commit-your-changes","title":"6. Commit Your Changes","text":""},{"location":"contributing/#commit-message-format","title":"Commit Message Format","text":"<p>Use clear, descriptive commit messages:</p> <pre><code># Good commit messages\ngit commit -m \"Add optical flow visualization method\"\ngit commit -m \"Fix memory leak in video processing\"\ngit commit -m \"Update documentation for audio analysis\"\n\n# Include issue references when applicable\ngit commit -m \"Fix video loading issue (#123)\"\n</code></pre>"},{"location":"contributing/#commit-best-practices","title":"Commit Best Practices","text":"<ul> <li>Make atomic commits (one logical change per commit)</li> <li>Write descriptive commit messages</li> <li>Reference issues in commit messages when applicable</li> </ul>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#bug-fixes","title":"Bug Fixes","text":"<ol> <li>Check existing issues - Make sure the bug hasn't been reported</li> <li>Create an issue if one doesn't exist</li> <li>Fix the bug and add a test to prevent regression</li> <li>Reference the issue in your pull request</li> </ol>"},{"location":"contributing/#new-features","title":"New Features","text":"<ol> <li>Discuss the feature by creating an issue first</li> <li>Follow existing patterns in the codebase</li> <li>Add comprehensive tests</li> <li>Update documentation</li> <li>Consider backward compatibility</li> </ol>"},{"location":"contributing/#documentation-improvements","title":"Documentation Improvements","text":"<ul> <li>Fix typos and grammatical errors</li> <li>Improve existing documentation clarity</li> <li>Add missing documentation</li> <li>Create new examples and tutorials</li> <li>Improve code comments</li> </ul>"},{"location":"contributing/#examples-and-tutorials","title":"Examples and Tutorials","text":"<ul> <li>Create Jupyter notebooks demonstrating features</li> <li>Add real-world use cases</li> <li>Improve existing examples</li> <li>Add visualizations and plots</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-prepare-your-pull-request","title":"1. Prepare Your Pull Request","text":"<p>Before creating a PR:</p> <pre><code># Sync with upstream\ngit checkout master\ngit pull upstream master\n\n# Rebase your feature branch\ngit checkout feature/your-feature-name\ngit rebase master\n\n# Run all checks\nblack musicalgestures/\nisort musicalgestures/\nflake8 musicalgestures/\npytest\n</code></pre>"},{"location":"contributing/#2-create-pull-request","title":"2. Create Pull Request","text":"<ul> <li>Title: Clear, descriptive title</li> <li>Description: Explain what changes were made and why</li> <li>Testing: Describe how the changes were tested</li> <li>Documentation: Note any documentation updates</li> <li>Issues: Reference any related issues</li> </ul>"},{"location":"contributing/#pr-template-example","title":"PR Template Example","text":"<pre><code>## Description\nBrief description of changes made.\n\n## Changes Made\n- Added new feature X\n- Fixed bug Y  \n- Updated documentation Z\n\n## Testing\n- [ ] All existing tests pass\n- [ ] Added new tests for new functionality\n- [ ] Tested manually with sample videos\n\n## Documentation\n- [ ] Updated docstrings\n- [ ] Updated user guide\n- [ ] Added examples\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Self-review completed\n- [ ] Tests added/updated\n- [ ] Documentation updated\n</code></pre>"},{"location":"contributing/#3-review-process","title":"3. Review Process","text":"<ul> <li>Automated checks must pass (CI/CD)</li> <li>Code review by maintainers</li> <li>Address feedback promptly</li> <li>Update PR as needed</li> </ul>"},{"location":"contributing/#code-organization","title":"Code Organization","text":""},{"location":"contributing/#file-structure","title":"File Structure","text":"<pre><code>musicalgestures/\n\u251c\u2500\u2500 __init__.py              # Package initialization\n\u251c\u2500\u2500 _video.py               # Video processing (MgVideo class)\n\u251c\u2500\u2500 _audio.py               # Audio processing (MgAudio class)\n\u251c\u2500\u2500 _flow.py                # Optical flow analysis\n\u251c\u2500\u2500 _utils.py               # Utility functions\n\u251c\u2500\u2500 _motionvideo.py         # Motion video creation\n\u251c\u2500\u2500 _motiongrams.py         # Motiongram generation\n\u2514\u2500\u2500 ...                     # Other specialized modules\n</code></pre>"},{"location":"contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Files: Use underscore prefix for internal modules (<code>_video.py</code>)</li> <li>Classes: PascalCase (<code>MgVideo</code>, <code>MgAudio</code>)</li> <li>Functions: snake_case (<code>mg_motion_video</code>, <code>create_motiongram</code>)</li> <li>Variables: snake_case (<code>frame_count</code>, <code>output_path</code>)</li> <li>Constants: UPPER_CASE (<code>DEFAULT_THRESHOLD</code>, <code>MAX_FRAMES</code>)</li> </ul>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-categories","title":"Test Categories","text":"<ol> <li>Unit Tests: Test individual functions/methods</li> <li>Integration Tests: Test component interactions</li> <li>End-to-End Tests: Test complete workflows</li> <li>Performance Tests: Test with large files (optional)</li> </ol>"},{"location":"contributing/#test-data","title":"Test Data","text":"<p>Use the built-in example videos for consistency: <pre><code>import musicalgestures as mg\n\n# Use example videos in tests\nvideo_path = mg.examples.dance\naudio_path = mg.examples.pianist\n</code></pre></p>"},{"location":"contributing/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>For tests that don't require actual video processing: <pre><code>from unittest.mock import patch, MagicMock\n\n@patch('musicalgestures._video.cv2.VideoCapture')\ndef test_video_loading(mock_cv2):\n    # Mock OpenCV video capture\n    mock_cap = MagicMock()\n    mock_cv2.return_value = mock_cap\n\n    # Test the functionality\n    mv = mg.MgVideo('fake_video.mp4')\n    # Assert expected behavior\n</code></pre></p>"},{"location":"contributing/#release-process","title":"Release Process","text":""},{"location":"contributing/#version-numbering","title":"Version Numbering","text":"<p>MGT-python follows semantic versioning (SemVer): - Major (X.0.0): Breaking changes - Minor (0.X.0): New features, backward compatible - Patch (0.0.X): Bug fixes, backward compatible</p>"},{"location":"contributing/#release-checklist","title":"Release Checklist","text":"<ol> <li>Update version in <code>setup.py</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create release tag</li> <li>Build and upload to PyPI</li> <li>Update documentation</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":""},{"location":"contributing/#communication-channels","title":"Communication Channels","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: General questions and ideas</li> <li>Email: a.r.jensenius@imv.uio.no (maintainer)</li> </ul>"},{"location":"contributing/#resources","title":"Resources","text":"<ul> <li>fourMs Lab: https://github.com/fourMs</li> <li>RITMO Centre: https://www.uio.no/ritmo/english/</li> <li>Documentation: https://mgt-python.readthedocs.io/</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help others learn and grow</li> <li>Follow the project's coding standards</li> <li>Credit others for their contributions</li> </ul> <p>Thank you for contributing to MGT-python! \ud83c\udfb5\ud83c\udfa5</p>"},{"location":"examples/","title":"Examples and Tutorials","text":"<p>This page provides comprehensive examples showing how to use MGT-python for various musical gesture analysis tasks.</p>"},{"location":"examples/#basic-examples","title":"Basic Examples","text":""},{"location":"examples/#example-1-simple-motion-analysis","title":"Example 1: Simple Motion Analysis","text":"<pre><code>import musicalgestures as mg\n\n# Load the example dance video\nmv = mg.MgVideo(mg.examples.dance)\n\n# Perform motion analysis\nmotion_data = mv.motion()\n\nprint(f\"Motion video created: {motion_data['motion_video']}\")\nprint(f\"Motion data saved: {motion_data['motion_data']}\")\n\n# Load and examine the motion data\nimport pandas as pd\ndf = pd.read_csv(motion_data['motion_data'])\nprint(\"Motion statistics:\")\nprint(df.describe())\n</code></pre>"},{"location":"examples/#example-2-audio-visual-analysis","title":"Example 2: Audio-Visual Analysis","text":"<pre><code>import musicalgestures as mg\nimport matplotlib.pyplot as plt\n\n# Load pianist video\nmv = mg.MgVideo(mg.examples.pianist)\n\n# Create motion visualizations\nmotiongrams = mv.motiongrams()\naverage_img = mv.average()\n\n# Analyze audio\naudio = mv.audio\nwaveform = audio.waveform()\nspectrogram = audio.spectrogram()\ndescriptors = audio.descriptors()\n\nprint(\"Analysis complete! Files created:\")\nprint(f\"Motiongrams: {motiongrams}\")\nprint(f\"Average image: {average_img}\")\nprint(f\"Audio analysis: {descriptors}\")\n</code></pre>"},{"location":"examples/#advanced-examples","title":"Advanced Examples","text":""},{"location":"examples/#example-3-custom-video-preprocessing","title":"Example 3: Custom Video Preprocessing","text":"<pre><code>import musicalgestures as mg\n\n# Load video with extensive preprocessing\nmv = mg.MgVideo(\n    mg.examples.dance,\n    starttime=5.0,          # Start at 5 seconds\n    endtime=25.0,           # End at 25 seconds\n    color=False,            # Convert to grayscale\n    contrast=0.3,           # Increase contrast\n    brightness=0.1,         # Slightly brighten\n    filtertype='Regular',   # Motion filter type\n    thresh=0.05,           # Motion threshold\n    skip=2,                # Keep every 3rd frame\n    rotate=10              # Rotate 10 degrees\n)\n\n# Perform analysis on preprocessed video\nmotion = mv.motion()\nmotiongrams = mv.motiongrams()\n\nprint(f\"Preprocessed video analysis complete\")\nprint(f\"Original length: {mv.length:.2f} seconds\")\nprint(f\"Frame rate: {mv.fps} fps\")\n</code></pre>"},{"location":"examples/#example-4-batch-processing-multiple-videos","title":"Example 4: Batch Processing Multiple Videos","text":"<pre><code>import musicalgestures as mg\nimport os\nimport glob\n\ndef analyze_video_batch(video_pattern, output_dir):\n    \"\"\"\n    Analyze multiple videos matching a pattern\n    \"\"\"\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Find all matching videos\n    video_files = glob.glob(video_pattern)\n\n    results = []\n    for video_file in video_files:\n        print(f\"Processing: {video_file}\")\n\n        try:\n            # Load video\n            mv = mg.MgVideo(video_file, outdir=output_dir)\n\n            # Perform analysis\n            motion_data = mv.motion()\n            motiongrams = mv.motiongrams()\n\n            # Store results\n            results.append({\n                'video': video_file,\n                'motion_data': motion_data['motion_data'],\n                'motiongrams': motiongrams,\n                'success': True\n            })\n\n        except Exception as e:\n            print(f\"Error processing {video_file}: {e}\")\n            results.append({\n                'video': video_file,\n                'error': str(e),\n                'success': False\n            })\n\n    return results\n\n# Example usage\n# results = analyze_video_batch('videos/*.mp4', 'output/')\n</code></pre>"},{"location":"examples/#example-5-detailed-audio-analysis","title":"Example 5: Detailed Audio Analysis","text":"<pre><code>import musicalgestures as mg\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load video with audio\nmv = mg.MgVideo(mg.examples.pianist)\naudio = mv.audio\n\n# Multiple audio analyses\nwaveform_plot = audio.waveform(dpi=300)\nspectrogram_plot = audio.spectrogram(\n    window_size=4096,\n    overlap=0.9\n)\n\n# Audio descriptors with custom parameters\ndescriptors = audio.descriptors(\n    window_size=1024,\n    hop_size=512\n)\n\n# Tempo and beat analysis\ntempogram = audio.tempogram()\n\n# Self-similarity matrix\nssm = audio.ssm()\n\nprint(\"Audio analysis files:\")\nprint(f\"Waveform: {waveform_plot}\")\nprint(f\"Spectrogram: {spectrogram_plot}\")\nprint(f\"Descriptors: {descriptors}\")\nprint(f\"Tempogram: {tempogram}\")\nprint(f\"SSM: {ssm}\")\n</code></pre>"},{"location":"examples/#specialized-use-cases","title":"Specialized Use Cases","text":""},{"location":"examples/#example-6-pose-estimation-integration","title":"Example 6: Pose Estimation Integration","text":"<pre><code>import musicalgestures as mg\n\n# Load video for pose analysis\nmv = mg.MgVideo(mg.examples.dance)\n\n# Extract pose information (requires OpenPose)\ntry:\n    pose_data = mv.pose()\n    print(f\"Pose estimation complete: {pose_data}\")\nexcept Exception as e:\n    print(f\"Pose estimation requires OpenPose: {e}\")\n\n# Alternative: Use centroid tracking\ncentroid = mv.centroid()\nprint(f\"Motion centroid tracking: {centroid}\")\n</code></pre>"},{"location":"examples/#example-7-360-degree-video-analysis","title":"Example 7: 360-Degree Video Analysis","text":"<pre><code>import musicalgestures as mg\n\n# For 360-degree videos (experimental)\ntry:\n    mv360 = mg.Mg360Video('360_video.mp4')\n\n    # Specific 360 analysis methods\n    motion_360 = mv360.motion()\n\n    print(f\"360 video analysis: {motion_360}\")\nexcept:\n    print(\"360 video analysis requires specific video format\")\n</code></pre>"},{"location":"examples/#example-8-custom-visualization-parameters","title":"Example 8: Custom Visualization Parameters","text":"<pre><code>import musicalgestures as mg\n\nmv = mg.MgVideo(mg.examples.pianist)\n\n# Motiongrams with custom parameters\nmotiongrams = mv.motiongrams(\n    filtertype='Regular',\n    thresh=0.1,\n    blur='Medium',\n    use_median=True\n)\n\n# History video with custom settings\nhistory = mv.history(\n    history_length=60,  # 60 frame history\n    normalize=True\n)\n\n# Average image with different methods\naverage_standard = mv.average()\naverage_median = mv.average(method='median')\n\nprint(\"Custom visualizations created:\")\nprint(f\"Motiongrams: {motiongrams}\")\nprint(f\"History: {history}\")\nprint(f\"Averages: {average_standard}, {average_median}\")\n</code></pre>"},{"location":"examples/#research-examples","title":"Research Examples","text":""},{"location":"examples/#example-9-motion-feature-extraction","title":"Example 9: Motion Feature Extraction","text":"<pre><code>import musicalgestures as mg\nimport pandas as pd\nimport numpy as np\n\ndef extract_motion_features(video_path):\n    \"\"\"\n    Extract comprehensive motion features for research\n    \"\"\"\n    mv = mg.MgVideo(video_path)\n\n    # Basic motion analysis\n    motion_data = mv.motion()\n\n    # Load motion data\n    df = pd.read_csv(motion_data['motion_data'])\n\n    # Calculate additional features\n    features = {\n        'total_motion': df['Quantity of Motion'].sum(),\n        'avg_motion': df['Quantity of Motion'].mean(),\n        'motion_std': df['Quantity of Motion'].std(),\n        'peak_motion': df['Quantity of Motion'].max(),\n        'motion_range': df['Quantity of Motion'].max() - df['Quantity of Motion'].min(),\n\n        # Centroid features\n        'centroid_x_range': df['Centroid X'].max() - df['Centroid X'].min(),\n        'centroid_y_range': df['Centroid Y'].max() - df['Centroid Y'].min(),\n\n        # Area features\n        'avg_area': df['Area of Motion'].mean(),\n        'max_area': df['Area of Motion'].max(),\n    }\n\n    return features, motion_data\n\n# Example usage\nfeatures, data = extract_motion_features(mg.examples.dance)\nprint(\"Motion features:\")\nfor key, value in features.items():\n    print(f\"{key}: {value:.3f}\")\n</code></pre>"},{"location":"examples/#example-10-comparative-analysis","title":"Example 10: Comparative Analysis","text":"<pre><code>import musicalgestures as mg\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef compare_videos(video1, video2, label1=\"Video 1\", label2=\"Video 2\"):\n    \"\"\"\n    Compare motion characteristics between two videos\n    \"\"\"\n    # Analyze both videos\n    mv1 = mg.MgVideo(video1)\n    mv2 = mg.MgVideo(video2)\n\n    motion1 = mv1.motion()\n    motion2 = mv2.motion()\n\n    # Load motion data\n    df1 = pd.read_csv(motion1['motion_data'])\n    df2 = pd.read_csv(motion2['motion_data'])\n\n    # Create comparison plot\n    plt.figure(figsize=(12, 8))\n\n    # Motion quantity over time\n    plt.subplot(2, 2, 1)\n    plt.plot(df1['Frame'], df1['Quantity of Motion'], label=label1)\n    plt.plot(df2['Frame'], df2['Quantity of Motion'], label=label2)\n    plt.title('Quantity of Motion')\n    plt.xlabel('Frame')\n    plt.ylabel('Motion')\n    plt.legend()\n\n    # Centroid X\n    plt.subplot(2, 2, 2)\n    plt.plot(df1['Frame'], df1['Centroid X'], label=label1)\n    plt.plot(df2['Frame'], df2['Centroid X'], label=label2)\n    plt.title('Centroid X')\n    plt.xlabel('Frame')\n    plt.ylabel('X Position')\n    plt.legend()\n\n    # Centroid Y\n    plt.subplot(2, 2, 3)\n    plt.plot(df1['Frame'], df1['Centroid Y'], label=label1)\n    plt.plot(df2['Frame'], df2['Centroid Y'], label=label2)\n    plt.title('Centroid Y')\n    plt.xlabel('Frame')\n    plt.ylabel('Y Position')\n    plt.legend()\n\n    # Area of motion\n    plt.subplot(2, 2, 4)\n    plt.plot(df1['Frame'], df1['Area of Motion'], label=label1)\n    plt.plot(df2['Frame'], df2['Area of Motion'], label=label2)\n    plt.title('Area of Motion')\n    plt.xlabel('Frame')\n    plt.ylabel('Area')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.savefig('video_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    return df1, df2\n\n# Example: Compare dance and pianist videos\n# df_dance, df_pianist = compare_videos(\n#     mg.examples.dance, \n#     mg.examples.pianist,\n#     \"Dance\", \"Pianist\"\n# )\n</code></pre>"},{"location":"examples/#interactive-examples","title":"Interactive Examples","text":""},{"location":"examples/#example-11-jupyter-notebook-integration","title":"Example 11: Jupyter Notebook Integration","text":"<pre><code># Cell 1: Setup\nimport musicalgestures as mg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Load video\nmv = mg.MgVideo(mg.examples.pianist)\n\n# Cell 2: Quick motion analysis\nmotion = mv.motion()\nprint(f\"Motion analysis complete: {motion}\")\n\n# Cell 3: Display results\n# This will show the video player in Jupyter\nmv.show()\n\n# Cell 4: Create and display motiongrams\nmotiongrams = mv.motiongrams()\n\n# Load and display the motiongrams\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\n# Display horizontal motiongram\nimg_x = Image.open(motiongrams['mg_x'])\nax1.imshow(img_x)\nax1.set_title('Horizontal Motiongram')\nax1.axis('off')\n\n# Display vertical motiongram  \nimg_y = Image.open(motiongrams['mg_y'])\nax2.imshow(img_y)\nax2.set_title('Vertical Motiongram')\nax2.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"examples/#example-12-real-time-analysis-workflow","title":"Example 12: Real-time Analysis Workflow","text":"<pre><code>import musicalgestures as mg\nimport time\n\ndef analyze_workflow(video_path, verbose=True):\n    \"\"\"\n    Complete analysis workflow with timing\n    \"\"\"\n    start_time = time.time()\n\n    if verbose:\n        print(f\"Starting analysis of: {video_path}\")\n\n    # Load video\n    load_start = time.time()\n    mv = mg.MgVideo(video_path)\n    load_time = time.time() - load_start\n\n    if verbose:\n        print(f\"Video loaded in {load_time:.2f}s\")\n        print(f\"Video info: {mv.width}x{mv.height}, {mv.length:.2f}s, {mv.fps}fps\")\n\n    # Motion analysis\n    motion_start = time.time()\n    motion = mv.motion()\n    motion_time = time.time() - motion_start\n\n    if verbose:\n        print(f\"Motion analysis completed in {motion_time:.2f}s\")\n\n    # Visualizations\n    viz_start = time.time()\n    motiongrams = mv.motiongrams()\n    average = mv.average()\n    viz_time = time.time() - viz_start\n\n    if verbose:\n        print(f\"Visualizations created in {viz_time:.2f}s\")\n\n    # Audio analysis\n    audio_start = time.time()\n    spectrogram = mv.audio.spectrogram()\n    audio_time = time.time() - audio_start\n\n    if verbose:\n        print(f\"Audio analysis completed in {audio_time:.2f}s\")\n\n    total_time = time.time() - start_time\n\n    results = {\n        'motion_data': motion,\n        'visualizations': {\n            'motiongrams': motiongrams,\n            'average': average\n        },\n        'audio': {\n            'spectrogram': spectrogram\n        },\n        'timing': {\n            'load_time': load_time,\n            'motion_time': motion_time,\n            'viz_time': viz_time,\n            'audio_time': audio_time,\n            'total_time': total_time\n        }\n    }\n\n    if verbose:\n        print(f\"Total analysis time: {total_time:.2f}s\")\n\n    return results\n\n# Run complete workflow\nresults = analyze_workflow(mg.examples.dance)\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<p>These examples demonstrate the versatility of MGT-python. For more detailed information:</p> <ul> <li>User Guide - Comprehensive documentation</li> <li>API Reference - Complete function reference</li> <li>GitHub Repository - Source code and issues</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have a great use case for MGT-python? We'd love to include it! Please:</p> <ol> <li>Fork the repository</li> <li>Add your example to this page</li> <li>Submit a pull request</li> </ol> <p>Include a brief description, complete code, and expected output for the best examples.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#quick-installation","title":"Quick Installation","text":"<p>The easiest way to install MGT-python is via pip:</p> <pre><code>pip install musicalgestures\n</code></pre>"},{"location":"installation/#system-requirements","title":"System Requirements","text":""},{"location":"installation/#python-version","title":"Python Version","text":"<p>MGT-python requires Python 3.7 or higher. We recommend using the latest stable version of Python.</p> <pre><code>python --version  # Should be 3.7+\n</code></pre>"},{"location":"installation/#operating-systems","title":"Operating Systems","text":"<p>MGT-python is cross-platform and supports:</p> <ul> <li>Linux (Ubuntu, CentOS, etc.)</li> <li>macOS (10.14+)</li> <li>Windows (10+)</li> </ul>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>MGT-python automatically installs the following core dependencies:</p>"},{"location":"installation/#core-scientific-libraries","title":"Core Scientific Libraries","text":"<ul> <li><code>numpy</code> - Numerical computing</li> <li><code>pandas</code> - Data manipulation and analysis  </li> <li><code>scipy</code> - Scientific computing</li> <li><code>matplotlib</code> - Plotting and visualization</li> </ul>"},{"location":"installation/#computer-vision-media-processing","title":"Computer Vision &amp; Media Processing","text":"<ul> <li><code>opencv-python</code> - Computer vision algorithms</li> <li><code>scikit-image</code> - Image processing</li> <li><code>librosa</code> - Audio analysis</li> </ul>"},{"location":"installation/#interactive-computing","title":"Interactive Computing","text":"<ul> <li><code>ipython&gt;=7.12</code> - Enhanced Python shell</li> </ul>"},{"location":"installation/#external-dependencies","title":"External Dependencies","text":""},{"location":"installation/#ffmpeg-required","title":"FFmpeg (Required)","text":"<p>MGT-python relies on FFmpeg for video processing. Install it based on your operating system:</p>"},{"location":"installation/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code>sudo apt update\nsudo apt install ffmpeg\n</code></pre>"},{"location":"installation/#macos-with-homebrew","title":"macOS (with Homebrew)","text":"<pre><code>brew install ffmpeg\n</code></pre>"},{"location":"installation/#windows","title":"Windows","text":"<ol> <li>Download FFmpeg from https://ffmpeg.org/download.html</li> <li>Extract and add to your system PATH</li> <li>Or use Chocolatey: <code>choco install ffmpeg</code></li> </ol>"},{"location":"installation/#verify-ffmpeg-installation","title":"Verify FFmpeg Installation","text":"<pre><code>ffmpeg -version\n</code></pre>"},{"location":"installation/#opencv-usually-automatic","title":"OpenCV (Usually automatic)","text":"<p>OpenCV is typically installed automatically with <code>opencv-python</code>. If you encounter issues:</p>"},{"location":"installation/#linux-additional-packages","title":"Linux additional packages","text":"<pre><code>sudo apt install libgl1-mesa-glx libglib2.0-0\n</code></pre>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#1-standard-installation-recommended","title":"1. Standard Installation (Recommended)","text":"<pre><code>pip install musicalgestures\n</code></pre>"},{"location":"installation/#2-development-installation","title":"2. Development Installation","text":"<p>For contributing or using the latest features:</p> <pre><code># Clone the repository\ngit clone https://github.com/fourMs/MGT-python.git\ncd MGT-python\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"installation/#3-conda-installation","title":"3. Conda Installation","text":"<p>While not officially supported, you can use conda for dependency management:</p> <pre><code># Create a new environment\nconda create -n mgt python=3.9\nconda activate mgt\n\n# Install pip dependencies\npip install musicalgestures\n</code></pre>"},{"location":"installation/#virtual-environments-recommended","title":"Virtual Environments (Recommended)","text":"<p>Using virtual environments prevents dependency conflicts:</p>"},{"location":"installation/#using-venv","title":"Using venv","text":"<pre><code># Create virtual environment\npython -m venv mgt-env\n\n# Activate (Linux/macOS)\nsource mgt-env/bin/activate\n\n# Activate (Windows)\nmgt-env\\Scripts\\activate\n\n# Install MGT-python\npip install musicalgestures\n</code></pre>"},{"location":"installation/#using-conda","title":"Using conda","text":"<pre><code>conda create -n mgt python=3.9\nconda activate mgt\npip install musicalgestures\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import musicalgestures as mg\n\n# Check version\nprint(mg.__version__)\n\n# Load example data\nexamples = mg.examples\nprint(f\"Dance video: {examples.dance}\")\nprint(f\"Pianist video: {examples.pianist}\")\n\n# Basic functionality test\nmv = mg.MgVideo(examples.dance)\nprint(f\"Video loaded: {mv.filename}\")\nprint(f\"Duration: {mv.length:.2f} seconds\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":""},{"location":"installation/#1-ffmpeg-not-found","title":"1. FFmpeg not found","text":"<p><pre><code>Error: ffmpeg not found\n</code></pre> Solution: Install FFmpeg following the instructions above.</p>"},{"location":"installation/#2-opencv-import-errors","title":"2. OpenCV import errors","text":"<p><pre><code>ImportError: libGL.so.1: cannot open shared object file\n</code></pre> Solution (Linux): <pre><code>sudo apt install libgl1-mesa-glx\n</code></pre></p>"},{"location":"installation/#3-permission-errors-on-windows","title":"3. Permission errors on Windows","text":"<p><pre><code>PermissionError: [WinError 5] Access is denied\n</code></pre> Solution: Run terminal as Administrator or use <code>--user</code> flag: <pre><code>pip install --user musicalgestures\n</code></pre></p>"},{"location":"installation/#4-jupyter-notebook-integration","title":"4. Jupyter Notebook integration","text":"<p>If using in Jupyter notebooks, you might need: <pre><code>pip install jupyter ipywidgets\n</code></pre></p>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter installation issues:</p> <ol> <li>Check the GitHub Issues for known problems</li> <li>Create a new issue with:</li> <li>Your operating system and version</li> <li>Python version (<code>python --version</code>)</li> <li>Complete error message</li> <li>Installation method used</li> </ol>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Once installed successfully:</p> <ul> <li>Quick Start Guide - Your first steps with MGT-python</li> <li>Examples - Sample code and tutorials</li> <li>User Guide - Comprehensive documentation</li> </ul>"},{"location":"installation/#performance-optimization","title":"Performance Optimization","text":""},{"location":"installation/#for-large-video-files","title":"For Large Video Files","text":"<p>Consider installing additional optimized libraries:</p> <pre><code># For faster NumPy operations\npip install mkl\n\n# For GPU acceleration (if available)\npip install opencv-contrib-python\n</code></pre>"},{"location":"installation/#memory-management","title":"Memory Management","text":"<p>For processing large videos, ensure adequate RAM and consider: - Processing videos in chunks - Using lower resolution for initial analysis - Monitoring memory usage with <code>htop</code> or Task Manager</p>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>Welcome to MGT-python! This guide will get you up and running with the Musical Gestures Toolbox in just a few minutes.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have MGT-python installed. If not, see the Installation Guide.</p> <pre><code>pip install musicalgestures\n</code></pre>"},{"location":"quickstart/#your-first-mgt-python-script","title":"Your First MGT-python Script","text":"<p>Let's start with a simple example using the built-in sample videos:</p> <pre><code>import musicalgestures as mg\n\n# Access example videos\nexamples = mg.examples\nprint(f\"Dance video: {examples.dance}\")\nprint(f\"Pianist video: {examples.pianist}\")\n\n# Load a video\nmv = mg.MgVideo(examples.dance)\nprint(f\"Loaded: {mv.filename}\")\nprint(f\"Duration: {mv.length:.2f} seconds\")\nprint(f\"Frame rate: {mv.fps} fps\")\n</code></pre>"},{"location":"quickstart/#core-concepts","title":"Core Concepts","text":""},{"location":"quickstart/#mgvideo-class","title":"MgVideo Class","text":"<p>The <code>MgVideo</code> class is your main interface for video analysis:</p> <pre><code># Load your own video\nmv = mg.MgVideo('path/to/your/video.mp4')\n\n# Or use preprocessing options\nmv = mg.MgVideo(\n    'path/to/video.mp4',\n    starttime=10,      # Start at 10 seconds\n    endtime=30,        # End at 30 seconds  \n    color=False,       # Convert to grayscale\n    filtertype='Regular',  # Motion detection filter\n    thresh=0.1         # Motion threshold\n)\n</code></pre>"},{"location":"quickstart/#mgaudio-class","title":"MgAudio Class","text":"<p>For audio-only analysis:</p> <pre><code># Load audio from video or audio file\nma = mg.MgAudio('path/to/audio.wav')\n\n# Or extract audio from video\nmv = mg.MgVideo('video.mp4')\nma = mv.audio  # Get the audio component\n</code></pre>"},{"location":"quickstart/#basic-analysis-workflows","title":"Basic Analysis Workflows","text":""},{"location":"quickstart/#1-motion-analysis","title":"1. Motion Analysis","text":"<p>Extract motion information from your video:</p> <pre><code>mv = mg.MgVideo(examples.dance)\n\n# Perform motion analysis\nmotion_data = mv.motion()\n\n# This creates several outputs:\nprint(f\"Motion video: {motion_data['motion_video']}\")\nprint(f\"Data file: {motion_data['motion_data']}\")\n\n# Access motion metrics\nimport pandas as pd\ndata = pd.read_csv(motion_data['motion_data'])\nprint(data.head())\n</code></pre>"},{"location":"quickstart/#2-create-visualizations","title":"2. Create Visualizations","text":"<p>Generate various visualizations:</p> <pre><code>mv = mg.MgVideo(examples.pianist)\n\n# Motiongrams (motion over time)\nmotiongrams = mv.motiongrams()\nprint(f\"Horizontal motiongram: {motiongrams['mg_x']}\")\nprint(f\"Vertical motiongram: {motiongrams['mg_y']}\")\n\n# Average image\naverage_img = mv.average()\nprint(f\"Average image saved: {average_img}\")\n\n# Motion history\nhistory = mv.history()\nprint(f\"Motion history: {history}\")\n</code></pre>"},{"location":"quickstart/#3-audio-analysis","title":"3. Audio Analysis","text":"<p>Analyze the audio component:</p> <pre><code>mv = mg.MgVideo(examples.pianist)\n\n# Get audio object\naudio = mv.audio\n\n# Create waveform plot\nwaveform = audio.waveform()\nprint(f\"Waveform plot: {waveform}\")\n\n# Generate spectrogram\nspectrogram = audio.spectrogram()\nprint(f\"Spectrogram: {spectrogram}\")\n\n# Extract audio descriptors\ndescriptors = audio.descriptors()\nprint(f\"Descriptors: {descriptors}\")\n</code></pre>"},{"location":"quickstart/#working-with-your-own-videos","title":"Working with Your Own Videos","text":""},{"location":"quickstart/#supported-formats","title":"Supported Formats","text":"<p>MGT-python works with most common video formats: - MP4, AVI, MOV, MKV - Audio: WAV, MP3, FLAC, etc.</p>"},{"location":"quickstart/#basic-processing-pipeline","title":"Basic Processing Pipeline","text":"<pre><code># 1. Load and preprocess\nmv = mg.MgVideo(\n    'my_video.mp4',\n    starttime=5,       # Skip first 5 seconds\n    endtime=60,        # Use only first minute\n    color=False        # Grayscale for motion analysis\n)\n\n# 2. Perform motion analysis\nmotion = mv.motion()\n\n# 3. Create visualizations\nmotiongrams = mv.motiongrams()\naverage = mv.average()\n\n# 4. Analyze audio\naudio_analysis = mv.audio.spectrogram()\n\nprint(\"Analysis complete!\")\nprint(f\"Motion data: {motion['motion_data']}\")\nprint(f\"Visualizations created in: {mv.outdir}\")\n</code></pre>"},{"location":"quickstart/#understanding-output-files","title":"Understanding Output Files","text":"<p>MGT-python creates several types of output files:</p>"},{"location":"quickstart/#video-files","title":"Video Files","text":"<ul> <li><code>*_motion.mp4</code> - Motion detection video</li> <li><code>*_history.mp4</code> - Motion history visualization</li> </ul>"},{"location":"quickstart/#image-files","title":"Image Files","text":"<ul> <li><code>*_average.png</code> - Average of all frames</li> <li><code>*_mgx.png</code> - Horizontal motiongram</li> <li><code>*_mgy.png</code> - Vertical motiongram</li> </ul>"},{"location":"quickstart/#data-files","title":"Data Files","text":"<ul> <li><code>*_motion.csv</code> - Numerical motion data</li> <li><code>*_audio_descriptors.csv</code> - Audio feature data</li> </ul>"},{"location":"quickstart/#working-directory","title":"Working Directory","text":"<p>By default, outputs are saved in the same directory as your input video. You can specify a different location:</p> <pre><code>mv = mg.MgVideo('video.mp4', outdir='/path/to/output/')\n</code></pre>"},{"location":"quickstart/#interactive-analysis","title":"Interactive Analysis","text":""},{"location":"quickstart/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>MGT-python works great in Jupyter notebooks:</p> <pre><code>import musicalgestures as mg\nimport matplotlib.pyplot as plt\n\n# Load video\nmv = mg.MgVideo(mg.examples.dance)\n\n# Create motion analysis\nmotion = mv.motion()\n\n# Display results inline\nplt.figure(figsize=(12, 4))\nmv.show()  # Shows the video player\n</code></pre>"},{"location":"quickstart/#batch-processing","title":"Batch Processing","text":"<p>Process multiple videos:</p> <pre><code>import glob\n\nvideo_files = glob.glob('videos/*.mp4')\n\nfor video_file in video_files:\n    print(f\"Processing: {video_file}\")\n    mv = mg.MgVideo(video_file)\n\n    # Perform analysis\n    motion = mv.motion()\n    motiongrams = mv.motiongrams()\n\n    print(f\"Completed: {video_file}\")\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you're familiar with the basics, explore more advanced features:</p> <ul> <li>Core Classes - Detailed class documentation</li> <li>Video Processing - Advanced video techniques</li> <li>Audio Analysis - Comprehensive audio features</li> <li>Examples - More complete examples and use cases</li> </ul>"},{"location":"quickstart/#common-issues","title":"Common Issues","text":""},{"location":"quickstart/#video-wont-load","title":"Video Won't Load","text":"<pre><code># Check if file exists and is readable\nimport os\nvideo_path = 'my_video.mp4'\nif os.path.exists(video_path):\n    print(f\"File found: {video_path}\")\nelse:\n    print(f\"File not found: {video_path}\")\n</code></pre>"},{"location":"quickstart/#ffmpeg-errors","title":"FFmpeg Errors","text":"<p>If you get FFmpeg-related errors, ensure FFmpeg is installed: <pre><code>ffmpeg -version\n</code></pre></p> <p>See the Installation Guide for help with FFmpeg setup.</p>"},{"location":"quickstart/#memory-issues-with-large-videos","title":"Memory Issues with Large Videos","text":"<p>For large videos, consider: <pre><code># Process shorter segments\nmv = mg.MgVideo('large_video.mp4', starttime=0, endtime=30)\n\n# Or reduce resolution during preprocessing\nmv = mg.MgVideo('large_video.mp4', scale=0.5)  # 50% size\n</code></pre></p> <p>Ready to dive deeper? Check out our comprehensive User Guide!</p>"},{"location":"releases/","title":"Release Notes","text":""},{"location":"releases/#version-132-current","title":"Version 1.3.2 (Current)","text":"<p>Current stable release of MGT-python.</p>"},{"location":"releases/#features","title":"Features","text":"<ul> <li>Complete video analysis toolkit</li> <li>Audio processing and visualization</li> <li>Motion detection and tracking</li> <li>Optical flow analysis</li> <li>Motiongram and videogram generation</li> <li>Self-similarity matrix analysis</li> <li>Pose estimation integration</li> <li>Comprehensive example videos and tutorials</li> </ul>"},{"location":"releases/#dependencies","title":"Dependencies","text":"<ul> <li>Python 3.7+</li> <li>NumPy, pandas, matplotlib</li> <li>OpenCV, scikit-image</li> <li>Librosa for audio analysis</li> <li>FFmpeg for video processing</li> </ul>"},{"location":"releases/#installation","title":"Installation","text":"<pre><code>pip install musicalgestures\n</code></pre>"},{"location":"releases/#previous-versions","title":"Previous Versions","text":""},{"location":"releases/#version-131","title":"Version 1.3.1","text":"<ul> <li>Bug fixes and stability improvements</li> <li>Enhanced documentation</li> <li>Performance optimizations</li> </ul>"},{"location":"releases/#version-130","title":"Version 1.3.0","text":"<ul> <li>Added 360-degree video support</li> <li>Improved audio analysis features</li> <li>New visualization options</li> <li>Extended API functionality</li> </ul>"},{"location":"releases/#version-12x","title":"Version 1.2.x","text":"<ul> <li>Initial stable release series</li> <li>Core video and audio processing</li> <li>Basic motion analysis</li> <li>Fundamental visualization tools</li> </ul>"},{"location":"releases/#future-releases","title":"Future Releases","text":""},{"location":"releases/#planned-features","title":"Planned Features","text":"<ul> <li>Real-time processing capabilities</li> <li>Enhanced pose estimation</li> <li>Machine learning integration</li> <li>Additional visualization options</li> <li>Performance improvements</li> </ul>"},{"location":"releases/#breaking-changes","title":"Breaking Changes","text":"<p>No breaking changes are currently planned. The project maintains backward compatibility whenever possible.</p>"},{"location":"releases/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"releases/#from-131-to-132","title":"From 1.3.1 to 1.3.2","text":"<p>No breaking changes. Simply update:</p> <pre><code>pip install --upgrade musicalgestures\n</code></pre>"},{"location":"releases/#from-12x-to-13x","title":"From 1.2.x to 1.3.x","text":"<p>Minor API changes may affect some advanced use cases. Check the documentation for updated examples.</p>"},{"location":"releases/#support","title":"Support","text":"<p>For issues and support:</p> <ul> <li>GitHub Issues</li> <li>Documentation</li> <li>Email: a.r.jensenius@imv.uio.no</li> </ul>"},{"location":"testing/","title":"Testing Guide","text":"<p>This guide covers how to run and write tests for MGT-python, ensuring code quality and reliability.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":""},{"location":"testing/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have the development dependencies installed:</p> <pre><code>pip install pytest pytest-cov\n</code></pre>"},{"location":"testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest tests/test_video.py\n\n# Run specific test method\npytest tests/test_video.py::TestMgVideo::test_video_loading\n\n# Run tests matching a pattern\npytest -k \"test_motion\"\n</code></pre>"},{"location":"testing/#test-coverage","title":"Test Coverage","text":"<pre><code># Run tests with coverage report\npytest --cov=musicalgestures\n\n# Generate HTML coverage report\npytest --cov=musicalgestures --cov-report=html\n\n# View detailed coverage\npytest --cov=musicalgestures --cov-report=term-missing\n</code></pre>"},{"location":"testing/#performance-testing","title":"Performance Testing","text":"<pre><code># Run tests with timing information\npytest --durations=10\n\n# Profile slow tests\npytest --profile\n</code></pre>"},{"location":"testing/#test-structure","title":"Test Structure","text":""},{"location":"testing/#current-test-files","title":"Current Test Files","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_audio.py          # Audio processing tests\n\u251c\u2500\u2500 test_average.py        # Average image tests  \n\u251c\u2500\u2500 test_centroid.py       # Centroid tracking tests\n\u251c\u2500\u2500 test_init.py           # Package initialization tests\n\u251c\u2500\u2500 test_motionvideo.py    # Motion analysis tests\n\u251c\u2500\u2500 test_ssm.py            # Self-similarity matrix tests\n\u251c\u2500\u2500 test_utils.py          # Utility function tests\n\u2514\u2500\u2500 test_videograms.py     # Videogram generation tests\n</code></pre>"},{"location":"testing/#test-organization","title":"Test Organization","text":"<p>Tests are organized by functionality and follow this structure:</p> <pre><code>import pytest\nimport musicalgestures as mg\nfrom pathlib import Path\n\n\n@pytest.fixture\ndef sample_video():\n    \"\"\"Fixture providing sample video path.\"\"\"\n    return mg.examples.dance\n\n\n@pytest.fixture  \ndef sample_audio():\n    \"\"\"Fixture providing sample audio path.\"\"\"\n    return mg.examples.pianist\n\n\nclass TestClassName:\n    \"\"\"Test suite for specific functionality.\"\"\"\n\n    def test_basic_functionality(self, sample_video):\n        \"\"\"Test description.\"\"\"\n        # Test implementation\n        pass\n\n    def test_error_conditions(self):\n        \"\"\"Test error handling.\"\"\" \n        # Test implementation\n        pass\n</code></pre>"},{"location":"testing/#writing-tests","title":"Writing Tests","text":""},{"location":"testing/#test-categories","title":"Test Categories","text":""},{"location":"testing/#1-unit-tests","title":"1. Unit Tests","text":"<p>Test individual functions and methods in isolation:</p> <pre><code>def test_utility_function():\n    \"\"\"Test a utility function.\"\"\"\n    from musicalgestures._utils import generate_outfilename\n\n    # Test basic functionality\n    result = generate_outfilename('input.mp4', 'output')\n    assert result.endswith('_output.mp4')\n\n    # Test with custom extension\n    result = generate_outfilename('input.avi', 'test', '.png')\n    assert result.endswith('_test.png')\n</code></pre>"},{"location":"testing/#2-integration-tests","title":"2. Integration Tests","text":"<p>Test interaction between components:</p> <pre><code>def test_video_audio_integration(sample_video):\n    \"\"\"Test video and audio integration.\"\"\"\n    mv = mg.MgVideo(sample_video)\n\n    # Test that audio component is accessible\n    assert mv.audio is not None\n    assert hasattr(mv.audio, 'waveform')\n\n    # Test audio analysis works\n    waveform = mv.audio.waveform()\n    assert Path(waveform).exists()\n</code></pre>"},{"location":"testing/#3-end-to-end-tests","title":"3. End-to-End Tests","text":"<p>Test complete workflows:</p> <pre><code>def test_complete_motion_analysis_workflow(sample_video, tmp_path):\n    \"\"\"Test complete motion analysis from start to finish.\"\"\"\n    # Load video with custom output directory\n    mv = mg.MgVideo(sample_video, outdir=str(tmp_path))\n\n    # Perform motion analysis\n    motion_result = mv.motion()\n\n    # Verify outputs exist\n    assert Path(motion_result['motion_video']).exists()\n    assert Path(motion_result['motion_data']).exists()\n\n    # Verify data content\n    import pandas as pd\n    df = pd.read_csv(motion_result['motion_data'])\n    assert len(df) &gt; 0\n    assert 'Quantity of Motion' in df.columns\n</code></pre>"},{"location":"testing/#test-fixtures","title":"Test Fixtures","text":"<p>Use fixtures for common test data and setup:</p> <pre><code>@pytest.fixture\ndef temp_output_dir(tmp_path):\n    \"\"\"Provide temporary output directory.\"\"\"\n    output_dir = tmp_path / \"test_output\"\n    output_dir.mkdir()\n    return str(output_dir)\n\n\n@pytest.fixture\ndef test_video_short(tmp_path):\n    \"\"\"Create a short test video for faster tests.\"\"\"\n    # Create minimal test video using OpenCV\n    import cv2\n    import numpy as np\n\n    video_path = tmp_path / \"test_short.avi\"\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    writer = cv2.VideoWriter(str(video_path), fourcc, 25.0, (640, 480))\n\n    # Create 25 frames (1 second at 25fps)\n    for i in range(25):\n        frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n        writer.write(frame)\n\n    writer.release()\n    return str(video_path)\n\n\n@pytest.fixture\ndef mock_video_properties():\n    \"\"\"Mock video properties for testing.\"\"\"\n    return {\n        'width': 640,\n        'height': 480,\n        'fps': 25.0,\n        'framecount': 100,\n        'length': 4.0\n    }\n</code></pre>"},{"location":"testing/#parameterized-tests","title":"Parameterized Tests","text":"<p>Test multiple scenarios efficiently:</p> <pre><code>@pytest.mark.parametrize(\"filtertype,expected\", [\n    ('Regular', True),\n    ('Binary', True), \n    ('Blob', True),\n    ('Invalid', False)\n])\ndef test_motion_filtertype(sample_video, filtertype, expected):\n    \"\"\"Test different motion filter types.\"\"\"\n    mv = mg.MgVideo(sample_video)\n\n    if expected:\n        # Should succeed\n        result = mv.motion(filtertype=filtertype)\n        assert Path(result['motion_video']).exists()\n    else:\n        # Should raise error\n        with pytest.raises(ValueError):\n            mv.motion(filtertype=filtertype)\n\n\n@pytest.mark.parametrize(\"starttime,endtime\", [\n    (0, 5),      # First 5 seconds\n    (2, 8),      # Middle section\n    (5, 0),      # From 5s to end\n])\ndef test_video_trimming(sample_video, starttime, endtime):\n    \"\"\"Test video trimming with different time ranges.\"\"\"\n    mv = mg.MgVideo(sample_video, starttime=starttime, endtime=endtime)\n\n    if endtime &gt; 0:\n        expected_length = endtime - starttime\n        assert abs(mv.length - expected_length) &lt; 0.5  # Allow small tolerance\n    else:\n        # endtime=0 means use full video from starttime\n        assert mv.length &gt; 0\n</code></pre>"},{"location":"testing/#mocking-external-dependencies","title":"Mocking External Dependencies","text":"<p>Mock external tools and heavy operations:</p> <pre><code>from unittest.mock import patch, MagicMock\nimport pytest\n\n\n@patch('musicalgestures._utils.ffmpeg_cmd')\ndef test_video_conversion_mock(mock_ffmpeg, sample_video):\n    \"\"\"Test video conversion without actually running FFmpeg.\"\"\"\n    mock_ffmpeg.return_value = True\n\n    from musicalgestures._utils import convert_to_mp4\n    result = convert_to_mp4(sample_video)\n\n    # Verify FFmpeg was called\n    mock_ffmpeg.assert_called_once()\n    assert result.endswith('.mp4')\n\n\n@patch('cv2.VideoCapture')\ndef test_video_loading_mock(mock_videocap):\n    \"\"\"Test video loading with mocked OpenCV.\"\"\"\n    # Setup mock\n    mock_cap = MagicMock()\n    mock_cap.isOpened.return_value = True\n    mock_cap.get.side_effect = lambda prop: {\n        cv2.CAP_PROP_FRAME_WIDTH: 640,\n        cv2.CAP_PROP_FRAME_HEIGHT: 480,\n        cv2.CAP_PROP_FPS: 25.0,\n        cv2.CAP_PROP_FRAME_COUNT: 100\n    }.get(prop, 0)\n\n    mock_videocap.return_value = mock_cap\n\n    # Test\n    mv = mg.MgVideo('fake_video.mp4')\n    assert mv.width == 640\n    assert mv.height == 480\n    assert mv.fps == 25.0\n</code></pre>"},{"location":"testing/#error-testing","title":"Error Testing","text":"<p>Test error conditions and edge cases:</p> <pre><code>def test_file_not_found():\n    \"\"\"Test handling of non-existent files.\"\"\"\n    with pytest.raises(FileNotFoundError):\n        mg.MgVideo('nonexistent_file.mp4')\n\n\ndef test_invalid_parameters(sample_video):\n    \"\"\"Test parameter validation.\"\"\"\n    # Invalid threshold\n    with pytest.raises(ValueError):\n        mv = mg.MgVideo(sample_video)\n        mv.motion(thresh=-0.5)  # Negative threshold\n\n    # Invalid time range\n    with pytest.raises(ValueError):\n        mg.MgVideo(sample_video, starttime=10, endtime=5)  # End before start\n\n\ndef test_corrupted_video_handling(tmp_path):\n    \"\"\"Test handling of corrupted video files.\"\"\"\n    # Create fake corrupted video file\n    corrupted_video = tmp_path / \"corrupted.mp4\"\n    corrupted_video.write_text(\"This is not a video file\")\n\n    with pytest.raises((ValueError, RuntimeError)):\n        mg.MgVideo(str(corrupted_video))\n</code></pre>"},{"location":"testing/#performance-tests","title":"Performance Tests","text":"<p>Test performance with larger datasets:</p> <pre><code>@pytest.mark.slow\ndef test_large_video_performance(tmp_path):\n    \"\"\"Test performance with larger video (marked as slow).\"\"\"\n    # This test is marked as 'slow' and can be skipped\n    # Run with: pytest -m \"not slow\" to skip slow tests\n    pass\n\n\n@pytest.mark.parametrize(\"video_size\", [\n    (320, 240),   # Small\n    (640, 480),   # Medium  \n    (1920, 1080), # Large\n])\ndef test_video_size_handling(video_size, tmp_path):\n    \"\"\"Test handling of different video sizes.\"\"\"\n    width, height = video_size\n    # Create test video of specified size\n    # Test processing\n    pass\n</code></pre>"},{"location":"testing/#test-configuration","title":"Test Configuration","text":""},{"location":"testing/#pytestini","title":"pytest.ini","text":"<p>Create a <code>pytest.ini</code> file in the project root:</p> <pre><code>[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    unit: marks tests as unit tests\n    requires_ffmpeg: marks tests that require FFmpeg\n    requires_opencv: marks tests that require OpenCV\naddopts = \n    --strict-markers\n    --disable-warnings\n    --tb=short\n</code></pre>"},{"location":"testing/#continuous-integration","title":"Continuous Integration","text":"<p>Example GitHub Actions workflow (<code>.github/workflows/test.yml</code>):</p> <pre><code>name: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        python-version: [3.7, 3.8, 3.9]\n\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v2\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -e .\n        pip install pytest pytest-cov\n\n    - name: Install FFmpeg\n      run: |\n        # OS-specific FFmpeg installation\n\n    - name: Run tests\n      run: |\n        pytest --cov=musicalgestures --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v1\n</code></pre>"},{"location":"testing/#test-data-management","title":"Test Data Management","text":""},{"location":"testing/#using-example-videos","title":"Using Example Videos","text":"<p>Always use the built-in examples for consistency:</p> <pre><code>def test_with_dance_video():\n    \"\"\"Test using the dance example video.\"\"\"\n    mv = mg.MgVideo(mg.examples.dance)\n    # Test implementation\n\n\ndef test_with_pianist_video():  \n    \"\"\"Test using the pianist example video.\"\"\"\n    mv = mg.MgVideo(mg.examples.pianist)\n    # Test implementation\n</code></pre>"},{"location":"testing/#creating-test-data","title":"Creating Test Data","text":"<p>For specific test scenarios, create minimal test data:</p> <pre><code>@pytest.fixture\ndef minimal_video(tmp_path):\n    \"\"\"Create minimal video for testing.\"\"\"\n    import cv2\n    import numpy as np\n\n    video_path = tmp_path / \"minimal.avi\"\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    writer = cv2.VideoWriter(str(video_path), fourcc, 10.0, (100, 100))\n\n    # Create 10 frames\n    for i in range(10):\n        frame = np.zeros((100, 100, 3), dtype=np.uint8)\n        frame[i*10:(i+1)*10, :] = 255  # Moving white bar\n        writer.write(frame)\n\n    writer.release()\n    return str(video_path)\n</code></pre>"},{"location":"testing/#running-specific-test-suites","title":"Running Specific Test Suites","text":""},{"location":"testing/#by-category","title":"By Category","text":"<pre><code># Run only unit tests\npytest -m unit\n\n# Run only integration tests  \npytest -m integration\n\n# Skip slow tests\npytest -m \"not slow\"\n\n# Run tests requiring specific dependencies\npytest -m requires_ffmpeg\n</code></pre>"},{"location":"testing/#by-component","title":"By Component","text":"<pre><code># Test video functionality\npytest tests/test_video.py tests/test_motionvideo.py\n\n# Test audio functionality\npytest tests/test_audio.py\n\n# Test utilities\npytest tests/test_utils.py\n</code></pre>"},{"location":"testing/#debugging-tests","title":"Debugging Tests","text":"<pre><code># Drop into debugger on failure\npytest --pdb\n\n# Show print statements\npytest -s\n\n# Show full traceback\npytest --tb=long\n\n# Run single test with debugging\npytest -s -vv tests/test_video.py::test_specific_function\n</code></pre>"},{"location":"testing/#test-best-practices","title":"Test Best Practices","text":""},{"location":"testing/#general-guidelines","title":"General Guidelines","text":"<ol> <li>Test one thing at a time - Each test should focus on a single behavior</li> <li>Use descriptive names - Test names should explain what is being tested</li> <li>Keep tests independent - Tests should not depend on each other</li> <li>Use fixtures for setup - Avoid duplication in test setup</li> <li>Test both success and failure cases - Include error condition testing</li> </ol>"},{"location":"testing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Use small test videos when possible</li> <li>Mock heavy operations when testing logic</li> <li>Mark slow tests appropriately</li> <li>Clean up temporary files</li> </ul>"},{"location":"testing/#coverage-goals","title":"Coverage Goals","text":"<ul> <li>Aim for &gt;90% code coverage</li> <li>Focus on critical paths first</li> <li>Test error conditions thoroughly</li> <li>Include integration tests for workflows</li> </ul>"},{"location":"testing/#troubleshooting-tests","title":"Troubleshooting Tests","text":""},{"location":"testing/#common-issues","title":"Common Issues","text":""},{"location":"testing/#ffmpeg-not-found","title":"FFmpeg Not Found","text":"<pre><code># Install FFmpeg for testing\nsudo apt install ffmpeg  # Ubuntu\nbrew install ffmpeg      # macOS\n</code></pre>"},{"location":"testing/#opencv-issues","title":"OpenCV Issues","text":"<pre><code># Install OpenCV dependencies\nsudo apt install libgl1-mesa-glx  # Ubuntu\n</code></pre>"},{"location":"testing/#permission-errors","title":"Permission Errors","text":"<pre><code># Run tests with proper permissions\nsudo pytest  # If needed (not recommended)\n\n# Or fix file permissions\nchmod +x test_files/*\n</code></pre>"},{"location":"testing/#test-environment","title":"Test Environment","text":"<p>Ensure consistent test environment:</p> <pre><code># Clean Python cache\nfind . -name \"*.pyc\" -delete\nfind . -name \"__pycache__\" -delete\n\n# Reset git state (if needed)\ngit clean -fd\n\n# Fresh virtual environment\nrm -rf venv/\npython -m venv venv\nsource venv/bin/activate\npip install -e .\n</code></pre> <p>Ready to contribute? Start by running the test suite and then adding tests for your new features!</p>"},{"location":"musicalgestures/","title":"Musicalgestures","text":"<p>Auto-generated documentation for musicalgestures module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures<ul> <li>Examples</li> <li>Modules<ul> <li>360video</li> <li>Audio</li> <li>Blend</li> <li>Blurfaces</li> <li>CenterFace</li> <li>Colored</li> <li>Cropping Window</li> <li>Cropvideo</li> <li>Directograms</li> <li>Filter</li> <li>Flow</li> <li>Frameaverage</li> <li>Grid</li> <li>History</li> <li>Impacts</li> <li>Info</li> <li>Input Test</li> <li>MgList</li> <li>Motionanalysis</li> <li>Motionvideo</li> <li>Motionvideo Mp Render</li> <li>Motionvideo Mp Run</li> <li>Pose</li> <li>Show</li> <li>Show Window</li> <li>Ssm</li> <li>Subtract</li> <li>Utils</li> <li>Video</li> <li>Videoadjust</li> <li>Videograms</li> <li>Videoreader</li> <li>Warp</li> </ul> </li> </ul> </li> </ul>"},{"location":"musicalgestures/#examples","title":"Examples","text":"<p>[find in source code]</p> <pre><code>class Examples():\n    def __init__():\n</code></pre>"},{"location":"musicalgestures/_360video/","title":"360video","text":"<p>Auto-generated documentation for musicalgestures._360video module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / 360video<ul> <li>Mg360Video<ul> <li>Mg360Video().convert_projection</li> </ul> </li> <li>Projection</li> </ul> </li> </ul>"},{"location":"musicalgestures/_360video/#mg360video","title":"Mg360Video","text":"<p>[find in source code]</p> <pre><code>class Mg360Video(MgVideo):\n    def __init__(\n        filename: str,\n        projection: str | Projection,\n        camera: str = None,\n        **kwargs,\n    ):\n</code></pre> <p>Class for 360 videos.</p>"},{"location":"musicalgestures/_360video/#see-also","title":"See also","text":"<ul> <li>MgVideo</li> </ul>"},{"location":"musicalgestures/_360video/#mg360videoconvert_projection","title":"Mg360Video().convert_projection","text":"<p>[find in source code]</p> <pre><code>def convert_projection(\n    target_projection: Projection | str,\n    options: dict[str, str] = None,\n    print_cmd: bool = False,\n):\n</code></pre> <p>Convert the video to a different projection.</p>"},{"location":"musicalgestures/_360video/#arguments","title":"Arguments","text":"<ul> <li><code>target_projection</code> Projection - Target projection. options (dict[str, str], optional): Options for the conversion. Defaults to None.</li> <li><code>print_cmd</code> bool, optional - Print the ffmpeg command. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_360video/#projection","title":"Projection","text":"<p>[find in source code]</p> <pre><code>class Projection(Enum):\n</code></pre> <p>same as https://ffmpeg.org/ffmpeg-filters.html#v360.</p>"},{"location":"musicalgestures/_audio/","title":"Audio","text":"<p>Auto-generated documentation for musicalgestures._audio module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Audio<ul> <li>MgAudio<ul> <li>MgAudio().descriptors</li> <li>MgAudio().format_time</li> <li>MgAudio().hpss</li> <li>MgAudio().numpy</li> <li>MgAudio().spectrogram</li> <li>MgAudio().tempogram</li> <li>MgAudio().waveform</li> </ul> </li> </ul> </li> </ul>"},{"location":"musicalgestures/_audio/#mgaudio","title":"MgAudio","text":"<p>[find in source code]</p> <pre><code>class MgAudio():\n    def __init__(filename, sr=None, n_fft=2048, hop_length=512):\n</code></pre> <p>Class container for audio analysis processes.</p>"},{"location":"musicalgestures/_audio/#mgaudiodescriptors","title":"MgAudio().descriptors","text":"<p>[find in source code]</p> <pre><code>def descriptors(\n    n_mels=128,\n    fmin=0.0,\n    fmax=None,\n    power=2,\n    dpi=300,\n    autoshow=True,\n    original_time=False,\n    title=None,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a figure of plots showing spectral/loudness descriptors, including RMS energy, spectral flatness, centroid, bandwidth, rolloff of the video/audio file.</p>"},{"location":"musicalgestures/_audio/#arguments","title":"Arguments","text":"<ul> <li><code>n_mels</code> int, optional - The number of mel filters to use for filtering the frequency domain. Affects the vertical resolution (sharpness) of the spectrogram. NB: Too high values with relatively small window sizes can result in artifacts (typically black lines) in the resulting image. Defaults to 128.</li> <li><code>fmin</code> float, optional - Lowest frequency (in Hz). Defaults to 0.0.</li> <li><code>fmax</code> float, optional - Highest frequency (in Hz). Defaults to None, use fmax = sr / 2.0</li> <li><code>power</code> float, optional - The steepness of the curve for the color mapping. Defaults to 2.</li> <li><code>dpi</code> int, optional - Image quality of the rendered figure in DPI. Defaults to 300.</li> <li><code>autoshow</code> bool, optional - Whether to show the resulting figure automatically. Defaults to True.</li> <li><code>original_time</code> bool, optional - Whether to plot original time or not. This parameter can be useful if the file has been shortened beforehand (e.g. skip). Defaults to False.</li> <li><code>title</code> str, optional - Optionally add title to the figure. Possible to set the filename as the title using the string 'filename'. Defaults to None.</li> <li><code>target_name</code> str, optional - The name of the output image. Defaults to None (which assumes that the input filename with the suffix \"_descriptors.png\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_audio/#returns","title":"Returns","text":"<ul> <li><code>MgFigure</code> - An MgFigure object referring to the internal figure and its data.</li> </ul>"},{"location":"musicalgestures/_audio/#mgaudioformat_time","title":"MgAudio().format_time","text":"<p>[find in source code]</p> <pre><code>def format_time(ax, original_time=True, original_duration=None):\n</code></pre> <p>Format time for audio plotting of video file. This is useful if one wants to plot the original time of the video when frames have been skipped beforehand.</p>"},{"location":"musicalgestures/_audio/#arguments_1","title":"Arguments","text":"<ul> <li><code>ax</code> str, optional - Axis of the figure.</li> <li><code>original_time</code> bool, optional - Whether to get the original time for audio plotting or not. Defaults to True.</li> <li><code>original_duration</code> bool, optional - Whether to add the original duration of the file to be formatted manually. Defaults to None.</li> </ul>"},{"location":"musicalgestures/_audio/#mgaudiohpss","title":"MgAudio().hpss","text":"<p>[find in source code]</p> <pre><code>def hpss(\n    dim=2,\n    n_mels=128,\n    fmin=0.0,\n    fmax=None,\n    kernel_size=31,\n    margin=(1.0, 5.0),\n    power=2.0,\n    top_db=80.0,\n    mask=False,\n    residual=False,\n    dpi=300,\n    autoshow=True,\n    original_time=False,\n    title=None,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a figure with a plots of harmonic and percussive components of the audio file.</p>"},{"location":"musicalgestures/_audio/#arguments_2","title":"Arguments","text":"<ul> <li><code>dim</code> str, optional - Whether to plot hpss in one (i.e. waveform) or two (i.e. spectrogram) dimensions. Defaults to 2.</li> <li><code>n_mels</code> int, optional - Number of Mel bands to generate. Defaults to 128.</li> <li><code>fmin</code> float, optional - Lowest frequency (in Hz). Defaults to 0.0.</li> <li><code>fmax</code> float, optional - Highest frequency (in Hz). Defaults to None, use fmax = sr / 2.0. kernel_size (int or tuple, optional): Kernel size(s) for the median filters. If tuple, the first value specifies the width of the harmonic filter, and the second value specifies the width of the percussive filter. Defaults to 31. margin (float or tuple, optional): Margin size(s) for the masks (as described in this paper). If tuple, the first value specifies the margin of the harmonic mask, and the second value specifies the margin of the percussive mask. Defaults to (1.0,5.0).</li> <li><code>power</code> float, optional - Exponent for the Wiener filter when constructing soft mask matrices. Defaults to 2.0.</li> <li><code>top_db</code> float, optional - threshold the output at top_db below the peak: max(20 * log10(S/ref)) - top_db. Defaults to 80.0.</li> <li><code>mask</code> bool, optional - Return the masking matrices instead of components. Defaults to False.</li> <li><code>residual</code> bool, optional - Whether to return residual components of the audio file or not. Defaults to False.</li> <li><code>dpi</code> int, optional - Image quality of the rendered figure in DPI. Defaults to 300.</li> <li><code>autoshow</code> bool, optional - Whether to show the resulting figure automatically. Defaults to True.</li> <li><code>original_time</code> bool, optional - Whether to plot original time or not. This parameter can be useful if the video file has been shortened beforehand (e.g. skip). Defaults to False.</li> <li><code>title</code> str, optional - Optionally add title to the figure. Possible to set the filename as the title using the string 'filename'. Defaults to None.</li> <li><code>target_name</code> str, optional - The name of the output image. Defaults to None (which assumes that the input filename with the suffix \"_tempogram.png\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_audio/#returns_1","title":"Returns","text":"<ul> <li><code>MgFigure</code> - An MgFigure object referring to the internal figure and its data.</li> </ul>"},{"location":"musicalgestures/_audio/#mgaudionumpy","title":"MgAudio().numpy","text":"<p>[find in source code]</p> <pre><code>def numpy():\n</code></pre> <p>Read the original file of the MgAudio object as a numpy array using librosa.</p>"},{"location":"musicalgestures/_audio/#mgaudiospectrogram","title":"MgAudio().spectrogram","text":"<p>[find in source code]</p> <pre><code>def spectrogram(\n    fmin=0.0,\n    fmax=None,\n    n_mels=128,\n    power=2.0,\n    top_db=80.0,\n    dpi=300,\n    autoshow=True,\n    raw=False,\n    original_time=False,\n    title=None,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a figure showing the mel-scaled spectrogram of the video/audio file.</p>"},{"location":"musicalgestures/_audio/#arguments_3","title":"Arguments","text":"<ul> <li><code>n_mels</code> int, optional - The number of filters to use for filtering the frequency domain. Affects the vertical resolution (sharpness) of the spectrogram. NB: Too high values with relatively small window sizes can result in artifacts (typically black lines) in the resulting image. Defaults to 128.</li> <li><code>fmin</code> float, optional - Lowest frequency (in Hz). Defaults to 0.0.</li> <li><code>fmax</code> float, optional - Highest frequency (in Hz). Defaults to None, use fmax = sr / 2.0.</li> <li><code>power</code> float, optional - The steepness of the curve for the color mapping. Defaults to 2.</li> <li><code>top_db</code> float, optional - threshold the output at top_db below the peak: max(20 * log10(S/ref)) - top_db. Defaults to 80.0.</li> <li><code>dpi</code> int, optional - Image quality of the rendered figure in DPI. Defaults to 300.</li> <li><code>autoshow</code> bool, optional - Whether to show the resulting figure automatically. Defaults to True.</li> <li><code>raw</code> bool, optional - Whether to show labels and ticks on the plot. Defaults to False.</li> <li><code>original_time</code> bool, optional - Whether to plot original time or not. This parameter can be useful if the video file has been shortened beforehand (e.g. skip). Defaults to False.</li> <li><code>title</code> str, optional - Optionally add title to the figure. Possible to set the filename as the title using the string 'filename'. Defaults to None.</li> <li><code>target_name</code> str, optional - The name of the output image. Defaults to None (which assumes that the input filename with the suffix \"_spectrogram.png\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_audio/#returns_2","title":"Returns","text":"<ul> <li><code>MgFigure</code> - An MgFigure object referring to the internal figure and its data.</li> </ul>"},{"location":"musicalgestures/_audio/#mgaudiotempogram","title":"MgAudio().tempogram","text":"<p>[find in source code]</p> <pre><code>def tempogram(\n    dpi=300,\n    autoshow=True,\n    raw=False,\n    original_time=False,\n    title=None,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a figure with a plots of onset strength and tempogram of the video/audio file.</p>"},{"location":"musicalgestures/_audio/#arguments_4","title":"Arguments","text":"<ul> <li><code>dpi</code> int, optional - Image quality of the rendered figure in DPI. Defaults to 300.</li> <li><code>autoshow</code> bool, optional - Whether to show the resulting figure automatically. Defaults to True.</li> <li><code>raw</code> bool, optional - Whether to show labels and ticks on the plot. Defaults to False.</li> <li><code>original_time</code> bool, optional - Whether to plot original time or not. This parameter can be useful if the video file has been shortened beforehand (e.g. skip). Defaults to False.</li> <li><code>title</code> str, optional - Optionally add title to the figure. Possible to set the filename as the title using the string 'filename'. Defaults to None.</li> <li><code>target_name</code> str, optional - The name of the output image. Defaults to None (which assumes that the input filename with the suffix \"_tempogram.png\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_audio/#returns_3","title":"Returns","text":"<ul> <li><code>MgFigure</code> - An MgFigure object referring to the internal figure and its data.</li> </ul>"},{"location":"musicalgestures/_audio/#mgaudiowaveform","title":"MgAudio().waveform","text":"<p>[find in source code]</p> <pre><code>def waveform(\n    dpi=300,\n    autoshow=True,\n    raw=False,\n    colored=False,\n    image_width=2500,\n    image_height=500,\n    fmin=500,\n    fmax=None,\n    cmap='freesound',\n    original_time=True,\n    title=None,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a figure showing the waveform of the video/audio file.</p>"},{"location":"musicalgestures/_audio/#arguments_5","title":"Arguments","text":"<ul> <li><code>dpi</code> int, optional - Image quality of the rendered figure in DPI. Defaults to 300.</li> <li><code>autoshow</code> bool, optional - Whether to show the resulting figure automatically. Defaults to True.</li> <li><code>raw</code> bool, optional - Whether to show labels and ticks on the plot. Defaults to False.</li> <li><code>colored</code> bool, optional - Whether to create a colored waveform image (freesound-style) from an audio input file. Defauts to False.</li> <li><code>image_width</code> int, optional - Number of pixels for the colored waveform image width. Defaults to 2500.</li> <li><code>image_height</code> int, optional - Number of pixels for the colored waveform image height. Defaults to 500.</li> <li><code>fmin</code> int, optional - Minimum frequency for computing spectral centroid for the colored waveform image. Defaults to 500.</li> <li><code>fmax</code> int, optional - Maximum frequency for computing spectral centroid for the colored waveform image. Defaults to None (i.e. Nyquist frequency).</li> <li><code>cmap</code> str, optional - Colormap used for coloring the waveform, all colormaps included with matplotlib can be used. Defaults to 'freesound'.</li> <li><code>original_time</code> bool, optional - Whether to plot original time or not. This parameter can be useful if the video file has been shortened beforehand (e.g. skip). Defaults to True.</li> <li><code>title</code> str, optional - Optionally add title to the figure. Possible to set the filename as the title using the string 'filename'. Defaults to None.</li> <li><code>target_name</code> str, optional - The name of the output image. Defaults to None (which assumes that the input filename with the suffix \"_waveform.png\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_audio/#returns_4","title":"Returns","text":"<ul> <li><code>MgFigure</code> - An MgFigure object referring to the internal figure and its data.</li> </ul>"},{"location":"musicalgestures/_blend/","title":"Blend","text":"<p>Auto-generated documentation for musicalgestures._blend module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Blend<ul> <li>mg_blend_image</li> </ul> </li> </ul>"},{"location":"musicalgestures/_blend/#mg_blend_image","title":"mg_blend_image","text":"<p>[find in source code]</p> <pre><code>def mg_blend_image(\n    self,\n    filename=None,\n    mode='all_mode',\n    component_mode='average',\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Finds and saves a blended image of an input video file using FFmpeg. The FFmpeg tblend (time blend) filter takes two consecutive frames from one single stream, and outputs the result obtained by blending the new frame on top of the old frame.</p>"},{"location":"musicalgestures/_blend/#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> str, optional - Path to the input video file. If None, the video file of the MgObject is used. Defaults to None.</li> <li><code>mode</code> str, optional - Set blend mode for specific pixel component or all pixel components. Accepted options are 'c0_mode', 'c1_mode', c2_mode', 'c3_mode' and 'all_mode'. Defaults to 'all_mode'.</li> <li><code>component_mode</code> str, optional - Component mode of the FFmpeg tblend. Available values for component modes can be accessed here: https://ffmpeg.org/ffmpeg-filters.html#blend-1. Defaults to 'average'.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename with the component mode suffix should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_blend/#returns","title":"Returns","text":"<ul> <li><code>MgImage</code> - A new MgImage pointing to the output image file.</li> </ul>"},{"location":"musicalgestures/_blurfaces/","title":"Blurfaces","text":"<p>Auto-generated documentation for musicalgestures._blurfaces module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Blurfaces<ul> <li>centroid_mask</li> <li>heatmap_data</li> <li>mg_blurfaces</li> <li>nearest_neighbours</li> <li>scaling_mask</li> </ul> </li> </ul>"},{"location":"musicalgestures/_blurfaces/#centroid_mask","title":"centroid_mask","text":"<p>[find in source code]</p> <pre><code>def centroid_mask(data):\n</code></pre>"},{"location":"musicalgestures/_blurfaces/#heatmap_data","title":"heatmap_data","text":"<p>[find in source code]</p> <pre><code>def heatmap_data(data, resolution, data_min, data_max):\n</code></pre>"},{"location":"musicalgestures/_blurfaces/#mg_blurfaces","title":"mg_blurfaces","text":"<p>[find in source code]</p> <pre><code>def mg_blurfaces(\n    self,\n    mask='blur',\n    mask_image=None,\n    mask_scale=1.0,\n    ellipse=True,\n    draw_heatmap=False,\n    neighbours=32,\n    resolution=250,\n    draw_scores=False,\n    save_data=True,\n    data_format='csv',\n    color=(0, 0, 0),\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Automatic anonymization of faces in videos. This function works by first detecting all human faces in each video frame and then applying an anonymization filter (blurring, black rectangles or images) on each detected face region.</p> <p>Credits: <code>centerface.onnx</code> (original) and <code>centerface.py</code> are based on https://github.com/Star-Clouds/centerface (revision 8c39a49), released under MIT license.</p>"},{"location":"musicalgestures/_blurfaces/#arguments","title":"Arguments","text":"<ul> <li><code>mask</code> str, optional - Mask filter mode for face regions. 'blur' applies a strong gaussian blurring, 'rectangle' draws a solid black box, 'image' replaces the face with a custom image and 'none' does leaves the input unchanged. Defaults to 'blur'.</li> <li><code>mask_image</code> str, optional - Anonymization image path which can be used for masking face regions. This can be activated by specifying 'image' in the mask parameter. Defaults to None.</li> <li><code>mask_scale</code> float, optional - Scale factor for face masks, to make sure that the masks cover the complete face. Defaults to 1.0.</li> <li><code>ellipse</code> bool, optional - Mask faces with blurred ellipses. Defaults to True.</li> <li><code>draw_heatmap</code> bool, optional - Draw heatmap of the detected faces using the centroid of the face mask. Defaults to False.</li> <li><code>neighbours</code> int, optional - Number of neighbours for smoothing the heatmap image. Defaults to 32.</li> <li><code>resolution</code> int, optional - Number of pixel resolution for the heatmap visualization. Defaults to 250.</li> <li><code>draw_scores</code> bool, optional - Draw detection faceness scores onto outputs (a score between 0 and 1 that roughly corresponds to the detector's confidence that something is a face). Defaults to False.</li> <li><code>save_data</code> bool, optional - Whether to save the scaled coordinates of the face mask (time (ms), x1, y1, x2, y2) for each frame to a file. Defaults to True.</li> <li><code>data_format</code> str, optional - Specifies format of blur_faces-data. Accepted values are 'csv', 'tsv' and 'txt'. For multiple output formats, use list, e.g. ['csv', 'txt']. Defaults to 'csv'.</li> <li><code>color</code> tuple, optional - Customized color of the rectangle boxes. Defaults to black (0, 0, 0).</li> <li><code>target_name</code> str, optional - Target output name. Defaults to None (which assumes that the input filename with the suffix \"_blurred\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_blurfaces/#returns","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A MgVideo as blur_faces for parent MgVideo</li> </ul>"},{"location":"musicalgestures/_blurfaces/#nearest_neighbours","title":"nearest_neighbours","text":"<p>[find in source code]</p> <pre><code>def nearest_neighbours(x, y, width, height, resolution, n_neighbours):\n</code></pre>"},{"location":"musicalgestures/_blurfaces/#scaling_mask","title":"scaling_mask","text":"<p>[find in source code]</p> <pre><code>def scaling_mask(x1, y1, x2, y2, mask_scale=1.0):\n</code></pre> <p>Scale factor for face masks, to make sure that the masks cover the complete face.</p>"},{"location":"musicalgestures/_blurfaces/#arguments_1","title":"Arguments","text":"<ul> <li><code>x1</code> int - X start coordinate value</li> <li><code>y1</code> int - Y start coordinate value</li> <li><code>x2</code> int - X end coordinate value</li> <li><code>y2</code> int - Y end coordinate value</li> <li><code>mask_scale</code> float, optional - Scale factor for adjusting the size of the face masks. Defaults to 1.0.</li> </ul>"},{"location":"musicalgestures/_blurfaces/#returns_1","title":"Returns","text":"<p>[x1, y1, x2, y2]: A list of intergers corresponding to the scaled coordinates of the face masks.</p>"},{"location":"musicalgestures/_centerface/","title":"CenterFace","text":"<p>Auto-generated documentation for musicalgestures._centerface module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / CenterFace<ul> <li>CenterFace<ul> <li>CenterFace().decode</li> <li>CenterFace().inference_opencv</li> <li>CenterFace().nms</li> <li>CenterFace().postprocess</li> <li>CenterFace().transform</li> </ul> </li> </ul> </li> </ul>"},{"location":"musicalgestures/_centerface/#centerface_1","title":"CenterFace","text":"<p>[find in source code]</p> <pre><code>class CenterFace(object):\n    def __init__(landmarks=True):\n</code></pre>"},{"location":"musicalgestures/_centerface/#centerfacedecode","title":"CenterFace().decode","text":"<p>[find in source code]</p> <pre><code>def decode(heatmap, scale, offset, landmark, size, threshold=0.1):\n</code></pre>"},{"location":"musicalgestures/_centerface/#centerfaceinference_opencv","title":"CenterFace().inference_opencv","text":"<p>[find in source code]</p> <pre><code>def inference_opencv(img, threshold):\n</code></pre>"},{"location":"musicalgestures/_centerface/#centerfacenms","title":"CenterFace().nms","text":"<p>[find in source code]</p> <pre><code>def nms(boxes, scores, nms_thresh):\n</code></pre>"},{"location":"musicalgestures/_centerface/#centerfacepostprocess","title":"CenterFace().postprocess","text":"<p>[find in source code]</p> <pre><code>def postprocess(heatmap, lms, offset, scale, threshold):\n</code></pre>"},{"location":"musicalgestures/_centerface/#centerfacetransform","title":"CenterFace().transform","text":"<p>[find in source code]</p> <pre><code>def transform(h, w):\n</code></pre>"},{"location":"musicalgestures/_centroid/","title":"Centroid","text":"<p>Auto-generated documentation for musicalgestures._centroid module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Centroid<ul> <li>centroid</li> </ul> </li> </ul>"},{"location":"musicalgestures/_centroid/#centroid_1","title":"centroid","text":"<p>[find in source code]</p> <pre><code>def centroid(image, width, height):\n</code></pre> <p>Computes the centroid of an image or frame.</p>"},{"location":"musicalgestures/_centroid/#arguments","title":"Arguments","text":"<ul> <li><code>image</code> np.array(uint8) - The input image matrix for the centroid estimation function.</li> <li><code>width</code> int - The pixel width of the input video capture.</li> <li><code>height</code> int - The pixel height of the input video capture.</li> </ul>"},{"location":"musicalgestures/_centroid/#returns","title":"Returns","text":"<ul> <li><code>np.array(2)</code> - X and Y coordinates of the centroid of motion.</li> <li><code>int</code> - Quantity of motion: How large the change was in pixels.</li> </ul>"},{"location":"musicalgestures/_colored/","title":"Colored","text":"<p>Auto-generated documentation for musicalgestures._colored module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Colored<ul> <li>MgAudioProcessor<ul> <li>MgAudioProcessor().peaks</li> <li>MgAudioProcessor().read_samples</li> <li>MgAudioProcessor().spectral_centroid</li> </ul> </li> <li>MgWaveformImage<ul> <li>MgWaveformImage().draw_peaks</li> <li>MgWaveformImage().interpolate_colors</li> </ul> </li> <li>min_max_level</li> </ul> </li> </ul>"},{"location":"musicalgestures/_colored/#mgaudioprocessor","title":"MgAudioProcessor","text":"<p>[find in source code]</p> <pre><code>class MgAudioProcessor(object):\n    def __init__(\n        filename,\n        n_fft,\n        fmin,\n        fmax=None,\n        window_function=np.hanning,\n    ):\n</code></pre>"},{"location":"musicalgestures/_colored/#mgaudioprocessorpeaks","title":"MgAudioProcessor().peaks","text":"<p>[find in source code]</p> <pre><code>def peaks(start_seek, end_seek, block_size=1024):\n</code></pre> <p>Read all samples between start_seek and end_seek, then find the minimum and maximum peak in that range. Returns that pair in the order they were found. So if min was found first, it returns (min, max) else the other way around.</p>"},{"location":"musicalgestures/_colored/#mgaudioprocessorread_samples","title":"MgAudioProcessor().read_samples","text":"<p>[find in source code]</p> <pre><code>def read_samples(start, size, resize_if_less=False):\n</code></pre> <p>Read size samples starting at start, if resize_if_less is True and less than size samples are read, resize the array to size and fill with zeros</p>"},{"location":"musicalgestures/_colored/#mgaudioprocessorspectral_centroid","title":"MgAudioProcessor().spectral_centroid","text":"<p>[find in source code]</p> <pre><code>def spectral_centroid(seek_point):\n</code></pre> <p>Starting at seek_point to read n_fft samples and calculate the spectral centroid</p>"},{"location":"musicalgestures/_colored/#mgwaveformimage","title":"MgWaveformImage","text":"<p>[find in source code]</p> <pre><code>class MgWaveformImage(object):\n    def __init__(image_width=2500, image_height=500, cmap='freesound'):\n</code></pre>"},{"location":"musicalgestures/_colored/#mgwaveformimagedraw_peaks","title":"MgWaveformImage().draw_peaks","text":"<p>[find in source code]</p> <pre><code>def draw_peaks(x, peaks, spectral_centroid):\n</code></pre> <p>Draw 2 peaks at x using the spectral_centroid for color</p>"},{"location":"musicalgestures/_colored/#mgwaveformimageinterpolate_colors","title":"MgWaveformImage().interpolate_colors","text":"<p>[find in source code]</p> <pre><code>def interpolate_colors(colors, flat=False, num_colors=256):\n</code></pre> <p>Given a list of colors, create a larger list of colors linearly interpolating the first one. If flatten is True a list of numbers will be returned. If False, a list of (r,g,b) tuples. num_colors is the number of colors wanted in the final list</p>"},{"location":"musicalgestures/_colored/#min_max_level","title":"min_max_level","text":"<p>[find in source code]</p> <pre><code>def min_max_level(filename):\n</code></pre>"},{"location":"musicalgestures/_cropping_window/","title":"Cropping Window","text":"<p>Auto-generated documentation for musicalgestures._cropping_window module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Cropping Window<ul> <li>draw_rectangle</li> </ul> </li> </ul>"},{"location":"musicalgestures/_cropping_window/#draw_rectangle","title":"draw_rectangle","text":"<p>[find in source code]</p> <pre><code>def draw_rectangle(event, x, y, flags, param):\n</code></pre> <p>Helper function to render a cropping window to the user in case of manual cropping, using cv2.</p>"},{"location":"musicalgestures/_cropvideo/","title":"Cropvideo","text":"<p>Auto-generated documentation for musicalgestures._cropvideo module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Cropvideo<ul> <li>async_subprocess</li> <li>cropping_window</li> <li>find_motion_box_ffmpeg</li> <li>manual_text_input</li> <li>mg_cropvideo_ffmpeg</li> <li>run_cropping_window</li> </ul> </li> </ul>"},{"location":"musicalgestures/_cropvideo/#async_subprocess","title":"async_subprocess","text":"<p>[find in source code]</p> <pre><code>async def async_subprocess(command):\n</code></pre>"},{"location":"musicalgestures/_cropvideo/#cropping_window","title":"cropping_window","text":"<p>[find in source code]</p> <pre><code>def cropping_window(filename):\n</code></pre>"},{"location":"musicalgestures/_cropvideo/#find_motion_box_ffmpeg","title":"find_motion_box_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def find_motion_box_ffmpeg(\n    filename,\n    motion_box_thresh=0.1,\n    motion_box_margin=12,\n):\n</code></pre> <p>Helper function to find the area of motion in a video, using ffmpeg.</p>"},{"location":"musicalgestures/_cropvideo/#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file.</li> <li><code>motion_box_thresh</code> float, optional - Pixel threshold to apply to the video before assessing the area of motion. Defaults to 0.1.</li> <li><code>motion_box_margin</code> int, optional - Margin (in pixels) to add to the detected motion box. Defaults to 12.</li> </ul>"},{"location":"musicalgestures/_cropvideo/#raises","title":"Raises","text":"<ul> <li><code>KeyboardInterrupt</code> - In case we stop the process manually.</li> </ul>"},{"location":"musicalgestures/_cropvideo/#returns","title":"Returns","text":"<ul> <li><code>int</code> - The width of the motion box.</li> <li><code>int</code> - The height of the motion box.</li> <li><code>int</code> - The X coordinate of the top left corner of the motion box.</li> <li><code>int</code> - The Y coordinate of the top left corner of the motion box.</li> </ul>"},{"location":"musicalgestures/_cropvideo/#manual_text_input","title":"manual_text_input","text":"<p>[find in source code]</p> <pre><code>def manual_text_input():\n</code></pre> <p>Helper function for mg_crop_video_ffmpeg when its crop_movement is 'manual', but the environment is in Colab. In this case we can't display the windowed cropping UI, so we ask for the values as a text input.</p>"},{"location":"musicalgestures/_cropvideo/#returns_1","title":"Returns","text":"<ul> <li><code>list</code> - x, y, w, h for crop_ffmpeg.</li> </ul>"},{"location":"musicalgestures/_cropvideo/#mg_cropvideo_ffmpeg","title":"mg_cropvideo_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def mg_cropvideo_ffmpeg(\n    filename,\n    crop_movement='Auto',\n    motion_box_thresh=0.1,\n    motion_box_margin=12,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Crops the video using ffmpeg.</p>"},{"location":"musicalgestures/_cropvideo/#arguments_1","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file.</li> <li><code>crop_movement</code> str, optional - 'Auto' finds the bounding box that contains the total motion in the video. Motion threshold is given by motion_box_thresh. 'Manual' opens up a simple GUI that is used to crop the video manually by looking at the first frame. Defaults to 'Auto'.</li> <li><code>motion_box_thresh</code> float, optional - Only meaningful if <code>crop_movement='Auto'</code>. Takes floats between 0 and 1, where 0 includes all the motion and 1 includes none. Defaults to 0.1.</li> <li><code>motion_box_margin</code> int, optional - Only meaningful if <code>crop_movement='Auto'</code>. Adds margin to the bounding box. Defaults to 12.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename with the suffix \"_crop\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_cropvideo/#returns_2","title":"Returns","text":"<ul> <li><code>str</code> - Path to the cropped video.</li> </ul>"},{"location":"musicalgestures/_cropvideo/#run_cropping_window","title":"run_cropping_window","text":"<p>[find in source code]</p> <pre><code>def run_cropping_window(imgpath, scale_ratio, scaled_width, scaled_height):\n</code></pre>"},{"location":"musicalgestures/_directograms/","title":"Directograms","text":"<p>Auto-generated documentation for musicalgestures._directograms module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Directograms<ul> <li>directogram</li> <li>matrix3D_norm</li> <li>mg_directograms</li> </ul> </li> </ul>"},{"location":"musicalgestures/_directograms/#directogram","title":"directogram","text":"<p>[find in source code]</p> <pre><code>@jit(nopython=True)\ndef directogram(optical_flow):\n</code></pre>"},{"location":"musicalgestures/_directograms/#matrix3d_norm","title":"matrix3D_norm","text":"<p>[find in source code]</p> <pre><code>@jit(nopython=True)\ndef matrix3D_norm(matrix):\n</code></pre>"},{"location":"musicalgestures/_directograms/#mg_directograms","title":"mg_directograms","text":"<p>[find in source code]</p> <pre><code>def mg_directograms(\n    self,\n    title=None,\n    filtertype='Adaptative',\n    thresh=0.05,\n    kernel_size=5,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Compute a directogram to factor the magnitude of motion into different angles. Each columun of the directogram is computed as the weighted histogram (HISTOGRAM_BINS) of angles for the optical flow of an input frame.</p> <p>Source: Abe Davis -- Visual Rhythm and Beat (section 4.1)</p>"},{"location":"musicalgestures/_directograms/#arguments","title":"Arguments","text":"<ul> <li><code>title</code> str, optional - Optionally add title to the figure. Defaults to None, which uses 'Directogram' as a title. Defaults to None.</li> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. 'Adaptative' perform adaptative threshold as the weighted sum of 11 neighborhood pixels where weights are a Gaussian window. Defaults to 'Adaptative'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>kernel_size</code> int, optional - Size of structuring element. Defaults to 5.</li> <li><code>target_name</code> str, optional - Target output name for the directogram. Defaults to None (which assumes that the input filename with the suffix \"_dg\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_directograms/#returns","title":"Returns","text":"<ul> <li><code>MgFigure</code> - A MgFigure object referring to the internal figure and its data.</li> </ul>"},{"location":"musicalgestures/_filter/","title":"Filter","text":"<p>Auto-generated documentation for musicalgestures._filter module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Filter<ul> <li>filter_frame</li> <li>filter_frame_ffmpeg</li> </ul> </li> </ul>"},{"location":"musicalgestures/_filter/#filter_frame","title":"filter_frame","text":"<p>[find in source code]</p> <pre><code>def filter_frame(motion_frame, filtertype, thresh, kernel_size):\n</code></pre> <p>Applies a threshold filter and then a median filter (of <code>kernel_size</code>x<code>kernel_size</code>) to an image or videoframe.</p>"},{"location":"musicalgestures/_filter/#arguments","title":"Arguments","text":"<ul> <li><code>motion_frame</code> np.array(uint8) - Input motion image.</li> <li><code>filtertype</code> str - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method.</li> <li><code>thresh</code> float - A number in the range of 0 to 1. Eliminates pixel values less than given threshold.</li> <li><code>kernel_size</code> int - Size of structuring element.</li> </ul>"},{"location":"musicalgestures/_filter/#returns","title":"Returns","text":"<ul> <li><code>np.array(uint8)</code> - The filtered frame.</li> </ul>"},{"location":"musicalgestures/_filter/#filter_frame_ffmpeg","title":"filter_frame_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def filter_frame_ffmpeg(\n    filename,\n    cmd,\n    color,\n    blur,\n    filtertype,\n    threshold,\n    kernel_size,\n    use_median,\n    invert=False,\n):\n</code></pre>"},{"location":"musicalgestures/_flow/","title":"Flow","text":"<p>Auto-generated documentation for musicalgestures._flow module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Flow<ul> <li>Flow<ul> <li>Flow().dense</li> <li>Flow().get_acceleration</li> <li>Flow().get_velocity</li> <li>Flow().sparse</li> <li>Flow().velocity_meters_per_second</li> </ul> </li> </ul> </li> </ul>"},{"location":"musicalgestures/_flow/#flow_1","title":"Flow","text":"<p>[find in source code]</p> <pre><code>class Flow():\n    def __init__(parent, filename, color, has_audio):\n</code></pre> <p>Class container for the sparse and dense optical flow processes.</p>"},{"location":"musicalgestures/_flow/#flowdense","title":"Flow().dense","text":"<p>[find in source code]</p> <pre><code>def dense(\n    filename=None,\n    pyr_scale=0.5,\n    levels=3,\n    winsize=15,\n    iterations=3,\n    poly_n=5,\n    poly_sigma=1.2,\n    flags=0,\n    velocity=False,\n    distance=None,\n    timestep=1,\n    move_step=1,\n    angle_of_view=0,\n    scaledown=1,\n    skip_empty=False,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a dense optical flow video of the input video file using <code>cv2.calcOpticalFlowFarneback()</code>. The description of the matching parameters are taken from the cv2 documentation.</p>"},{"location":"musicalgestures/_flow/#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> str, optional - Path to the input video file. If None the video file of the MgVideo is used. Defaults to None.</li> <li><code>pyr_scale</code> float, optional - Specifies the image scale (&lt;1) to build pyramids for each image. <code>pyr_scale=0.5</code> means a classical pyramid, where each next layer is twice smaller than the previous one. Defaults to 0.5.</li> <li><code>levels</code> int, optional - The number of pyramid layers including the initial image. <code>levels=1</code> means that no extra layers are created and only the original images are used. Defaults to 3.</li> <li><code>winsize</code> int, optional - The averaging window size. Larger values increase the algorithm robustness to image noise and give more chances for fast motion detection, but yield more blurred motion field. Defaults to 15.</li> <li><code>iterations</code> int, optional - The number of iterations the algorithm does at each pyramid level. Defaults to 3.</li> <li><code>poly_n</code> int, optional - The size of the pixel neighborhood used to find polynomial expansion in each pixel. Larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7. Defaults to 5.</li> <li><code>poly_sigma</code> float, optional - The standard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion. For <code>poly_n=5</code>, you can set <code>poly_sigma=1.1</code>, for <code>poly_n=7</code>, a good value would be <code>poly_sigma=1.5</code>. Defaults to 1.2.</li> <li><code>flags</code> int, optional - Operation flags that can be a combination of the following: - OPTFLOW_USE_INITIAL_FLOW uses the input flow as an initial flow approximation. - OPTFLOW_FARNEBACK_GAUSSIAN uses the Gaussian \\f$\\texttt{winsize}\\times\\texttt{winsize}\\f$ filter instead of a box filter of the same size for optical flow estimation. Usually, this option gives z more accurate flow than with a box filter, at the cost of lower speed. Normally, <code>winsize</code> for a Gaussian window should be set to a larger value to achieve the same level of robustness. Defaults to 0.</li> <li><code>velocity</code> bool, optional - Whether to compute optical flow velocity or not. Defaults to False.</li> <li><code>distance</code> int, optional - Distance in meters to image (focal length) for returning flow in meters per second. Defaults to None.</li> <li><code>timestep</code> int, optional - Time step in seconds for returning flow in meters per second. Defaults to 1.</li> <li><code>move_step</code> int, optional - step size in pixels for sampling the flow image. Defaults to 1.</li> <li><code>angle_of_view</code> int, optional - angle of view of camera, for reporting flow in meters per second. Defaults to 0.</li> <li><code>scaledown</code> int, optional - factor to scaledown frame size of the video. Defaults to 1.</li> <li><code>skip_empty</code> bool, optional - If True, repeats previous frame in the output when encounters an empty frame. Defaults to False.</li> <li><code>target_name</code> str, optional - Target output name for the video. Defaults to None (which assumes that the input filename with the suffix \"_flow_dense\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_flow/#returns","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A new MgVideo pointing to the output video file.</li> </ul>"},{"location":"musicalgestures/_flow/#flowget_acceleration","title":"Flow().get_acceleration","text":"<p>[find in source code]</p> <pre><code>def get_acceleration(velocity, fps):\n</code></pre>"},{"location":"musicalgestures/_flow/#flowget_velocity","title":"Flow().get_velocity","text":"<p>[find in source code]</p> <pre><code>def get_velocity(\n    flow,\n    sum_flow_pixels,\n    flow_shape,\n    distance_meters,\n    timestep_seconds,\n    move_step,\n    angle_of_view,\n):\n</code></pre>"},{"location":"musicalgestures/_flow/#flowsparse","title":"Flow().sparse","text":"<p>[find in source code]</p> <pre><code>def sparse(\n    filename=None,\n    corner_max_corners=100,\n    corner_quality_level=0.3,\n    corner_min_distance=7,\n    corner_block_size=7,\n    of_win_size=(15, 15),\n    of_max_level=2,\n    of_criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a sparse optical flow video of the input video file using <code>cv2.calcOpticalFlowPyrLK()</code>. <code>cv2.goodFeaturesToTrack()</code> is used for the corner estimation. The description of the matching parameters are taken from the cv2 documentation.</p>"},{"location":"musicalgestures/_flow/#arguments_1","title":"Arguments","text":"<ul> <li><code>filename</code> str, optional - Path to the input video file. If None, the video file of the MgVideo is used. Defaults to None.</li> <li><code>corner_max_corners</code> int, optional - Maximum number of corners to return. If there are more corners than are found, the strongest of them is returned. <code>maxCorners &lt;= 0</code> implies that no limit on the maximum is set and all detected corners are returned. Defaults to 100.</li> <li><code>corner_quality_level</code> float, optional - Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue (see cornerMinEigenVal in cv2 docs) or the Harris function response (see cornerHarris in cv2 docs). The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500, and the qualityLevel=0.01, then all the corners with the quality measure less than 15 are rejected. Defaults to 0.3.</li> <li><code>corner_min_distance</code> int, optional - Minimum possible Euclidean distance between the returned corners. Defaults to 7.</li> <li><code>corner_block_size</code> int, optional - Size of an average block for computing a derivative covariation matrix over each pixel neighborhood. See cornerEigenValsAndVecs in cv2 docs. Defaults to 7.</li> <li><code>of_win_size</code> tuple, optional - Size of the search window at each pyramid level. Defaults to (15, 15).</li> <li><code>of_max_level</code> int, optional - 0-based maximal pyramid level number. If set to 0, pyramids are not used (single level), if set to 1, two levels are used, and so on. If pyramids are passed to input then the algorithm will use as many levels as pyramids have but no more than <code>maxLevel</code>. Defaults to 2.</li> <li><code>of_criteria</code> tuple, optional - Specifies the termination criteria of the iterative search algorithm (after the specified maximum number of iterations criteria.maxCount or when the search window moves by less than criteria.epsilon). Defaults to (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03).</li> <li><code>target_name</code> str, optional - Target output name for the video. Defaults to None (which assumes that the input filename with the suffix \"_flow_sparse\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_flow/#returns_1","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A new MgVideo pointing to the output video file.</li> </ul>"},{"location":"musicalgestures/_flow/#flowvelocity_meters_per_second","title":"Flow().velocity_meters_per_second","text":"<p>[find in source code]</p> <pre><code>def velocity_meters_per_second(\n    velocity_pixels_per_second,\n    flow_shape,\n    distance_meters,\n    angle_of_view,\n):\n</code></pre>"},{"location":"musicalgestures/_frameaverage/","title":"Frameaverage","text":"<p>Auto-generated documentation for musicalgestures._frameaverage module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Frameaverage<ul> <li>mg_pixelarray</li> <li>mg_pixelarray_cv2</li> <li>mg_pixelarray_stats</li> </ul> </li> </ul>"},{"location":"musicalgestures/_frameaverage/#mg_pixelarray","title":"mg_pixelarray","text":"<p>[find in source code]</p> <pre><code>def mg_pixelarray(self, width=640, target_name=None, overwrite=False):\n</code></pre> <p>Creates a 'Frame-Averaged Pixel Array' of a video by reducing each frame to a single pixel and arranging all frames into a single image. This is equivalent to the bash script that scales each frame to 1x1 pixel and then tiles them into a grid.</p> <p>Based on the original bash script concept: - Each frame is reduced to a single pixel (average color of the frame) - All pixel values are arranged in a grid with specified width - Height is calculated automatically based on total frames and width</p>"},{"location":"musicalgestures/_frameaverage/#arguments","title":"Arguments","text":"<ul> <li><code>width</code> int, optional - Width of the output image in pixels (number of frame-pixels per row).                       Defaults to 640.</li> <li><code>target_name</code> str, optional - The name of the output image file. If None, uses input filename                            with 'framearray' suffix. Defaults to None. <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically                           increment target filenames to avoid overwriting. Defaults to False.</li>"},{"location":"musicalgestures/_frameaverage/#returns","title":"Returns","text":"<ul> <li><code>MgImage</code> - A new MgImage pointing to the output frame-averaged pixel array image file.</li> </ul>"},{"location":"musicalgestures/_frameaverage/#mg_pixelarray_cv2","title":"mg_pixelarray_cv2","text":"<p>[find in source code]</p> <pre><code>def mg_pixelarray_cv2(self, width=640, target_name=None, overwrite=False):\n</code></pre> <p>Alternative implementation using OpenCV for more control over the process. Creates a 'Frame-Averaged Pixel Array' by reading each frame, calculating its average color, and arranging these average colors in a grid.</p>"},{"location":"musicalgestures/_frameaverage/#arguments_1","title":"Arguments","text":"<ul> <li><code>width</code> int, optional - Width of the output image in pixels. Defaults to 640.</li> <li><code>target_name</code> str, optional - The name of the output image file. Defaults to None.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_frameaverage/#returns_1","title":"Returns","text":"<ul> <li><code>MgImage</code> - A new MgImage pointing to the output frame-averaged pixel array image file.</li> </ul>"},{"location":"musicalgestures/_frameaverage/#mg_pixelarray_stats","title":"mg_pixelarray_stats","text":"<p>[find in source code]</p> <pre><code>def mg_pixelarray_stats(self, width=640, include_stats=True):\n</code></pre> <p>Creates a frame-averaged pixel array and optionally returns statistics about the video. This function provides additional information similar to the bash script's output.</p>"},{"location":"musicalgestures/_frameaverage/#arguments_2","title":"Arguments","text":"<ul> <li><code>width</code> int, optional - Width of the output image in pixels. Defaults to 640.</li> <li><code>include_stats</code> bool, optional - Whether to return detailed statistics. Defaults to True.</li> </ul>"},{"location":"musicalgestures/_frameaverage/#returns_2","title":"Returns","text":"<ul> <li><code>dict</code> - Dictionary containing the generated MgImage and optional statistics.</li> </ul>"},{"location":"musicalgestures/_grid/","title":"Grid","text":"<p>Auto-generated documentation for musicalgestures._grid module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Grid<ul> <li>mg_grid</li> </ul> </li> </ul>"},{"location":"musicalgestures/_grid/#mg_grid","title":"mg_grid","text":"<p>[find in source code]</p> <pre><code>def mg_grid(\n    self,\n    height=300,\n    rows=3,\n    cols=3,\n    padding=0,\n    margin=0,\n    target_name=None,\n    overwrite=False,\n    return_array=False,\n):\n</code></pre> <p>Generates frame strip video preview using ffmpeg.</p>"},{"location":"musicalgestures/_grid/#arguments","title":"Arguments","text":"<ul> <li><code>height</code> int, optional - Frame height, width is adjusted automatically to keep the correct aspect ratio. Defaults to 300.</li> <li><code>rows</code> int, optional - Number of rows of the grid. Defaults to 3.</li> <li><code>cols</code> int, optional - Number of columns of the grid. Defaults to 3.</li> <li><code>padding</code> int, optional - Padding size between the frames. Defaults to 0.</li> <li><code>margin</code> int, optional - Margin size for the grid. Defaults to 0.</li> <li><code>target_name</code> [type], optional - Target output name for the grid image. Defaults to None.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> <li><code>return_array</code> bool, optional - Whether to return an array of not. If set to False the function writes the grid image to disk. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_grid/#returns","title":"Returns","text":"<ul> <li><code>MgImage</code> - An MgImage object referring to the internal grid image.</li> </ul>"},{"location":"musicalgestures/_history/","title":"History","text":"<p>Auto-generated documentation for musicalgestures._history module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / History<ul> <li>ParameterError</li> <li>history_cv2</li> <li>history_ffmpeg</li> </ul> </li> </ul>"},{"location":"musicalgestures/_history/#parametererror","title":"ParameterError","text":"<p>[find in source code]</p> <pre><code>class ParameterError(Exception):\n</code></pre> <p>Base class for argument errors.</p>"},{"location":"musicalgestures/_history/#history_cv2","title":"history_cv2","text":"<p>[find in source code]</p> <pre><code>def history_cv2(\n    self,\n    filename=None,\n    history_length=10,\n    weights=1,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>This function  creates a video where each frame is the average of the N previous frames, where n is determined by <code>history_length</code>. The history frames are summed up and normalized, and added to the current frame to show the history. Uses cv2.</p>"},{"location":"musicalgestures/_history/#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> str, optional - Path to the input video file. If None, the video file of the MgVideo is used. Defaults to None.</li> <li><code>history_length</code> int, optional - Number of frames to be saved in the history tail. Defaults to 10.</li> <li><code>weights</code> int/float/list, optional - Defines the weight or weights applied to the frames in the history tail. If given as list the first element in the list will correspond to the weight of the newest frame in the tail. Defaults to 1.</li> <li><code>target_name</code> str, optional - Target output name for the video. Defaults to None (which assumes that the input filename with the suffix \"_history\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_history/#returns","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A new MgVideo pointing to the output video file.</li> </ul>"},{"location":"musicalgestures/_history/#history_ffmpeg","title":"history_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def history_ffmpeg(\n    self,\n    filename=None,\n    history_length=10,\n    weights=1,\n    normalize=False,\n    norm_strength=1,\n    norm_smooth=0,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>This function  creates a video where each frame is the average of the N previous frames, where n is determined by <code>history_length</code>. The history frames are summed up and normalized, and added to the current frame to show the history. Uses ffmpeg.</p>"},{"location":"musicalgestures/_history/#arguments_1","title":"Arguments","text":"<ul> <li><code>filename</code> str, optional - Path to the input video file. If None, the video file of the MgVideo is used. Defaults to None.</li> <li><code>history_length</code> int, optional - Number of frames to be saved in the history tail. Defaults to 10.</li> <li><code>weights</code> int/float/list/str, optional - Defines the weight or weights applied to the frames in the history tail. If given as list the first element in the list will correspond to the weight of the newest frame in the tail. If given as a str - like \"3 1.2 1\" - it will be automatically converted to a list - like [3, 1.2, 1]. Defaults to 1.</li> <li><code>normalize</code> bool, optional - If True, the history video will be normalized. This can be useful when processing motion (frame difference) videos. Defaults to False.</li> <li><code>norm_strength</code> int/float, optional - Defines the strength of the normalization where 1 represents full strength. Defaults to 1.</li> <li><code>norm_smooth</code> int, optional - Defines the number of previous frames to use for temporal smoothing. The input range of each channel is smoothed using a rolling average over the current frame and the <code>norm_smooth</code> previous frames. Defaults to 0.</li> <li><code>target_name</code> str, optional - Target output name for the video. Defaults to None (which assumes that the input filename with the suffix \"_history\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_history/#returns_1","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A new MgVideo pointing to the output video file.</li> </ul>"},{"location":"musicalgestures/_impacts/","title":"Impacts","text":"<p>Auto-generated documentation for musicalgestures._impacts module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Impacts<ul> <li>impact_detection</li> <li>impact_envelope</li> <li>mg_impacts</li> </ul> </li> </ul>"},{"location":"musicalgestures/_impacts/#impact_detection","title":"impact_detection","text":"<p>[find in source code]</p> <pre><code>@jit(nopython=True)\ndef impact_detection(envelopes, time, fps, local_mean=0.1, local_maxima=0.15):\n</code></pre>"},{"location":"musicalgestures/_impacts/#impact_envelope","title":"impact_envelope","text":"<p>[find in source code]</p> <pre><code>def impact_envelope(directogram, kernel_size=5):\n</code></pre>"},{"location":"musicalgestures/_impacts/#mg_impacts","title":"mg_impacts","text":"<p>[find in source code]</p> <pre><code>def mg_impacts(\n    self,\n    title=None,\n    detection=True,\n    local_mean=0.1,\n    local_maxima=0.15,\n    filtertype='Adaptative',\n    thresh=0.05,\n    kernel_size=5,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Compute a visual analogue of an onset envelope, aslo known as an impact envelope (Abe Davis). This is computed by summing over positive entries in the columns of the directogram. This gives an impact envelope with precisely the same form as an onset envelope. To account for large outlying spikes that sometimes happen at shot boundaries (i.e., cuts), the 99th percentile of the impact envelope values are clipped to the 98th percentile. Then, the impact envelopes are normalized by their maximum to make calculations more consistent across video resolutions. Fianlly, the local mean of the impact envelopes are calculated using a 0.1-second window, and local maxima using a 0.15-second window. Impacts are defined as local maxima that are above their local mean by at least 10% of the envelope\u2019s global maximum.</p> <p>Source: Abe Davis -- Visual Rhythm and Beat (section 4.2 and 4.3)</p>"},{"location":"musicalgestures/_impacts/#arguments","title":"Arguments","text":"<ul> <li><code>title</code> str, optional - Optionally add title to the figure. Defaults to None, which uses 'Directogram' as a title. Defaults to None.</li> <li><code>detection</code> bool, optional - Whether to allow the detection of impacts based on local mean and local maxima or not.</li> <li><code>local_mean</code> float, optional - Size of the local mean window in seconds which reduces the amount of intensity variation between one impact and the next.</li> <li><code>local_maxima</code> float, optional - Size of the local maxima window in seconds for the impact envelopes</li> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. 'Adaptative' perform adaptative threshold as the weighted sum of 11 neighborhood pixels where weights are a Gaussian window. Defaults to 'Adaptative'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>kernel_size</code> int, optional - Size of structuring element. Defaults to 5.</li> <li><code>target_name</code> str, optional - Target output name for the directogram. Defaults to None (which assumes that the input filename with the suffix \"_dg\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_impacts/#returns","title":"Returns","text":"<ul> <li><code>MgFigure</code> - An MgFigure object referring to the internal figure and its data.</li> </ul>"},{"location":"musicalgestures/_info/","title":"Info","text":"<p>Auto-generated documentation for musicalgestures._info module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Info<ul> <li>mg_info</li> <li>plot_frames</li> </ul> </li> </ul>"},{"location":"musicalgestures/_info/#mg_info","title":"mg_info","text":"<p>[find in source code]</p> <pre><code>def mg_info(self, type=None, autoshow=True, overwrite=False):\n</code></pre> <p>Returns info about video/audio/format file using ffprobe.</p>"},{"location":"musicalgestures/_info/#arguments","title":"Arguments","text":"<ul> <li><code>type</code> str, optional - Type of information to retrieve. Possible choice are 'audio', 'video', 'format' or 'frame'. Defaults to None (which gives info about video, audio and format).</li> <li><code>autoshow</code> bool, optional - Whether to show the I/P/B frames figure automatically. Defaults to True. NB: The type argument needs to be set to 'frame'.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_info/#returns","title":"Returns","text":"<ul> <li><code>str</code> - decoded ffprobe output (stdout) as a list containing three dictionaries for video, audio and format metadata.</li> </ul>"},{"location":"musicalgestures/_info/#plot_frames","title":"plot_frames","text":"<p>[find in source code]</p> <pre><code>def plot_frames(\n    df,\n    label,\n    color_list=['#636EFA', '#00CC96', '#EF553B'],\n    index=0,\n):\n</code></pre>"},{"location":"musicalgestures/_input_test/","title":"Input Test","text":"<p>Auto-generated documentation for musicalgestures._input_test module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Input Test<ul> <li>Error</li> <li>InputError</li> <li>mg_input_test</li> </ul> </li> </ul>"},{"location":"musicalgestures/_input_test/#error","title":"Error","text":"<p>[find in source code]</p> <pre><code>class Error(Exception):\n</code></pre> <p>Base class for exceptions in this module.</p>"},{"location":"musicalgestures/_input_test/#inputerror","title":"InputError","text":"<p>[find in source code]</p> <pre><code>class InputError(Error):\n    def __init__(message):\n</code></pre> <p>Exception raised for errors in the input.</p>"},{"location":"musicalgestures/_input_test/#arguments","title":"Arguments","text":"<ul> <li><code>Error</code> str - Explanation of the error.</li> </ul>"},{"location":"musicalgestures/_input_test/#see-also","title":"See also","text":"<ul> <li>Error</li> </ul>"},{"location":"musicalgestures/_input_test/#mg_input_test","title":"mg_input_test","text":"<p>[find in source code]</p> <pre><code>def mg_input_test(\n    filename,\n    array,\n    fps,\n    filtertype,\n    thresh,\n    starttime,\n    endtime,\n    blur,\n    skip,\n    frames,\n):\n</code></pre> <p>Gives feedback to user if initialization from input went wrong.</p>"},{"location":"musicalgestures/_input_test/#arguments_1","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>array</code> np.ndarray, optional - Generates an MgVideo object from a video array. Defauts to None.</li> <li><code>fps</code> float, optional - The frequency at which consecutive images from the video array are captured or displayed. Defauts to None.</li> <li><code>filtertype</code> str - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method.</li> <li><code>thresh</code> float - A number in the range of 0 to 1. Eliminates pixel values less than given threshold.</li> <li><code>starttime</code> int/float - Trims the video from this start time (s).</li> <li><code>endtime</code> int/float - Trims the video until this end time (s).</li> <li><code>blur</code> str - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise.</li> <li><code>skip</code> int - Every n frames to discard. <code>skip=0</code> keeps all frames, <code>skip=1</code> skips every other frame.</li> <li><code>frames</code> int - Specify a fixed target number of frames to extract from the video.</li> </ul>"},{"location":"musicalgestures/_input_test/#raises","title":"Raises","text":"<ul> <li><code>InputError</code> - If the types or options are wrong in the input.</li> </ul>"},{"location":"musicalgestures/_mglist/","title":"MgList","text":"<p>Auto-generated documentation for musicalgestures._mglist module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / MgList<ul> <li>MgList<ul> <li>MgList().__add__</li> <li>MgList().__delitem__</li> <li>MgList().__getitem__</li> <li>MgList().__iadd__</li> <li>MgList().__iter__</li> <li>MgList().__len__</li> <li>MgList().__setitem__</li> <li>MgList().as_figure</li> <li>MgList().show</li> </ul> </li> </ul> </li> </ul>"},{"location":"musicalgestures/_mglist/#mglist_1","title":"MgList","text":"<p>[find in source code]</p> <pre><code>class MgList():\n    def __init__(*objectlist):\n</code></pre> <p>Class for handling lists of MgImage, MgFigure and MgList objects in the Musical Gestures Toolbox.</p>"},{"location":"musicalgestures/_mglist/#attributes","title":"Attributes","text":"<ul> <li>*objectlist : objects and/or list(s) of objects</li> </ul> <p>MgObjects and/or MgImages to include in the list.</p>"},{"location":"musicalgestures/_mglist/#mglist__add__","title":"MgList().__add__","text":"<p>[find in source code]</p> <pre><code>def __add__(other):\n</code></pre> <p>Implements <code>+</code>.</p>"},{"location":"musicalgestures/_mglist/#arguments","title":"Arguments","text":"<ul> <li><code>other</code> MgImage/MgFigure/MgList - The object(s) to add to the MgList.</li> </ul>"},{"location":"musicalgestures/_mglist/#returns","title":"Returns","text":"<ul> <li><code>MgList</code> - The incremented MgList.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglist__delitem__","title":"MgList().__delitem__","text":"<p>[find in source code]</p> <pre><code>def __delitem__(key):\n</code></pre> <p>Implements deleting elements given an index from the MgList.</p>"},{"location":"musicalgestures/_mglist/#arguments_1","title":"Arguments","text":"<ul> <li><code>key</code> int - The index of the element to delete.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglist__getitem__","title":"MgList().__getitem__","text":"<p>[find in source code]</p> <pre><code>def __getitem__(key):\n</code></pre> <p>Implements getting elements given an index from the MgList.</p>"},{"location":"musicalgestures/_mglist/#arguments_2","title":"Arguments","text":"<ul> <li><code>key</code> int - The index of the element to retrieve.</li> </ul>"},{"location":"musicalgestures/_mglist/#returns_1","title":"Returns","text":"<ul> <li><code>MgImage/MgFigure/MgList</code> - The element at <code>key</code>.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglist__iadd__","title":"MgList().__iadd__","text":"<p>[find in source code]</p> <pre><code>def __iadd__(other):\n</code></pre> <p>Implements <code>+=</code>.</p>"},{"location":"musicalgestures/_mglist/#arguments_3","title":"Arguments","text":"<ul> <li><code>other</code> MgImage/MgFigure/MgList - The object(s) to add to the MgList.</li> </ul>"},{"location":"musicalgestures/_mglist/#returns_2","title":"Returns","text":"<ul> <li><code>MgList</code> - The incremented MgList.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglist__iter__","title":"MgList().__iter__","text":"<p>[find in source code]</p> <pre><code>def __iter__():\n</code></pre> <p>Implements <code>iter()</code>.</p>"},{"location":"musicalgestures/_mglist/#returns_3","title":"Returns","text":"<ul> <li><code>iterator</code> - The iterator of <code>self.objectlist</code>.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglist__len__","title":"MgList().__len__","text":"<p>[find in source code]</p> <pre><code>def __len__():\n</code></pre> <p>Implements <code>len()</code>.</p>"},{"location":"musicalgestures/_mglist/#returns_4","title":"Returns","text":"<ul> <li><code>int</code> - The length of the MgList.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglist__setitem__","title":"MgList().__setitem__","text":"<p>[find in source code]</p> <pre><code>def __setitem__(key, value):\n</code></pre> <p>Implements setting elements given an index from the MgList.</p>"},{"location":"musicalgestures/_mglist/#arguments_4","title":"Arguments","text":"<ul> <li><code>key</code> int - The index of the element to change.</li> <li><code>value</code> MgImage/MgFigure/MgList - The element to place at <code>key</code>.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglistas_figure","title":"MgList().as_figure","text":"<p>[find in source code]</p> <pre><code>def as_figure(dpi=300, autoshow=True, title=None, export_png=True):\n</code></pre> <p>Creates a time-aligned figure from all the elements in the MgList.</p>"},{"location":"musicalgestures/_mglist/#arguments_5","title":"Arguments","text":"<ul> <li><code>dpi</code> int, optional - Image quality of the rendered figure in DPI. Defaults to 300.</li> <li><code>autoshow</code> bool, optional - Whether to show the resulting figure automatically. Defaults to True.</li> <li><code>title</code> str, optional - Optionally add a title to the figure. Defaults to None (no title).</li> <li><code>export_png</code> bool, optional - Whether to export a png image of the resulting figure automatically. Defaults to True.</li> </ul>"},{"location":"musicalgestures/_mglist/#returns_5","title":"Returns","text":"<ul> <li><code>MgFigure</code> - The MgFigure with all the elements from the MgList as layers.</li> </ul>"},{"location":"musicalgestures/_mglist/#mglistshow","title":"MgList().show","text":"<p>[find in source code]</p> <pre><code>def show(\n    filename=None,\n    key=None,\n    mode='windowed',\n    window_width=640,\n    window_height=480,\n    window_title=None,\n):\n</code></pre> <p>Iterates all objects in the MgList and calls <code>mg_show()</code> on them.</p>"},{"location":"musicalgestures/_motionanalysis/","title":"Motionanalysis","text":"<p>Auto-generated documentation for musicalgestures._motionanalysis module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Motionanalysis<ul> <li>area</li> <li>centroid</li> </ul> </li> </ul>"},{"location":"musicalgestures/_motionanalysis/#area","title":"area","text":"<p>[find in source code]</p> <pre><code>def area(motion_frame, height, width):\n</code></pre>"},{"location":"musicalgestures/_motionanalysis/#centroid","title":"centroid","text":"<p>[find in source code]</p> <pre><code>def centroid(image, width, height):\n</code></pre> <p>Computes the centroid and quantity of motion in an image or frame.</p>"},{"location":"musicalgestures/_motionanalysis/#arguments","title":"Arguments","text":"<ul> <li><code>image</code> np.array(uint8) - The input image matrix for the centroid estimation function.</li> <li><code>width</code> int - The pixel width of the input video capture.</li> <li><code>height</code> int - The pixel height of the input video capture.</li> </ul>"},{"location":"musicalgestures/_motionanalysis/#returns","title":"Returns","text":"<ul> <li><code>np.array(2)</code> - X and Y coordinates of the centroid of motion.</li> <li><code>int</code> - Quantity of motion: How large the change was in pixels.</li> </ul>"},{"location":"musicalgestures/_motionvideo/","title":"Motionvideo","text":"<p>Auto-generated documentation for musicalgestures._motionvideo module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Motionvideo<ul> <li>mg_motion</li> <li>mg_motiondata</li> <li>mg_motiongrams</li> <li>mg_motionplots</li> <li>mg_motionscore</li> <li>mg_motionvideo</li> <li>save_analysis</li> <li>save_txt</li> </ul> </li> </ul>"},{"location":"musicalgestures/_motionvideo/#mg_motion","title":"mg_motion","text":"<p>[find in source code]</p> <pre><code>def mg_motion(\n    self,\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    kernel_size=5,\n    use_median=False,\n    unit='seconds',\n    atadenoise=False,\n    motion_analysis='all',\n    inverted_motionvideo=False,\n    inverted_motiongram=False,\n    equalize_motiongram=False,\n    audio_descriptors=False,\n    save_plot=True,\n    plot_title=None,\n    save_data=True,\n    data_format='csv',\n    save_motiongrams=True,\n    save_video=True,\n    target_name_video=None,\n    target_name_plot=None,\n    target_name_data=None,\n    target_name_mgx=None,\n    target_name_mgy=None,\n    overwrite=False,\n):\n</code></pre> <p>Finds the difference in pixel value from one frame to the next in an input video, and saves the frames into a new video. Describes the motion in the recording. Outputs: a motion video, a plot describing the centroid of motion and the quantity of motion, horizontal and vertical motiongrams, and a text file containing the quantity of motion and the centroid of motion for each frame with timecodes in milliseconds.</p>"},{"location":"musicalgestures/_motionvideo/#arguments","title":"Arguments","text":"<ul> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>kernel_size</code> int, optional - Size of structuring element. Defaults to 5.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>unit</code> str, optional - Unit in QoM plot. Accepted values are 'seconds' or 'samples'. Defaults to 'seconds'.</li> <li><code>atadenoise</code> bool, optional - If True, applies an adaptive temporal averaging denoiser every 129 frames. Defaults to False.</li> <li><code>motion_analysis</code> str, optional - Specify which motion analysis to process or all. 'AoM' renders the Area of Motion. 'CoM' renders the Centroid of Motion. 'QoM' renders the Quantity of Motion. 'all' renders all the motion analysis available. Defaults to 'all'.</li> <li><code>inverted_motionvideo</code> bool, optional - If True, inverts colors of the motion video. Defaults to False.</li> <li><code>inverted_motiongram</code> bool, optional - If True, inverts colors of the motiongrams. Defaults to False.</li> <li><code>equalize_motiongram</code> bool, optional - If True, converts the motiongrams to hsv-color space and flattens the value channel (v). Defaults to True.</li> <li><code>save_plot</code> bool, optional - If True, outputs motion-plot. Defaults to True.</li> <li><code>plot_title</code> str, optional - Optionally add title to the plot. Defaults to None, which uses the file name as a title.</li> <li><code>save_data</code> bool, optional - If True, outputs motion-data. Defaults to True.</li> <li><code>data_format</code> str/list, optional - Specifies format of motion-data. Accepted values are 'csv', 'tsv' and 'txt'. For multiple output formats, use list, eg. ['csv', 'txt']. Defaults to 'csv'.</li> <li><code>save_motiongrams</code> bool, optional - If True, outputs motiongrams. Defaults to True.</li> <li><code>save_video</code> bool, optional - If True, outputs the motion video. Defaults to True.</li> <li><code>target_name_video</code> str, optional - Target output name for the video. Defaults to None (which assumes that the input filename with the suffix \"_motion\" should be used).</li> <li><code>target_name_plot</code> str, optional - Target output name for the plot. Defaults to None (which assumes that the input filename with the suffix \"_motion_com_aom_qom\" should be used).</li> <li><code>target_name_data</code> str, optional - Target output name for the data. Defaults to None (which assumes that the input filename with the suffix \"_motion\" should be used).</li> <li><code>target_name_mgx</code> str, optional - Target output name for the vertical motiongram. Defaults to None (which assumes that the input filename with the suffix \"_mgx\" should be used).</li> <li><code>target_name_mgy</code> str, optional - Target output name for the horizontal motiongram. Defaults to None (which assumes that the input filename with the suffix \"_mgy\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#returns","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A new MgVideo pointing to the output video file. If <code>save_video=False</code>, it returns an MgVideo pointing to the input video file.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#mg_motiondata","title":"mg_motiondata","text":"<p>[find in source code]</p> <pre><code>def mg_motiondata(\n    self,\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    kernel_size=5,\n    atadenoise=False,\n    use_median=False,\n    motion_analysis='all',\n    data_format='csv',\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Shortcut for mg_motion to only render motion data.</p>"},{"location":"musicalgestures/_motionvideo/#arguments_1","title":"Arguments","text":"<ul> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>kernel_size</code> int, optional - Size of structuring element. Defaults to 5.</li> <li><code>atadenoise</code> bool, optional - If True, applies an adaptive temporal averaging denoiser every 129 frames. Defaults to False.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>motion_analysis</code> str, optional - Specify which motion analysis to process or all. 'AoM' renders the Area of Motion. 'CoM' renders the Centroid of Motion. 'QoM' renders the Quantity of Motion. 'all' renders all the motion analysis available. Defaults to 'all'.</li> <li><code>data_format</code> str/list, optional - Specifies format of motion-data. Accepted values are 'csv', 'tsv' and 'txt'. For multiple output formats, use list, eg. ['csv', 'txt']. Defaults to 'csv'.</li> <li><code>target_name</code> str, optional - Target output name for the data. Defaults to None (which assumes that the input filename with the suffix \"_motion\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#returns_1","title":"Returns","text":"<ul> <li><code>str/list</code> - The path(s) to the rendered data file(s).</li> </ul>"},{"location":"musicalgestures/_motionvideo/#mg_motiongrams","title":"mg_motiongrams","text":"<p>[find in source code]</p> <pre><code>def mg_motiongrams(\n    self,\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    use_median=False,\n    atadenoise=True,\n    kernel_size=5,\n    inverted_motiongram=False,\n    equalize_motiongram=True,\n    target_name_mgx=None,\n    target_name_mgy=None,\n    overwrite=False,\n):\n</code></pre> <p>Shortcut for mg_motion to only render motiongrams.</p>"},{"location":"musicalgestures/_motionvideo/#arguments_2","title":"Arguments","text":"<ul> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>atadenoise</code> bool, optional - If True, applies an adaptive temporal averaging denoiser every 129 frames. Defaults to True.</li> <li><code>kernel_size</code> int, optional - Size of the median filter (if <code>use_median=True</code>) or the erosion filter (if <code>filtertype='blob'</code>). Defaults to 5.</li> <li><code>inverted_motiongram</code> bool, optional - If True, inverts colors of the motiongrams. Defaults to False.</li> <li><code>equalize_motiongram</code> bool, optional - If True, converts the motiongrams to hsv-color space and flattens the value channel (v). Defaults to True.</li> <li><code>target_name_mgx</code> str, optional - Target output name for the vertical motiongram. Defaults to None (which assumes that the input filename with the suffix \"_mgx\" should be used).</li> <li><code>target_name_mgy</code> str, optional - Target output name for the horizontal motiongram. Defaults to None (which assumes that the input filename with the suffix \"_mgy\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#returns_2","title":"Returns","text":"<ul> <li><code>MgList</code> - An MgList pointing to the output motiongram images (as MgImages).</li> </ul>"},{"location":"musicalgestures/_motionvideo/#mg_motionplots","title":"mg_motionplots","text":"<p>[find in source code]</p> <pre><code>def mg_motionplots(\n    self,\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    kernel_size=5,\n    use_median=False,\n    atadenoise=False,\n    motion_analysis='all',\n    audio_descriptors=False,\n    unit='seconds',\n    title=None,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Shortcut for mg_motion to only render motion plots.</p>"},{"location":"musicalgestures/_motionvideo/#arguments_3","title":"Arguments","text":"<ul> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>kernel_size</code> int, optional - Size of structuring element. Defaults to 5.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>atadenoise</code> bool, optional - If True, applies an adaptive temporal averaging denoiser every 129 frames. Defaults to False.</li> <li><code>motion_analysis</code> str, optional - Specify which motion analysis to process or all. 'AoM' renders the Area of Motion. 'CoM' renders the Centroid of Motion. 'QoM' renders the Quantity of Motion. 'all' renders all the motion analysis available. Defaults to 'all'.</li> <li><code>audio_descriptors</code> bool, optional - Whether to plot motion plots together with audio descriptors in order to see possible correlations in the data. Deflauts to False.</li> <li><code>unit</code> str, optional - Unit in QoM plot. Accepted values are 'seconds' or 'samples'. Defaults to 'seconds'.</li> <li><code>title</code> str, optional - Optionally add title to the plot. Defaults to None, which uses the file name as a title.</li> <li><code>target_name</code> str, optional - Target output name for the plot. Defaults to None (which assumes that the input filename with the suffix \"_motion_com_aom_qom\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#returns_3","title":"Returns","text":"<ul> <li><code>MgImage</code> - An MgImage pointing to the exported image (png) of the motion plots.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#mg_motionscore","title":"mg_motionscore","text":"<p>[find in source code]</p> <pre><code>def mg_motionscore(self):\n</code></pre>"},{"location":"musicalgestures/_motionvideo/#mg_motionvideo","title":"mg_motionvideo","text":"<p>[find in source code]</p> <pre><code>def mg_motionvideo(\n    self,\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    use_median=False,\n    kernel_size=5,\n    inverted_motionvideo=False,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Shortcut to only render the motion video. Uses musicalgestures._utils.motionvideo_ffmpeg. Note that this does not apply median filter by default. If you need it use <code>use_median=True</code>.</p>"},{"location":"musicalgestures/_motionvideo/#arguments_4","title":"Arguments","text":"<ul> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>kernel_size</code> int, optional - Size of the median filter (if <code>use_median=True</code>) or the erosion filter (if <code>filtertype='blob'</code>). Defaults to 5.</li> <li><code>inverted_motionvideo</code> bool, optional - If True, inverts colors of the motion video. Defaults to False.</li> <li><code>target_name</code> str, optional - Target output name for the video. Defaults to None (which assumes that the input filename with the suffix \"_motion\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#returns_4","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A new MgVideo pointing to the output '_motion' video file.</li> </ul>"},{"location":"musicalgestures/_motionvideo/#save_analysis","title":"save_analysis","text":"<p>[find in source code]</p> <pre><code>def save_analysis(\n    of,\n    fps,\n    aom,\n    com,\n    qom,\n    motion_analysis,\n    audio_descriptors,\n    width,\n    height,\n    unit,\n    title,\n    target_name_plot,\n    overwrite,\n):\n</code></pre> <p>Helper function to plot the motion data using matplotlib.</p>"},{"location":"musicalgestures/_motionvideo/#save_txt","title":"save_txt","text":"<p>[find in source code]</p> <pre><code>def save_txt(\n    of,\n    time,\n    aom,\n    com,\n    qom,\n    motion_analysis,\n    width,\n    height,\n    data_format,\n    target_name_data,\n    overwrite,\n):\n</code></pre> <p>Helper function to export motion data as textfile(s).</p>"},{"location":"musicalgestures/_motionvideo_mp_render/","title":"Motionvideo Mp Render","text":"<p>Auto-generated documentation for musicalgestures._motionvideo_mp_render module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Motionvideo Mp Render<ul> <li>bool_from_str</li> <li>calc_frame_groups</li> <li>mg_motion_mp</li> <li>run_pool</li> </ul> </li> </ul>"},{"location":"musicalgestures/_motionvideo_mp_render/#bool_from_str","title":"bool_from_str","text":"<p>[find in source code]</p> <pre><code>def bool_from_str(boolstring):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_render/#calc_frame_groups","title":"calc_frame_groups","text":"<p>[find in source code]</p> <pre><code>def calc_frame_groups(framecount, num_cores):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_render/#mg_motion_mp","title":"mg_motion_mp","text":"<p>[find in source code]</p> <pre><code>def mg_motion_mp(args):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_render/#run_pool","title":"run_pool","text":"<p>[find in source code]</p> <pre><code>def run_pool(func, args, numprocesses):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_run/","title":"Motionvideo Mp Run","text":"<p>Auto-generated documentation for musicalgestures._motionvideo_mp_run module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Motionvideo Mp Run<ul> <li>TrackMultiProgress<ul> <li>TrackMultiProgress().progress</li> <li>TrackMultiProgress().reset</li> </ul> </li> <li>concat_videos</li> <li>mg_motion_mp</li> <li>run_socket_server</li> </ul> </li> </ul>"},{"location":"musicalgestures/_motionvideo_mp_run/#trackmultiprogress","title":"TrackMultiProgress","text":"<p>[find in source code]</p> <pre><code>class TrackMultiProgress():\n    def __init__(numprocesses):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_run/#trackmultiprogressprogress","title":"TrackMultiProgress().progress","text":"<p>[find in source code]</p> <pre><code>def progress(node, iteration):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_run/#trackmultiprogressreset","title":"TrackMultiProgress().reset","text":"<p>[find in source code]</p> <pre><code>def reset():\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_run/#concat_videos","title":"concat_videos","text":"<p>[find in source code]</p> <pre><code>def concat_videos(\n    list_of_videos,\n    target_name=None,\n    overwrite=False,\n    pb_prefix='Concatenating videos:',\n    stream=True,\n):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_run/#mg_motion_mp","title":"mg_motion_mp","text":"<p>[find in source code]</p> <pre><code>def mg_motion_mp(\n    self,\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    kernel_size=5,\n    inverted_motionvideo=False,\n    inverted_motiongram=False,\n    unit='seconds',\n    equalize_motiongram=True,\n    save_plot=True,\n    plot_title=None,\n    save_data=True,\n    data_format='csv',\n    save_motiongrams=True,\n    save_video=True,\n    target_name_video=None,\n    target_name_plot=None,\n    target_name_data=None,\n    target_name_mgx=None,\n    target_name_mgy=None,\n    overwrite=False,\n    num_processes=-1,\n):\n</code></pre>"},{"location":"musicalgestures/_motionvideo_mp_run/#run_socket_server","title":"run_socket_server","text":"<p>[find in source code]</p> <pre><code>def run_socket_server(host, port, pb, numprocesses):\n</code></pre>"},{"location":"musicalgestures/_pose/","title":"Pose","text":"<p>Auto-generated documentation for musicalgestures._pose module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Pose<ul> <li>download_model</li> <li>pose</li> </ul> </li> </ul>"},{"location":"musicalgestures/_pose/#download_model","title":"download_model","text":"<p>[find in source code]</p> <pre><code>def download_model(modeltype):\n</code></pre> <p>Helper function to automatically download model (.caffemodel) files.</p>"},{"location":"musicalgestures/_pose/#pose_1","title":"pose","text":"<p>[find in source code]</p> <pre><code>def pose(\n    self,\n    model='body_25',\n    device='gpu',\n    threshold=0.1,\n    downsampling_factor=2,\n    save_data=True,\n    data_format='csv',\n    save_video=True,\n    target_name_video=None,\n    target_name_data=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a video with the pose estimation (aka. \"keypoint detection\" or \"skeleton tracking\") overlaid on it. Outputs the predictions in a text file containing the normalized x and y coordinates of each keypoints (default format is csv). Uses models from the openpose project.</p>"},{"location":"musicalgestures/_pose/#arguments","title":"Arguments","text":"<ul> <li><code>model</code> str, optional - 'body_25' loads the model trained on the BODY_25 dataset, 'mpi' loads the model trained on the Multi-Person Dataset (MPII), 'coco' loads one trained on the COCO dataset. The BODY_25 model outputs 25 points, the MPII model outputs 15 points, while the COCO model produces 18 points. Defaults to 'body_25'.</li> <li><code>device</code> str, optional - Sets the backend to use for the neural network ('cpu' or 'gpu'). Defaults to 'gpu'.</li> <li><code>threshold</code> float, optional - The normalized confidence threshold that decides whether we keep or discard a predicted point. Discarded points get substituted with (0, 0) in the output data. Defaults to 0.1.</li> <li><code>downsampling_factor</code> int, optional - Decides how much we downsample the video before we pass it to the neural network. For example <code>downsampling_factor=4</code> means that the input to the network is one-fourth the resolution of the source video. Heaviver downsampling reduces rendering time but produces lower quality pose estimation. Defaults to 2.</li> <li><code>save_data</code> bool, optional - Whether we save the predicted pose data to a file. Defaults to True.</li> <li><code>data_format</code> str, optional - Specifies format of pose-data. Accepted values are 'csv', 'tsv' and 'txt'. For multiple output formats, use list, eg. ['csv', 'txt']. Defaults to 'csv'.</li> <li><code>save_video</code> bool, optional - Whether we save the video with the estimated pose overlaid on it. Defaults to True.</li> <li><code>target_name_video</code> str, optional - Target output name for the video. Defaults to None (which assumes that the input filename with the suffix \"_pose\" should be used).</li> <li><code>target_name_data</code> str, optional - Target output name for the data. Defaults to None (which assumes that the input filename with the suffix \"_pose\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_pose/#returns","title":"Returns","text":"<ul> <li><code>MgVideo</code> - An MgVideo pointing to the output video.</li> </ul>"},{"location":"musicalgestures/_show/","title":"Show","text":"<p>Auto-generated documentation for musicalgestures._show module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Show<ul> <li>mg_show</li> <li>show_in_new_process</li> </ul> </li> </ul>"},{"location":"musicalgestures/_show/#mg_show","title":"mg_show","text":"<p>[find in source code]</p> <pre><code>def mg_show(\n    self,\n    filename=None,\n    key=None,\n    mode='windowed',\n    window_width=640,\n    window_height=480,\n    window_title=None,\n    **ipython_kwargs,\n):\n</code></pre> <p>General method to show an image or video file either in a window, or inline in a jupyter notebook.</p>"},{"location":"musicalgestures/_show/#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> str, optional - If given, mg_show will show this file instead of what it inherits from its parent object. Defaults to None.</li> <li><code>key</code> str, optional - If given, mg_show will search for file names corresponding to certain processes you have previously rendered on your source. It is meant to be a shortcut, so you don't have to remember the exact name (and path) of eg. a motion video corresponding to your source in your MgVideo, but you rather just use <code>MgVideo('path/to/vid.mp4').show(key='motion')</code>. Accepted values are 'mgx', 'mgy', 'vgx', 'vgy', 'blend', 'plot', 'motion', 'history', 'motionhistory', 'sparse', and 'dense'. Defaults to None.</li> <li><code>mode</code> str, optional - Whether to show things in a separate window or inline in the jupyter notebook. Accepted values are 'windowed' and 'notebook'. Defaults to 'windowed'.</li> <li><code>window_width</code> int, optional - The width of the window. Defaults to 640.</li> <li><code>window_height</code> int, optional - The height of the window. Defaults to 480.</li> <li><code>window_title</code> str, optional - The title of the window. If None, the title of the window will be the file name. Defaults to None.</li> <li><code>ipython_kwargs</code> dict, optional - Additional arguments for IPython.display.Image or IPython.display.Video. Defaults to None.</li> </ul>"},{"location":"musicalgestures/_show/#show_in_new_process","title":"show_in_new_process","text":"<p>[find in source code]</p> <pre><code>def show_in_new_process(cmd):\n</code></pre>"},{"location":"musicalgestures/_show_window/","title":"Show Window","text":"<p>Auto-generated documentation for musicalgestures._show_window module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Show Window</li> </ul>"},{"location":"musicalgestures/_ssm/","title":"Ssm","text":"<p>Auto-generated documentation for musicalgestures._ssm module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Ssm<ul> <li>mg_ssm</li> <li>slow_dot</li> <li>smooth_downsample_feature_sequence</li> </ul> </li> </ul>"},{"location":"musicalgestures/_ssm/#mg_ssm","title":"mg_ssm","text":"<p>[find in source code]</p> <pre><code>def mg_ssm(\n    self,\n    features='motiongrams',\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    norm=np.inf,\n    threshold=0.001,\n    cmap='gray_r',\n    use_median=False,\n    kernel_size=5,\n    invert_yaxis=True,\n    title=None,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Compute Self-Similarity Matrix (SSM) by converting the input signal into a suitable feature sequence and comparing each element of the feature sequence with all other elements of the sequence. SSMs can be computed over different input features such as 'motiongrams', 'spectrogram', 'chromagram' and 'tempogram'.</p>"},{"location":"musicalgestures/_ssm/#arguments","title":"Arguments","text":"<ul> <li><code>features</code> str, optional - Defines the type of features on which to compute SSM. Possible to compute SSM on 'motiongrams', 'videograms', 'spectrogram', 'chromagram' and 'tempogram'. Defaults to 'motiongrams'.</li> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>norm</code> int, optional - Normalize the columns of the feature sequence. Possible to compute Manhattan norm (1), Euclidean norm (2), Minimum norm (-np.inf), Maximum norm (np.inf), etc. Defaults to np.inf.</li> <li><code>threshold</code> float, optional - Only the columns with norm at least the amount of <code>threshold</code> indicated are normalized. Defaults to 0.001.</li> <li><code>cmap</code> str, optional - A Colormap instance or registered colormap name. The colormap maps the C values to colors. Defaults to 'gray_r'.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>kernel_size</code> int, optional - Size of the median filter (if <code>use_median=True</code>) or the erosion filter (if <code>filtertype='blob'</code>). Defaults to 5.</li> <li><code>invert_axis</code> bool, optional - Whether to invert the y axis of the SSM. Defaults to True.</li> <li><code>title</code> str, optional - Optionally add title to the figure. Possible to set the filename as the title using the string 'filename'. Defaults to None.</li> <li><code>target_name</code> [type], optional - Target output name for the SSM. Defaults to None.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_ssm/#returns","title":"Returns","text":""},{"location":"musicalgestures/_ssm/#if-featuresmotiongrams","title":"if features='motiongrams':","text":"<ul> <li><code>MgList</code> - An MgList pointing to the output SSM images (as MgImages).</li> </ul>"},{"location":"musicalgestures/_ssm/#else","title":"else:","text":"<ul> <li><code>MgImage</code> - An MgImage to the output SSM.</li> </ul>"},{"location":"musicalgestures/_ssm/#slow_dot","title":"slow_dot","text":"<p>[find in source code]</p> <pre><code>def slow_dot(X, Y, length):\n</code></pre> <p>Low-memory implementation of dot product</p>"},{"location":"musicalgestures/_ssm/#smooth_downsample_feature_sequence","title":"smooth_downsample_feature_sequence","text":"<p>[find in source code]</p> <pre><code>def smooth_downsample_feature_sequence(\n    X,\n    sr,\n    filt_len=41,\n    down_sampling=10,\n    w_type='boxcar',\n):\n</code></pre> <p>Smoothes and downsamples a feature sequence. Smoothing is achieved by convolution with a filter kernel</p>"},{"location":"musicalgestures/_ssm/#arguments_1","title":"Arguments","text":"<ul> <li><code>X</code> np.ndarray - Feature sequence.</li> <li><code>sr</code> int - Sampling rate.</li> <li><code>filt_len</code> int, optional - Length of smoothing filter. Defaults to 41.</li> <li><code>down_sampling</code> int, optional - Downsampling factor. Defaults to 10.</li> <li><code>w_type</code> str, optional - Window type of smoothing filter. Defaults to 'boxcar'.</li> </ul>"},{"location":"musicalgestures/_ssm/#returns_1","title":"Returns","text":"<ul> <li><code>X_smooth</code> np.ndarray - Smoothed and downsampled feature sequence.</li> <li><code>sr_feature</code> scalar - Sampling rate of <code>X_smooth</code>.</li> </ul>"},{"location":"musicalgestures/_subtract/","title":"Subtract","text":"<p>Auto-generated documentation for musicalgestures._subtract module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Subtract<ul> <li>mg_subtract</li> </ul> </li> </ul>"},{"location":"musicalgestures/_subtract/#mg_subtract","title":"mg_subtract","text":"<p>[find in source code]</p> <pre><code>def mg_subtract(\n    self,\n    color=True,\n    filtertype=None,\n    threshold=0.05,\n    blur=False,\n    curves=0.15,\n    use_median=False,\n    kernel_size=5,\n    bg_img=None,\n    bg_color='#000000',\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders background subtraction using ffmpeg.</p>"},{"location":"musicalgestures/_subtract/#arguments","title":"Arguments","text":"<ul> <li><code>color</code> bool, optional - If False the input is converted to grayscale at the start of the process. This can significantly reduce render time. Defaults to True.</li> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>threshold</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> bool, optional - Whether to apply a smartblur ffmpeg filter or not. Defaults to False.</li> <li><code>curves</code> int, optional - Apply curves and equalisation threshold filter to subtract the background. Ranges from 0 to 1. Defaults to 0.15.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>kernel_size</code> int, optional - Size of the median filter (if <code>use_median=True</code>) or the erosion filter (if <code>filtertype='blob'</code>). Defaults to 5.</li> <li><code>bg_img</code> str, optional - Path to a background image (.png) that needs to be subtracted from the video. If set to None, it uses an average image of all frames in the video. Defaults to None.</li> <li><code>bg_color</code> str, optional - Set the background color in the video file in hex value. Defaults to '#000000' (black).</li> <li><code>target_name</code> str, optional - Target output name for the motiongram. Defaults to None.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_subtract/#returns","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A MgVideo as subtract for parent MgVideo</li> </ul>"},{"location":"musicalgestures/_utils/","title":"Utils","text":"<p>Auto-generated documentation for musicalgestures._utils module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Utils<ul> <li>FFmpegError</li> <li>FFprobeError</li> <li>FilesNotMatchError</li> <li>MgFigure<ul> <li>MgFigure().show</li> </ul> </li> <li>MgImage</li> <li>MgProgressbar<ul> <li>MgProgressbar().adjust_printlength</li> <li>MgProgressbar().get_now</li> <li>MgProgressbar().over_time_limit</li> <li>MgProgressbar().progress</li> </ul> </li> <li>NoDurationError</li> <li>NoStreamError</li> <li>WrongContainer</li> <li>audio_dilate</li> <li>cast_into_avi</li> <li>clamp</li> <li>convert</li> <li>convert_to_avi</li> <li>convert_to_grayscale</li> <li>convert_to_mp4</li> <li>convert_to_webm</li> <li>crop_ffmpeg</li> <li>embed_audio_in_video</li> <li>extract_frame</li> <li>extract_subclip</li> <li>extract_wav</li> <li>ffmpeg_cmd</li> <li>ffprobe</li> <li>frame2ms</li> <li>framediff_ffmpeg</li> <li>generate_outfilename</li> <li>get_box_video_ratio</li> <li>get_first_frame_as_image</li> <li>get_fps</li> <li>get_frame_planecount</li> <li>get_framecount</li> <li>get_length</li> <li>get_widthheight</li> <li>has_audio</li> <li>in_colab</li> <li>in_ipynb</li> <li>merge_videos</li> <li>motiongrams_ffmpeg</li> <li>motionvideo_ffmpeg</li> <li>pass_if_container_is</li> <li>pass_if_containers_match</li> <li>quality_metrics</li> <li>rotate_video</li> <li>roundup</li> <li>scale_array</li> <li>scale_num</li> <li>str2sec</li> <li>threshold_ffmpeg</li> <li>unwrap_str</li> <li>wrap_str</li> </ul> </li> </ul>"},{"location":"musicalgestures/_utils/#ffmpegerror","title":"FFmpegError","text":"<p>[find in source code]</p> <pre><code>class FFmpegError(Exception):\n    def __init__(message):\n</code></pre>"},{"location":"musicalgestures/_utils/#ffprobeerror","title":"FFprobeError","text":"<p>[find in source code]</p> <pre><code>class FFprobeError(Exception):\n    def __init__(message):\n</code></pre>"},{"location":"musicalgestures/_utils/#filesnotmatcherror","title":"FilesNotMatchError","text":"<p>[find in source code]</p> <pre><code>class FilesNotMatchError(Exception):\n    def __init__(message):\n</code></pre>"},{"location":"musicalgestures/_utils/#mgfigure","title":"MgFigure","text":"<p>[find in source code]</p> <pre><code>class MgFigure():\n    def __init__(\n        figure=None,\n        figure_type=None,\n        data=None,\n        layers=None,\n        image=None,\n    ):\n</code></pre> <p>Class for working with figures and plots within the Musical Gestures Toolbox.</p>"},{"location":"musicalgestures/_utils/#mgfigureshow","title":"MgFigure().show","text":"<p>[find in source code]</p> <pre><code>def show():\n</code></pre> <p>Shows the internal matplotlib.pyplot.figure.</p>"},{"location":"musicalgestures/_utils/#mgimage","title":"MgImage","text":"<p>[find in source code]</p> <pre><code>class MgImage():\n    def __init__(filename):\n</code></pre> <p>Class for handling images in the Musical Gestures Toolbox.</p>"},{"location":"musicalgestures/_utils/#mgprogressbar","title":"MgProgressbar","text":"<p>[find in source code]</p> <pre><code>class MgProgressbar():\n    def __init__(\n        total=100,\n        time_limit=0.5,\n        prefix='Progress',\n        suffix='Complete',\n        decimals=1,\n        length=40,\n        fill='\u2588',\n    ):\n</code></pre> <p>Calls in a loop to create terminal progress bar.</p>"},{"location":"musicalgestures/_utils/#mgprogressbaradjust_printlength","title":"MgProgressbar().adjust_printlength","text":"<p>[find in source code]</p> <pre><code>def adjust_printlength():\n</code></pre>"},{"location":"musicalgestures/_utils/#mgprogressbarget_now","title":"MgProgressbar().get_now","text":"<p>[find in source code]</p> <pre><code>def get_now():\n</code></pre> <p>Gets the current time.</p>"},{"location":"musicalgestures/_utils/#returns","title":"Returns","text":"<ul> <li><code>datetime.datetime.timestamp</code> - The current time.</li> </ul>"},{"location":"musicalgestures/_utils/#mgprogressbarover_time_limit","title":"MgProgressbar().over_time_limit","text":"<p>[find in source code]</p> <pre><code>def over_time_limit():\n</code></pre> <p>Checks if we should redraw the progress bar at this moment.</p>"},{"location":"musicalgestures/_utils/#returns_1","title":"Returns","text":"<ul> <li><code>bool</code> - True if equal or more time has passed than <code>self.time_limit</code> since the last redraw.</li> </ul>"},{"location":"musicalgestures/_utils/#mgprogressbarprogress","title":"MgProgressbar().progress","text":"<p>[find in source code]</p> <pre><code>def progress(iteration):\n</code></pre> <p>Progresses the progress bar to the next step.</p>"},{"location":"musicalgestures/_utils/#arguments","title":"Arguments","text":"<ul> <li><code>iteration</code> float - The current iteration. For example, the 57th out of 100 steps, or 12.3s out of the total 60s.</li> </ul>"},{"location":"musicalgestures/_utils/#nodurationerror","title":"NoDurationError","text":"<p>[find in source code]</p> <pre><code>class NoDurationError(FFprobeError):\n</code></pre>"},{"location":"musicalgestures/_utils/#see-also","title":"See also","text":"<ul> <li>FFprobeError</li> </ul>"},{"location":"musicalgestures/_utils/#nostreamerror","title":"NoStreamError","text":"<p>[find in source code]</p> <pre><code>class NoStreamError(FFprobeError):\n</code></pre>"},{"location":"musicalgestures/_utils/#see-also_1","title":"See also","text":"<ul> <li>FFprobeError</li> </ul>"},{"location":"musicalgestures/_utils/#wrongcontainer","title":"WrongContainer","text":"<p>[find in source code]</p> <pre><code>class WrongContainer(Exception):\n    def __init__(message):\n</code></pre>"},{"location":"musicalgestures/_utils/#audio_dilate","title":"audio_dilate","text":"<p>[find in source code]</p> <pre><code>def audio_dilate(\n    filename,\n    dilation_ratio=1,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Time-stretches or -shrinks (dilates) an audio file using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_1","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the audio file to dilate.</li> <li><code>dilation_ratio</code> float, optional - The source file's length divided by the resulting file's length. Defaults to 1.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename with the suffix \"_dilated\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_2","title":"Returns","text":"<ul> <li><code>str</code> - The path to the output audio file.</li> </ul>"},{"location":"musicalgestures/_utils/#cast_into_avi","title":"cast_into_avi","text":"<p>[find in source code]</p> <pre><code>def cast_into_avi(filename, target_name=None, overwrite=False):\n</code></pre> <p>Experimental Casts a video into and .avi container using ffmpeg. Much faster than convert_to_avi, but does not always work well with cv2 or built-in video players.</p>"},{"location":"musicalgestures/_utils/#arguments_2","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>target_name</code> str, optional - Target filename as path. Defaults to None (which assumes that the input filename should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_3","title":"Returns","text":"<ul> <li><code>str</code> - The path to the output '.avi' file.</li> </ul>"},{"location":"musicalgestures/_utils/#clamp","title":"clamp","text":"<p>[find in source code]</p> <pre><code>def clamp(num, min_value, max_value):\n</code></pre> <p>Clamps a number between a minimum and maximum value.</p>"},{"location":"musicalgestures/_utils/#arguments_3","title":"Arguments","text":"<ul> <li><code>num</code> float - The number to clamp.</li> <li><code>min_value</code> float - The minimum allowed value.</li> <li><code>max_value</code> float - The maximum allowed value.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_4","title":"Returns","text":"<ul> <li><code>float</code> - The clamped number.</li> </ul>"},{"location":"musicalgestures/_utils/#convert","title":"convert","text":"<p>[find in source code]</p> <pre><code>def convert(filename, target_name, overwrite=False):\n</code></pre> <p>Converts a video to another format/container using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_4","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file to convert.</li> <li><code>target_name</code> str - Target filename as path.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_5","title":"Returns","text":"<ul> <li><code>str</code> - The path to the output file.</li> </ul>"},{"location":"musicalgestures/_utils/#convert_to_avi","title":"convert_to_avi","text":"<p>[find in source code]</p> <pre><code>def convert_to_avi(filename, target_name=None, overwrite=False):\n</code></pre> <p>Converts a video to one with .avi extension using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_5","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file to convert.</li> <li><code>target_name</code> str, optional - Target filename as path. Defaults to None (which assumes that the input filename should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_6","title":"Returns","text":"<ul> <li><code>str</code> - The path to the output '.avi' file.</li> </ul>"},{"location":"musicalgestures/_utils/#convert_to_grayscale","title":"convert_to_grayscale","text":"<p>[find in source code]</p> <pre><code>def convert_to_grayscale(filename, target_name=None, overwrite=False):\n</code></pre> <p>Converts a video to grayscale using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_6","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>target_name</code> str, optional - Target filename as path. Defaults to None (which assumes that the input filename with the suffix \"_gray\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_7","title":"Returns","text":"<ul> <li><code>str</code> - The path to the grayscale video file.</li> </ul>"},{"location":"musicalgestures/_utils/#convert_to_mp4","title":"convert_to_mp4","text":"<p>[find in source code]</p> <pre><code>def convert_to_mp4(filename, target_name=None, overwrite=False):\n</code></pre> <p>Converts a video to one with .mp4 extension using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_7","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file to convert.</li> <li><code>target_name</code> str, optional - Target filename as path. Defaults to None (which assumes that the input filename should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_8","title":"Returns","text":"<ul> <li><code>str</code> - The path to the output '.mp4' file.</li> </ul>"},{"location":"musicalgestures/_utils/#convert_to_webm","title":"convert_to_webm","text":"<p>[find in source code]</p> <pre><code>def convert_to_webm(filename, target_name=None, overwrite=False):\n</code></pre> <p>Converts a video to one with .webm extension using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_8","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file to convert.</li> <li><code>target_name</code> str, optional - Target filename as path. Defaults to None (which assumes that the input filename should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_9","title":"Returns","text":"<ul> <li><code>str</code> - The path to the output '.webm' file.</li> </ul>"},{"location":"musicalgestures/_utils/#crop_ffmpeg","title":"crop_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def crop_ffmpeg(filename, w, h, x, y, target_name=None, overwrite=False):\n</code></pre> <p>Crops a video using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_9","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>w</code> int - The desired width.</li> <li><code>h</code> int - The desired height.</li> <li><code>x</code> int - The horizontal coordinate of the top left pixel of the cropping rectangle.</li> <li><code>y</code> int - The vertical coordinate of the top left pixel of the cropping rectangle.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename with the suffix \"_crop\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_10","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_utils/#embed_audio_in_video","title":"embed_audio_in_video","text":"<p>[find in source code]</p> <pre><code>def embed_audio_in_video(source_audio, destination_video, dilation_ratio=1):\n</code></pre> <p>Embeds an audio file as the audio channel of a video file using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_10","title":"Arguments","text":"<ul> <li><code>source_audio</code> str - Path to the audio file to embed.</li> <li><code>destination_video</code> str - Path to the video file to embed the audio file in.</li> <li><code>dilation_ratio</code> float, optional - The source file's length divided by the resulting file's length. Defaults to 1.</li> </ul>"},{"location":"musicalgestures/_utils/#extract_frame","title":"extract_frame","text":"<p>[find in source code]</p> <pre><code>def extract_frame(\n    filename: str,\n    frame: int = None,\n    time: str | float = None,\n    target_name: str = None,\n    overwrite: bool = False,\n) -&gt; str:\n</code></pre> <p>Extracts a single frame from a video using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_11","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>frame</code> int - The frame number to extract.</li> <li><code>time</code> str|float - The time in HH:MM:ss.ms where to extract the frame from. If float, it is interpreted as seconds from the start of the video.</li> <li><code>target_name</code> str, optional - The name for the output file. If None, the name will be \\&lt;input name&gt;FRAME\\&lt;frame number&gt;.\\&lt;file extension&gt;. Defaults to None.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#extract_subclip","title":"extract_subclip","text":"<p>[find in source code]</p> <pre><code>def extract_subclip(filename, t1, t2, target_name=None, overwrite=False):\n</code></pre> <p>Extracts a section of the video using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_12","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>t1</code> float - The start of the section to extract in seconds.</li> <li><code>t2</code> float - The end of the section to extract in seconds.</li> <li><code>target_name</code> str, optional - The name for the output file. If None, the name will be \\&lt;input name&gt;SUB\\&lt;start time in ms&gt;_\\&lt;end time in ms&gt;.\\&lt;file extension&gt;. Defaults to None.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_11","title":"Returns","text":"<ul> <li><code>str</code> - Path to the extracted section as a video.</li> </ul>"},{"location":"musicalgestures/_utils/#extract_wav","title":"extract_wav","text":"<p>[find in source code]</p> <pre><code>def extract_wav(filename, target_name=None, overwrite=False):\n</code></pre> <p>Extracts audio from video into a .wav file via ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_13","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file from which the audio track shall be extracted.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_12","title":"Returns","text":"<ul> <li><code>str</code> - The path to the output audio file.</li> </ul>"},{"location":"musicalgestures/_utils/#ffmpeg_cmd","title":"ffmpeg_cmd","text":"<p>[find in source code]</p> <pre><code>def ffmpeg_cmd(\n    command,\n    total_time,\n    pb_prefix='Progress',\n    print_cmd=False,\n    stream=True,\n    pipe=None,\n):\n</code></pre> <p>Run an ffmpeg command in a subprocess and show progress using an MgProgressbar.</p>"},{"location":"musicalgestures/_utils/#arguments_14","title":"Arguments","text":"<ul> <li><code>command</code> list - The ffmpeg command to execute as a list. Eg. ['ffmpeg', '-y', '-i', 'myVid.mp4', 'myVid.mov']</li> <li><code>total_time</code> float - The length of the output. Needed mainly for the progress bar.</li> <li><code>pb_prefix</code> str, optional - The prefix for the progress bar. Defaults to 'Progress'.</li> <li><code>print_cmd</code> bool, optional - Whether to print the full ffmpeg command to the console before executing it. Good for debugging. Defaults to False.</li> <li><code>stream</code> bool, optional - Whether to have a continuous output stream or just (the last) one. Defaults to True (continuous stream).</li> <li><code>pipe</code> str, optional - Whether to pipe video frames from FFmpeg to numpy array. Possible to read the video frame by frame with pipe='read', to load video in memory with pipe='load', or to write the frames of a numpy array to a video file with pipe='write'. Defaults to None.</li> </ul>"},{"location":"musicalgestures/_utils/#raises","title":"Raises","text":"<ul> <li><code>KeyboardInterrupt</code> - If the user stops the process.</li> <li><code>FFmpegError</code> - If the ffmpeg process was unsuccessful.</li> </ul>"},{"location":"musicalgestures/_utils/#ffprobe","title":"ffprobe","text":"<p>[find in source code]</p> <pre><code>def ffprobe(filename):\n</code></pre> <p>Returns info about video/audio file using FFprobe.</p>"},{"location":"musicalgestures/_utils/#arguments_15","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file to measure.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_13","title":"Returns","text":"<ul> <li><code>str</code> - decoded FFprobe output (stdout) as one string.</li> </ul>"},{"location":"musicalgestures/_utils/#frame2ms","title":"frame2ms","text":"<p>[find in source code]</p> <pre><code>def frame2ms(frame, fps):\n</code></pre> <p>Converts frames to milliseconds.</p>"},{"location":"musicalgestures/_utils/#arguments_16","title":"Arguments","text":"<ul> <li><code>frame</code> int - The index of the frame to be converted to milliseconds.</li> <li><code>fps</code> int - Frames per second.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_14","title":"Returns","text":"<ul> <li><code>int</code> - The rounded millisecond value of the input frame index.</li> </ul>"},{"location":"musicalgestures/_utils/#framediff_ffmpeg","title":"framediff_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def framediff_ffmpeg(filename, target_name=None, color=True, overwrite=False):\n</code></pre> <p>Renders a frame difference video from the input using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_17","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename with the suffix \"_framediff\" should be used).</li> <li><code>color</code> bool, optional - If False, the output will be grayscale. Defaults to True.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_15","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_utils/#generate_outfilename","title":"generate_outfilename","text":"<p>[find in source code]</p> <pre><code>def generate_outfilename(requested_name):\n</code></pre> <p>Returns a unique filepath to avoid overwriting existing files. Increments requested filename if necessary by appending an integer, like \"_0\" or \"_1\", etc to the file name.</p>"},{"location":"musicalgestures/_utils/#arguments_18","title":"Arguments","text":"<ul> <li><code>requested_name</code> str - Requested file name as path string.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_16","title":"Returns","text":"<ul> <li><code>str</code> - If file at requested_name is not present, then requested_name, else an incremented filename.</li> </ul>"},{"location":"musicalgestures/_utils/#get_box_video_ratio","title":"get_box_video_ratio","text":"<p>[find in source code]</p> <pre><code>def get_box_video_ratio(filename, box_width=800, box_height=600):\n</code></pre> <p>Gets the box-to-video ratio between an arbitrarily defind box and the video dimensions. Useful to fit windows into a certain area.</p>"},{"location":"musicalgestures/_utils/#arguments_19","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>box_width</code> int, optional - The width of the box to fit the video into.</li> <li><code>box_height</code> int, optional - The height of the box to fit the video into.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_17","title":"Returns","text":"<ul> <li><code>int</code> - The smallest ratio (ie. the one to use for scaling the video window to fit into the box).</li> </ul>"},{"location":"musicalgestures/_utils/#get_first_frame_as_image","title":"get_first_frame_as_image","text":"<p>[find in source code]</p> <pre><code>def get_first_frame_as_image(\n    filename,\n    target_name=None,\n    pict_format='.png',\n    overwrite=False,\n):\n</code></pre> <p>Extracts the first frame of a video and saves it as an image using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_20","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>target_name</code> str, optional - The name for the output image. Defaults to None (which assumes that the input filename should be used).</li> <li><code>pict_format</code> str, optional - The format to use for the output image. Defaults to '.png'.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_18","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output image file.</li> </ul>"},{"location":"musicalgestures/_utils/#get_fps","title":"get_fps","text":"<p>[find in source code]</p> <pre><code>def get_fps(filename):\n</code></pre> <p>Gets the FPS (frames per second) value of a video using FFprobe.</p>"},{"location":"musicalgestures/_utils/#arguments_21","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file to measure.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_19","title":"Returns","text":"<ul> <li><code>float</code> - The FPS value of the input video file.</li> </ul>"},{"location":"musicalgestures/_utils/#get_frame_planecount","title":"get_frame_planecount","text":"<p>[find in source code]</p> <pre><code>def get_frame_planecount(frame):\n</code></pre> <p>Gets the planecount (color channel count) of a video frame.</p>"},{"location":"musicalgestures/_utils/#arguments_22","title":"Arguments","text":"<p>frame (numpy array): A frame extracted by <code>cv2.VideoCapture().read()</code>.</p>"},{"location":"musicalgestures/_utils/#returns_20","title":"Returns","text":"<ul> <li><code>int</code> - The planecount of the input frame, 3 or 1.</li> </ul>"},{"location":"musicalgestures/_utils/#get_framecount","title":"get_framecount","text":"<p>[find in source code]</p> <pre><code>def get_framecount(filename, fast=True):\n</code></pre> <p>Returns the number of frames in a video using FFprobe.</p>"},{"location":"musicalgestures/_utils/#arguments_23","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file to measure.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_21","title":"Returns","text":"<ul> <li><code>int</code> - The number of frames in the input video file.</li> </ul>"},{"location":"musicalgestures/_utils/#get_length","title":"get_length","text":"<p>[find in source code]</p> <pre><code>def get_length(filename: str) -&gt; float:\n</code></pre> <p>Gets the length (in seconds) of a video using FFprobe.</p>"},{"location":"musicalgestures/_utils/#arguments_24","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file to measure.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_22","title":"Returns","text":"<ul> <li><code>float</code> - The length of the input video file in seconds.</li> </ul>"},{"location":"musicalgestures/_utils/#get_widthheight","title":"get_widthheight","text":"<p>[find in source code]</p> <pre><code>def get_widthheight(filename: str) -&gt; tuple[int, int]:\n</code></pre> <p>Gets the width and height of a video using FFprobe.</p>"},{"location":"musicalgestures/_utils/#arguments_25","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file to measure.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_23","title":"Returns","text":"<ul> <li><code>int</code> - The width of the input video file.</li> <li><code>int</code> - The height of the input video file.</li> </ul>"},{"location":"musicalgestures/_utils/#has_audio","title":"has_audio","text":"<p>[find in source code]</p> <pre><code>def has_audio(filename):\n</code></pre> <p>Checks if video has audio track using FFprobe.</p>"},{"location":"musicalgestures/_utils/#arguments_26","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video file to check.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_24","title":"Returns","text":"<ul> <li><code>bool</code> - True if <code>filename</code> has an audio track, False otherwise.</li> </ul>"},{"location":"musicalgestures/_utils/#in_colab","title":"in_colab","text":"<p>[find in source code]</p> <pre><code>def in_colab():\n</code></pre> <p>Check's if the environment is a Google Colab document.</p>"},{"location":"musicalgestures/_utils/#returns_25","title":"Returns","text":"<ul> <li><code>bool</code> - True if the environment is a Colab document, otherwise False.</li> </ul>"},{"location":"musicalgestures/_utils/#in_ipynb","title":"in_ipynb","text":"<p>[find in source code]</p> <pre><code>def in_ipynb():\n</code></pre> <p>Check if the environment is a Jupyter notebook. Taken from https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook.</p>"},{"location":"musicalgestures/_utils/#returns_26","title":"Returns","text":"<ul> <li><code>bool</code> - True if the environment is a Jupyter notebook, otherwise False.</li> </ul>"},{"location":"musicalgestures/_utils/#merge_videos","title":"merge_videos","text":"<p>[find in source code]</p> <pre><code>def merge_videos(\n    media_paths: list,\n    target_name: str = None,\n    overwrite: bool = False,\n    print_cmd: bool = False,\n) -&gt; str:\n</code></pre> <p>Merges a list of video files into a single video file using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_27","title":"Arguments","text":"<ul> <li><code>media_paths</code> list - List of paths to the video files to merge.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename with the suffix \"_merged\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_27","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_utils/#motiongrams_ffmpeg","title":"motiongrams_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def motiongrams_ffmpeg(\n    filename,\n    color=True,\n    filtertype='regular',\n    threshold=0.05,\n    blur='none',\n    use_median=False,\n    kernel_size=5,\n    invert=False,\n    target_name_x=None,\n    target_name_y=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders horizontal and vertical motiongrams using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_28","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>color</code> bool, optional - If False the input is converted to grayscale at the start of the process. This can significantly reduce render time. Defaults to True.</li> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>kernel_size</code> int, optional - Size of the median filter (if <code>use_median=True</code>) or the erosion filter (if <code>filtertype='blob'</code>). Defaults to 5.</li> <li><code>invert</code> bool, optional - If True, inverts colors of the motiongrams. Defaults to False.</li> <li><code>target_name_x</code> str, optional - Target output name for the motiongram on the X axis. Defaults to None (which assumes that the input filename with the suffix \"_mgx_ffmpeg\" should be used).</li> <li><code>target_name_y</code> str, optional - Target output name for the motiongram on the Y axis. Defaults to None (which assumes that the input filename with the suffix \"_mgy_ffmpeg\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_28","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output horizontal motiongram (_mgx).</li> <li><code>str</code> - Path to the output vertical motiongram (_mgy).</li> </ul>"},{"location":"musicalgestures/_utils/#motionvideo_ffmpeg","title":"motionvideo_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def motionvideo_ffmpeg(\n    filename,\n    color=True,\n    filtertype='regular',\n    threshold=0.05,\n    blur='none',\n    use_median=False,\n    kernel_size=5,\n    invert=False,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders a motion video using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_29","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>color</code> bool, optional - If False the input is converted to grayscale at the start of the process. This can significantly reduce render time. Defaults to True.</li> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. Defaults to 'Regular'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>blur</code> str, optional - 'Average' to apply a 10px * 10px blurring filter, 'None' otherwise. Defaults to 'None'.</li> <li><code>use_median</code> bool, optional - If True the algorithm applies a median filter on the thresholded frame-difference stream. Defaults to False.</li> <li><code>kernel_size</code> int, optional - Size of the median filter (if <code>use_median=True</code>) or the erosion filter (if <code>filtertype='blob'</code>). Defaults to 5.</li> <li><code>invert</code> bool, optional - If True, inverts colors of the motion video. Defaults to False.</li> <li><code>target_name</code> str, optional - Defaults to None (which assumes that the input filename with the suffix \"_motion\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_29","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_utils/#pass_if_container_is","title":"pass_if_container_is","text":"<p>[find in source code]</p> <pre><code>def pass_if_container_is(container, file):\n</code></pre> <p>Checks if a file's extension matches a desired one. Passes if so, raises WrongContainer if not.</p>"},{"location":"musicalgestures/_utils/#arguments_30","title":"Arguments","text":"<ul> <li><code>container</code> str - The container to match.</li> <li><code>file</code> str - Path to the file to inspect.</li> </ul>"},{"location":"musicalgestures/_utils/#raises_1","title":"Raises","text":"<ul> <li><code>WrongContainer</code> - If the file extension (container) matches the desired one.</li> </ul>"},{"location":"musicalgestures/_utils/#pass_if_containers_match","title":"pass_if_containers_match","text":"<p>[find in source code]</p> <pre><code>def pass_if_containers_match(file_1, file_2):\n</code></pre> <p>Checks if file extensions match between two files. If they do it passes, is they don't it raises WrongContainer exception.</p>"},{"location":"musicalgestures/_utils/#arguments_31","title":"Arguments","text":"<ul> <li><code>file_1</code> str - First file in comparison.</li> <li><code>file_2</code> str - Second file in comparison.</li> </ul>"},{"location":"musicalgestures/_utils/#raises_2","title":"Raises","text":"<ul> <li><code>WrongContainer</code> - If file extensions (containers) mismatch.</li> </ul>"},{"location":"musicalgestures/_utils/#quality_metrics","title":"quality_metrics","text":"<p>[find in source code]</p> <pre><code>def quality_metrics(original, processed, metric=None):\n</code></pre> <p>Compute video quality metrics between two video files for comparing the quality of video codecs or measuring the efficacy of encoding configuration. Possible to compute three major video quality metrics used for objective evaluation, namely:</p> <ul> <li>PSNR: It is the most commonly used video quality metric. But it has the lowest predictive value, so the results are inconsistent.   Used by major platforms like Netflix and Facebook to compare different codecs and for similar use cases. Overall usage is declining.</li> <li>SSIM: Mostly used by technical experts like codec researchers and compression engineers.   Usage is declining steadily. However, it has a higher predictive value than PSNR.</li> <li>VMAF: Introduced first by Netflix but then converted into an open-source asset. VMAF is easily accessible and widely used.   Designed specifically for evaluating the video quality of streams encoded for multiple-resolution rungs.</li> </ul>"},{"location":"musicalgestures/_utils/#arguments_32","title":"Arguments","text":"<ul> <li><code>original</code> str - Path to the original/reference video file.</li> <li><code>processed</code> str - Path to the processed/distorted video file.</li> <li><code>metric</code> str, optional - Type of quality metric to compute ('vmaf', 'ssim', or 'psnr'). Defaults to None (which computes all the metrics).</li> </ul>"},{"location":"musicalgestures/_utils/#rotate_video","title":"rotate_video","text":"<p>[find in source code]</p> <pre><code>def rotate_video(filename, angle, target_name=None, overwrite=False):\n</code></pre> <p>Rotates a video by an <code>angle</code> using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_33","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>angle</code> float - The angle (in degrees) specifying the amount of rotation. Positive values rotate clockwise.</li> <li><code>target_name</code> str, optional - Target filename as path. Defaults to None (which assumes that the input filename with the suffix \"_rot\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_30","title":"Returns","text":"<ul> <li><code>str</code> - The path to the rotated video file.</li> </ul>"},{"location":"musicalgestures/_utils/#roundup","title":"roundup","text":"<p>[find in source code]</p> <pre><code>def roundup(num, modulo_num):\n</code></pre> <p>Rounds up a number to the next integer multiple of another.</p>"},{"location":"musicalgestures/_utils/#arguments_34","title":"Arguments","text":"<ul> <li><code>num</code> int - The number to round up.</li> <li><code>modulo_num</code> int - The number whose next integer multiple we want.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_31","title":"Returns","text":"<ul> <li><code>int</code> - The rounded-up number.</li> </ul>"},{"location":"musicalgestures/_utils/#scale_array","title":"scale_array","text":"<p>[find in source code]</p> <pre><code>def scale_array(array, out_low, out_high):\n</code></pre> <p>Scales an array linearly.</p>"},{"location":"musicalgestures/_utils/#arguments_35","title":"Arguments","text":"<ul> <li><code>array</code> arraylike - The array to be scaled.</li> <li><code>out_low</code> float - Minimum of output range.</li> <li><code>out_high</code> float - Maximum of output range.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_32","title":"Returns","text":"<ul> <li><code>arraylike</code> - The scaled array.</li> </ul>"},{"location":"musicalgestures/_utils/#scale_num","title":"scale_num","text":"<p>[find in source code]</p> <pre><code>def scale_num(val, in_low, in_high, out_low, out_high):\n</code></pre> <p>Scales a number linearly.</p>"},{"location":"musicalgestures/_utils/#arguments_36","title":"Arguments","text":"<ul> <li><code>val</code> float - The value to be scaled.</li> <li><code>in_low</code> float - Minimum of input range.</li> <li><code>in_high</code> float - Maximum of input range.</li> <li><code>out_low</code> float - Minimum of output range.</li> <li><code>out_high</code> float - Maximum of output range.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_33","title":"Returns","text":"<ul> <li><code>float</code> - The scaled number.</li> </ul>"},{"location":"musicalgestures/_utils/#str2sec","title":"str2sec","text":"<p>[find in source code]</p> <pre><code>def str2sec(time_string):\n</code></pre> <p>Converts a time code string into seconds.</p>"},{"location":"musicalgestures/_utils/#arguments_37","title":"Arguments","text":"<ul> <li><code>time_string</code> str - The time code to convert. Eg. '01:33:42'.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_34","title":"Returns","text":"<ul> <li><code>float</code> - The time code converted to seconds.</li> </ul>"},{"location":"musicalgestures/_utils/#threshold_ffmpeg","title":"threshold_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def threshold_ffmpeg(\n    filename,\n    threshold=0.1,\n    target_name=None,\n    binary=False,\n    overwrite=False,\n):\n</code></pre> <p>Renders a pixel-thresholded video from the input using ffmpeg.</p>"},{"location":"musicalgestures/_utils/#arguments_38","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>threshold</code> float, optional - The normalized pixel value to use as the threshold. Pixels below the threshold will turn black. Defaults to 0.1.</li> <li><code>target_name</code> str, optional - The name of the output video. Defaults to None (which assumes that the input filename with the suffix \"_thresh\" should be used).</li> <li><code>binary</code> bool, optional - If True, the pixels above the threshold will turn white. Defaults to False.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_35","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_utils/#unwrap_str","title":"unwrap_str","text":"<p>[find in source code]</p> <pre><code>def unwrap_str(string):\n</code></pre> <p>Unwraps a string from quotes.</p>"},{"location":"musicalgestures/_utils/#arguments_39","title":"Arguments","text":"<ul> <li><code>string</code> str - The string to inspect.</li> </ul>"},{"location":"musicalgestures/_utils/#returns_36","title":"Returns","text":"<ul> <li><code>str</code> - The (unwrapped) string.</li> </ul>"},{"location":"musicalgestures/_utils/#wrap_str","title":"wrap_str","text":"<p>[find in source code]</p> <pre><code>def wrap_str(string, matchers=[' ', '(', ')']):\n</code></pre> <p>Wraps a string in double quotes if it contains any of <code>matchers</code> - by default: space or parentheses. Useful when working with shell commands.</p>"},{"location":"musicalgestures/_utils/#arguments_40","title":"Arguments","text":"<ul> <li><code>string</code> str - The string to inspect.</li> <li><code>matchers</code> list, optional - The list of characters to look for in the string. Defaults to [\" \", \"(\", \")\"].</li> </ul>"},{"location":"musicalgestures/_utils/#returns_37","title":"Returns","text":"<ul> <li><code>str</code> - The (wrapped) string.</li> </ul>"},{"location":"musicalgestures/_video/","title":"Video","text":"<p>Auto-generated documentation for musicalgestures._video module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Video<ul> <li>MgVideo<ul> <li>MgVideo().average</li> <li>MgVideo().extract_frame</li> <li>MgVideo().from_numpy</li> <li>MgVideo().get_video</li> <li>MgVideo().numpy</li> <li>MgVideo().test_input</li> </ul> </li> </ul> </li> </ul>"},{"location":"musicalgestures/_video/#mgvideo","title":"MgVideo","text":"<p>[find in source code]</p> <pre><code>class MgVideo(MgAudio):\n    def __init__(\n        filename: str | list[str],\n        array=None,\n        fps=None,\n        path=None,\n        filtertype='Regular',\n        thresh=0.05,\n        starttime=0,\n        endtime=0,\n        blur='None',\n        skip=0,\n        frames=0,\n        rotate=0,\n        color=True,\n        contrast=0,\n        brightness=0,\n        crop='None',\n        keep_all=False,\n        returned_by_process=False,\n        sr=22050,\n        n_fft=2048,\n        hop_length=512,\n    ):\n</code></pre> <p>This is the class for working with video files in the Musical Gestures Toolbox. It inherites from the class MgAudio for working with audio files as well. There is a set of preprocessing tools you can use when you load a video, such as: - trimming: to extract a section of the video, - skipping: to shrink the video by skipping N frames after keeping one, - rotating: to rotate the video by N degrees, - applying brightness and contrast - cropping: to crop the video either automatically (by assessing the area of motion) or manually with a pop-up user interface, - converting to grayscale</p> <p>These preprocesses will apply upon creating the MgVideo. Further processes are available as class methods.</p>"},{"location":"musicalgestures/_video/#see-also","title":"See also","text":"<ul> <li>MgAudio</li> </ul>"},{"location":"musicalgestures/_video/#mgvideoaverage","title":"MgVideo().average","text":"<p>[find in source code]</p> <pre><code>def average(**kwargs):\n</code></pre> <p>Backward compatibility alias for blend(component_mode='average'). Creates an average image of all frames in the video.</p>"},{"location":"musicalgestures/_video/#arguments","title":"Arguments","text":"<ul> <li><code>**kwargs</code> - Additional arguments passed to blend method.          - <code>Note</code> - 'normalize' parameter is accepted for backward compatibility but ignored.</li> </ul>"},{"location":"musicalgestures/_video/#returns","title":"Returns","text":"<ul> <li><code>MgImage</code> - A new MgImage pointing to the output average image file.</li> </ul>"},{"location":"musicalgestures/_video/#mgvideoextract_frame","title":"MgVideo().extract_frame","text":"<p>[find in source code]</p> <pre><code>def extract_frame(**kwargs):\n</code></pre> <p>Extracts a frame from the video at a given time. see _utils.extract_frame for details.</p>"},{"location":"musicalgestures/_video/#arguments_1","title":"Arguments","text":"<ul> <li><code>frame</code> int - The frame number to extract.</li> <li><code>time</code> str - The time in HH:MM:ss.ms where to extract the frame from.</li> <li><code>target_name</code> str, optional - The name for the output file. If None, the name will be \\&lt;input name&gt;FRAME\\&lt;frame number&gt;.\\&lt;file extension&gt;. Defaults to None.</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_video/#returns_1","title":"Returns","text":"<ul> <li><code>MgImage</code> - An MgImage object referring to the extracted frame.</li> </ul>"},{"location":"musicalgestures/_video/#mgvideofrom_numpy","title":"MgVideo().from_numpy","text":"<p>[find in source code]</p> <pre><code>def from_numpy(array, fps, target_name=None):\n</code></pre>"},{"location":"musicalgestures/_video/#mgvideoget_video","title":"MgVideo().get_video","text":"<p>[find in source code]</p> <pre><code>def get_video():\n</code></pre> <p>Creates a video attribute to the Musical Gestures object with the given correct settings.</p>"},{"location":"musicalgestures/_video/#mgvideonumpy","title":"MgVideo().numpy","text":"<p>[find in source code]</p> <pre><code>def numpy():\n</code></pre> <p>Pipe all video frames from FFmpeg to numpy array</p>"},{"location":"musicalgestures/_video/#mgvideotest_input","title":"MgVideo().test_input","text":"<p>[find in source code]</p> <pre><code>def test_input():\n</code></pre> <p>Gives feedback to user if initialization from input went wrong.</p>"},{"location":"musicalgestures/_videoadjust/","title":"Videoadjust","text":"<p>Auto-generated documentation for musicalgestures._videoadjust module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Videoadjust<ul> <li>contrast_brightness_ffmpeg</li> <li>fixed_frames_ffmpeg</li> <li>skip_frames_ffmpeg</li> </ul> </li> </ul>"},{"location":"musicalgestures/_videoadjust/#contrast_brightness_ffmpeg","title":"contrast_brightness_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def contrast_brightness_ffmpeg(\n    filename,\n    contrast=0,\n    brightness=0,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Applies contrast and brightness adjustments on the source video using ffmpeg.</p>"},{"location":"musicalgestures/_videoadjust/#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video to process.</li> <li><code>contrast</code> int/float, optional - Increase or decrease contrast. Values range from -100 to 100. Defaults to 0.</li> <li><code>brightness</code> int/float, optional - Increase or decrease brightness. Values range from -100 to 100. Defaults to 0.</li> <li><code>target_name</code> str, optional - Defaults to None (which assumes that the input filename with the suffix \"_cb\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_videoadjust/#returns","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_videoadjust/#fixed_frames_ffmpeg","title":"fixed_frames_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def fixed_frames_ffmpeg(\n    filename,\n    frames=0,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Specify a fixed target number frames to extract from the video. To extract only keyframes from the video, set the parameter keyframes to True.</p>"},{"location":"musicalgestures/_videoadjust/#arguments_1","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video to process.</li> <li><code>frames</code> int), optional - Number frames to extract from the video. If set to -1, it will only extract the keyframes of the video. Defaults to 0.</li> <li><code>target_name</code> str, optional - Defaults to None (which assumes that the input filename with the suffix \"_fixed\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_videoadjust/#returns_1","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_videoadjust/#skip_frames_ffmpeg","title":"skip_frames_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def skip_frames_ffmpeg(filename, skip=0, target_name=None, overwrite=False):\n</code></pre> <p>Time-shrinks the video by skipping (discarding) every n frames determined by <code>skip</code>. To discard half of the frames (ie. double the speed of the video) use <code>skip=1</code>.</p>"},{"location":"musicalgestures/_videoadjust/#arguments_2","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the video to process.</li> <li><code>skip</code> int, optional - Discard <code>skip</code> frames before keeping one. Defaults to 0.</li> <li><code>target_name</code> str, optional - Defaults to None (which assumes that the input filename with the suffix \"_skip\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filename to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_videoadjust/#returns_2","title":"Returns","text":"<ul> <li><code>str</code> - Path to the output video.</li> </ul>"},{"location":"musicalgestures/_videograms/","title":"Videograms","text":"<p>Auto-generated documentation for musicalgestures._videograms module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Videograms<ul> <li>videograms_ffmpeg</li> </ul> </li> </ul>"},{"location":"musicalgestures/_videograms/#videograms_ffmpeg","title":"videograms_ffmpeg","text":"<p>[find in source code]</p> <pre><code>def videograms_ffmpeg(\n    self,\n    target_name_x=None,\n    target_name_y=None,\n    overwrite=False,\n):\n</code></pre> <p>Renders horizontal and vertical videograms of the source video using ffmpeg. Averages videoframes by axes, and creates two images of the horizontal-axis and vertical-axis stacks. In these stacks, a single row or column corresponds to a frame from the source video, and the index of the row or column corresponds to the index of the source frame.</p>"},{"location":"musicalgestures/_videograms/#arguments","title":"Arguments","text":"<ul> <li><code>target_name_x</code> str, optional - Target output name for the videogram on the X axis. Defaults to None (which assumes that the input filename with the suffix \"_vgx\" should be used).</li> <li><code>target_name_y</code> str, optional - Target output name for the videogram on the Y axis. Defaults to None (which assumes that the input filename with the suffix \"_vgy\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_videograms/#returns","title":"Returns","text":"<ul> <li><code>MgList</code> - An MgList with the MgImage objects referring to the horizontal and vertical videograms respectively.</li> </ul>"},{"location":"musicalgestures/_videoreader/","title":"Videoreader","text":"<p>Auto-generated documentation for musicalgestures._videoreader module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Videoreader<ul> <li>ReadError</li> <li>mg_videoreader</li> </ul> </li> </ul>"},{"location":"musicalgestures/_videoreader/#readerror","title":"ReadError","text":"<p>[find in source code]</p> <pre><code>class ReadError(Exception):\n</code></pre> <p>Base class for file read errors.</p>"},{"location":"musicalgestures/_videoreader/#mg_videoreader","title":"mg_videoreader","text":"<p>[find in source code]</p> <pre><code>def mg_videoreader(\n    filename,\n    starttime=0,\n    endtime=0,\n    skip=0,\n    frames=0,\n    rotate=0,\n    contrast=0,\n    brightness=0,\n    crop='None',\n    color=True,\n    keep_all=False,\n    returned_by_process=False,\n):\n</code></pre> <p>Reads in a video file, and optionally apply several different processes on it. These include: - trimming, - skipping, - fixing, - rotating, - applying brightness and contrast, - cropping, - converting to grayscale.</p>"},{"location":"musicalgestures/_videoreader/#arguments","title":"Arguments","text":"<ul> <li><code>filename</code> str - Path to the input video file.</li> <li><code>starttime</code> int/float, optional - Trims the video from this start time (s). Defaults to 0.</li> <li><code>endtime</code> int/float, optional - Trims the video until this end time (s). Defaults to 0 (which will make the algorithm use the full length of the input video instead).</li> <li><code>skip</code> int, optional - Time-shrinks the video by skipping (discarding) every n frames determined by <code>skip</code>. Defaults to 0.</li> <li><code>frames</code> int, optional - Specify a fixed target number of frames to extract from the video. Defaults to 0.</li> <li><code>rotate</code> int/float, optional - Rotates the video by a <code>rotate</code> degrees. Defaults to 0.</li> <li><code>contrast</code> int/float, optional - Applies +/- 100 contrast to video. Defaults to 0.</li> <li><code>brightness</code> int/float, optional - Applies +/- 100 brightness to video. Defaults to 0.</li> <li><code>crop</code> str, optional - If 'manual', opens a window displaying the first frame of the input video file, where the user can draw a rectangle to which cropping is applied. If 'auto' the cropping function attempts to determine the area of significant motion and applies the cropping to that area. Defaults to 'None'.</li> <li><code>color</code> bool, optional - If False, converts the video to grayscale and sets every method in grayscale mode. Defaults to True.</li> <li><code>keep_all</code> bool, optional - If True, preserves an output video file after each used preprocessing stage. Defaults to False.</li> <li><code>returned_by_process</code> bool, optional - This parameter is only for internal use, do not use it. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_videoreader/#returns","title":"Returns","text":"<ul> <li><code>int</code> - The number of frames in the output video file.</li> <li><code>int</code> - The pixel width of the output video file.</li> <li><code>int</code> - The pixel height of the output video file.</li> <li><code>int</code> - The FPS (frames per second) of the output video file.</li> <li><code>float</code> - The length of the output video file in seconds.</li> <li><code>str</code> - The path to the output video file without its extension. The file name gets a suffix for each used process.</li> <li><code>str</code> - The file extension of the output video file.</li> <li><code>bool</code> - Whether the video has an audio track.</li> </ul>"},{"location":"musicalgestures/_warp/","title":"Warp","text":"<p>Auto-generated documentation for musicalgestures._warp module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Warp<ul> <li>beats_diff</li> <li>mg_warp_audiovisual_beats</li> </ul> </li> </ul>"},{"location":"musicalgestures/_warp/#beats_diff","title":"beats_diff","text":"<p>[find in source code]</p> <pre><code>@jit(nopython=True)\ndef beats_diff(beats, media):\n</code></pre>"},{"location":"musicalgestures/_warp/#mg_warp_audiovisual_beats","title":"mg_warp_audiovisual_beats","text":"<p>[find in source code]</p> <pre><code>def mg_warp_audiovisual_beats(\n    self,\n    audio_file,\n    speed=(0.5, 2),\n    data=None,\n    filtertype='Adaptative',\n    thresh=0.05,\n    kernel_size=5,\n    target_name=None,\n    overwrite=False,\n):\n</code></pre> <p>Warp audio beats with visual beats (patterns of motion that can be shifted in time to control visual rhythm). Visual beats are warped after computing a directogram which factors the magnitude of motion in the video into different angles.</p> <p>Source: Abe Davis -- Visual Rhythm and Beat (section 5)</p>"},{"location":"musicalgestures/_warp/#arguments","title":"Arguments","text":"<ul> <li><code>audio_file</code> str - Path to the audio file.</li> <li><code>speed</code> tuple, optional - Speed's change between the audiovisual beats which can be adjusted to slow down or speed up the visual rhythms. Defaults to (0.5,2).</li> <li><code>data</code> array_like, optional - Computed directogram data can be added separately to avoid the directogram processing time (which can be quite long). Defaults to None.</li> <li><code>filtertype</code> str, optional - 'Regular' turns all values below <code>thresh</code> to 0. 'Binary' turns all values below <code>thresh</code> to 0, above <code>thresh</code> to 1. 'Blob' removes individual pixels with erosion method. 'Adaptative' perform adaptative threshold as the weighted sum of 11 neighborhood pixels where weights are a Gaussian window. Defaults to 'Adaptative'.</li> <li><code>thresh</code> float, optional - Eliminates pixel values less than given threshold. Ranges from 0 to 1. Defaults to 0.05.</li> <li><code>kernel_size</code> int, optional - Size of structuring element. Defaults to 5.</li> <li><code>target_name</code> str, optional - Target output name for the directogram. Defaults to None (which assumes that the input filename with the suffix \"_dg\" should be used).</li> <li><code>overwrite</code> bool, optional - Whether to allow overwriting existing files or to automatically increment target filenames to avoid overwriting. Defaults to False.</li> </ul>"},{"location":"musicalgestures/_warp/#returns","title":"Returns","text":"<ul> <li><code>MgVideo</code> - A MgVideo as warp_audiovisual_beats for parent MgVideo</li> </ul>"},{"location":"musicalgestures/examples/","title":"Examples","text":"<p>Auto-generated documentation for musicalgestures.examples module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Examples<ul> <li>Modules<ul> <li>Test Dance</li> <li>Test Pianist</li> </ul> </li> </ul> </li> </ul>"},{"location":"musicalgestures/examples/test_dance/","title":"Test Dance","text":"<p>Auto-generated documentation for musicalgestures.examples.test_dance module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Examples / Test Dance</li> </ul>"},{"location":"musicalgestures/examples/test_dance/#attributes","title":"Attributes","text":"<ul> <li><code>mg</code> - CREATE MODULE OBJECT: Here is an example call to create an MgVideo, using loads of parameters: <code>musicalgestures.MgVideo('../dance.avi', startti...</code></li> </ul>"},{"location":"musicalgestures/examples/test_pianist/","title":"Test Pianist","text":"<p>Auto-generated documentation for musicalgestures.examples.test_pianist module.</p> <ul> <li>Mgt-python / Modules / Musicalgestures / Examples / Test Pianist</li> </ul>"},{"location":"musicalgestures/examples/test_pianist/#attributes","title":"Attributes","text":"<ul> <li><code>mg</code> - CREATE MODULE OBJECT: Here is an example call to create an MgVideo, using loads of parameters: <code>musicalgestures.MgVideo('pianist.avi', color=False, crop='auto', skip=3)</code></li> </ul>"},{"location":"user-guide/core-classes/","title":"Core Classes","text":"<p>MGT-python is built around several core classes that provide the main functionality for musical gesture analysis. This guide covers the primary classes and their key methods.</p>"},{"location":"user-guide/core-classes/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>MgVideo (inherits from MgAudio)\n\u251c\u2500\u2500 MgAudio\n\u251c\u2500\u2500 Mg360Video (specialized video class)\n\u2514\u2500\u2500 Flow (optical flow analysis)\n\nUtility Classes:\n\u251c\u2500\u2500 MgFigure (plotting utilities)\n\u251c\u2500\u2500 MgImage (image handling)\n\u2514\u2500\u2500 MgList (list processing)\n</code></pre>"},{"location":"user-guide/core-classes/#mgvideo-class","title":"MgVideo Class","text":"<p>The <code>MgVideo</code> class is the primary interface for video analysis and inherits all audio functionality from <code>MgAudio</code>.</p>"},{"location":"user-guide/core-classes/#constructor","title":"Constructor","text":"<pre><code>MgVideo(\n    filename,           # Video file path or list of paths\n    array=None,         # Optional: numpy array input\n    fps=None,           # Override frame rate\n    path=None,          # Output path\n    # Preprocessing options\n    filtertype=\"Regular\", # Motion filter type\n    thresh=0.05,        # Motion detection threshold\n    starttime=0,        # Start time in seconds\n    endtime=0,          # End time in seconds (0 = full video)\n    blur=\"None\",        # Blur type: \"None\", \"Heavy\", \"Medium\"\n    skip=0,             # Frame skipping (0 = no skip)\n    frames=0,           # Limit number of frames\n    rotate=0,           # Rotation angle in degrees\n    color=True,         # Color (True) or grayscale (False)\n    contrast=0,         # Contrast adjustment (-1 to 1)\n    brightness=0,       # Brightness adjustment (-1 to 1)\n    # Cropping options\n    crop_movement=False, # Auto-crop to motion area\n    motion_box_thresh=0.1, # Motion detection threshold for cropping\n    motion_box_margin=1,   # Margin around motion box\n    # Manual cropping\n    cropx=None,         # Manual crop x coordinates [start, end]\n    cropy=None,         # Manual crop y coordinates [start, end]\n    # Output options\n    target_name=None,   # Custom output filename\n    overwrite=False,    # Overwrite existing files\n    outdir=None        # Output directory\n)\n</code></pre>"},{"location":"user-guide/core-classes/#key-properties","title":"Key Properties","text":"<pre><code>mv = mg.MgVideo('video.mp4')\n\n# Basic video properties\nprint(f\"Filename: {mv.filename}\")\nprint(f\"Width: {mv.width}\")\nprint(f\"Height: {mv.height}\")\nprint(f\"Length: {mv.length} seconds\")\nprint(f\"Frame rate: {mv.fps}\")\nprint(f\"Frame count: {mv.framecount}\")\nprint(f\"Output directory: {mv.outdir}\")\n\n# Check if color or grayscale\nprint(f\"Color video: {mv.color}\")\n\n# Access underlying OpenCV VideoCapture\nprint(f\"Video capture object: {mv.cap}\")\n</code></pre>"},{"location":"user-guide/core-classes/#motion-analysis-methods","title":"Motion Analysis Methods","text":""},{"location":"user-guide/core-classes/#basic-motion-analysis","title":"Basic Motion Analysis","text":"<pre><code># Complete motion analysis - most common method\nmotion_data = mv.motion(\n    filtertype='Regular',     # 'Regular', 'Binary', 'Blob'\n    thresh=0.05,             # Motion threshold (0-1)\n    blur='None',             # Blur: 'None', 'Heavy', 'Medium'\n    use_median=False,        # Use median filtering\n    kernel_size=5,           # Morphological kernel size\n    normalize=False,         # Normalize motion values\n    inverted_motiongram=False, # Invert motiongram colors\n    target_name=None,        # Custom output name\n    overwrite=False          # Overwrite existing files\n)\n\n# Returns dictionary with:\n# - 'motion_video': path to motion video\n# - 'motion_data': path to CSV data file\n</code></pre>"},{"location":"user-guide/core-classes/#optical-flow-analysis","title":"Optical Flow Analysis","text":"<pre><code># Dense optical flow\nflow_data = mv.flow(\n    type='Dense',            # 'Dense' or 'Sparse'\n    target_name=None,\n    overwrite=False\n)\n\n# Sparse optical flow with feature tracking\nflow_sparse = mv.flow(\n    type='Sparse',\n    corners_max=100,         # Maximum corners to track\n    quality_level=0.3,       # Corner detection quality\n    min_distance=7,          # Minimum distance between corners\n    block_size=7             # Size of averaging window\n)\n</code></pre>"},{"location":"user-guide/core-classes/#visualization-methods","title":"Visualization Methods","text":""},{"location":"user-guide/core-classes/#motiongrams","title":"Motiongrams","text":"<pre><code># Create horizontal and vertical motiongrams\nmotiongrams = mv.motiongrams(\n    filtertype='Regular',\n    thresh=0.05,\n    blur='None',\n    use_median=False,\n    normalize=False,\n    inverted_motiongram=False,\n    target_name_x=None,      # Custom name for horizontal\n    target_name_y=None,      # Custom name for vertical\n    overwrite=False\n)\n\n# Returns dictionary:\n# - 'mg_x': horizontal motiongram path\n# - 'mg_y': vertical motiongram path\n</code></pre>"},{"location":"user-guide/core-classes/#average-images","title":"Average Images","text":"<pre><code># Standard average\naverage_img = mv.average(\n    method='mean',           # 'mean', 'median'\n    target_name=None,\n    overwrite=False\n)\n\n# Median average (good for removing outliers)\nmedian_avg = mv.average(method='median')\n</code></pre>"},{"location":"user-guide/core-classes/#motion-history","title":"Motion History","text":"<pre><code># Create motion history visualization\nhistory = mv.history(\n    history_length=10,       # Number of frames in history\n    target_name=None,\n    overwrite=False,\n    normalize=False\n)\n</code></pre>"},{"location":"user-guide/core-classes/#videograms","title":"Videograms","text":"<pre><code># Extract pixel intensity over time\nvideograms = mv.videograms(\n    target_name_x=None,\n    target_name_y=None,\n    overwrite=False\n)\n\n# Returns paths to horizontal and vertical videograms\n</code></pre>"},{"location":"user-guide/core-classes/#centroid-and-feature-extraction","title":"Centroid and Feature Extraction","text":"<pre><code># Motion centroid tracking\ncentroid = mv.centroid(\n    filtertype='Regular',\n    thresh=0.05,\n    target_name=None,\n    overwrite=False\n)\n\n# Pose estimation (requires OpenPose)\npose_data = mv.pose(\n    pose_model='BODY_25',    # Pose model type\n    target_name=None,\n    overwrite=False\n)\n</code></pre>"},{"location":"user-guide/core-classes/#video-processing-methods","title":"Video Processing Methods","text":"<pre><code># Apply video effects and transformations\nblend_result = mv.blend(\n    blend_mode='difference',  # Blending mode\n    target_name=None,\n    overwrite=False\n)\n\n# Subtract background\nsubtract_result = mv.subtract(\n    method='mog2',           # Background subtraction method\n    target_name=None,\n    overwrite=False\n)\n\n# Create video grid\ngrid_result = mv.grid(\n    height=300,              # Grid cell height\n    rows=3,                  # Number of rows\n    cols=3,                  # Number of columns\n    padding=0,               # Padding between cells\n    margin=0,                # Margin around grid\n    target_name=None,\n    overwrite=False\n)\n</code></pre>"},{"location":"user-guide/core-classes/#mgaudio-class","title":"MgAudio Class","text":"<p>The <code>MgAudio</code> class handles all audio processing functionality and is inherited by <code>MgVideo</code>.</p>"},{"location":"user-guide/core-classes/#constructor_1","title":"Constructor","text":"<pre><code>MgAudio(\n    filename,               # Audio/video file path\n    array=None,            # Optional: numpy array input\n    sr=None,               # Sample rate override\n    offset=0.0,            # Start offset in seconds\n    duration=None,         # Duration to load in seconds\n    mono=True,             # Convert to mono\n    dtype=np.float32       # Data type for audio array\n)\n</code></pre>"},{"location":"user-guide/core-classes/#audio-analysis-methods","title":"Audio Analysis Methods","text":""},{"location":"user-guide/core-classes/#waveform-visualization","title":"Waveform Visualization","text":"<pre><code>audio = mg.MgAudio('audio.wav')\n\nwaveform = audio.waveform(\n    mono=True,              # Mono or stereo display\n    title='Waveform',       # Plot title\n    target_name=None,       # Output filename\n    overwrite=False,\n    dpi=300,               # Image resolution\n    autoshow=True          # Display plot automatically\n)\n</code></pre>"},{"location":"user-guide/core-classes/#spectrogram-analysis","title":"Spectrogram Analysis","text":"<pre><code>spectrogram = audio.spectrogram(\n    window_size=4096,       # FFT window size\n    overlap=0.5,           # Window overlap (0-1)\n    mel=True,              # Use mel scale\n    power=2.0,             # Power for amplitude\n    title='Spectrogram',\n    target_name=None,\n    overwrite=False,\n    autoshow=True\n)\n</code></pre>"},{"location":"user-guide/core-classes/#audio-descriptors","title":"Audio Descriptors","text":"<pre><code>descriptors = audio.descriptors(\n    window_size=1024,       # Analysis window size\n    hop_size=512,          # Hop size between windows\n    target_name=None,\n    overwrite=False\n)\n\n# Creates CSV file with:\n# - Spectral centroid\n# - Spectral rolloff  \n# - Spectral bandwidth\n# - Zero crossing rate\n# - MFCC coefficients\n# - Chroma features\n# - Tonnetz features\n</code></pre>"},{"location":"user-guide/core-classes/#tempo-and-beat-analysis","title":"Tempo and Beat Analysis","text":"<pre><code>tempogram = audio.tempogram(\n    window_size=384,        # Window size for tempo analysis\n    hop_size=192,          # Hop size\n    target_name=None,\n    overwrite=False,\n    autoshow=True\n)\n</code></pre>"},{"location":"user-guide/core-classes/#self-similarity-matrix","title":"Self-Similarity Matrix","text":"<pre><code>ssm = audio.ssm(\n    feature='mfcc',         # Feature type: 'mfcc', 'chroma', 'tonnetz'\n    distance_metric='cosine', # Distance metric\n    target_name=None,\n    overwrite=False,\n    autoshow=True\n)\n</code></pre>"},{"location":"user-guide/core-classes/#flow-class","title":"Flow Class","text":"<p>Specialized class for optical flow analysis.</p>"},{"location":"user-guide/core-classes/#constructor_2","title":"Constructor","text":"<pre><code>flow = Flow(\n    filename,               # Video file path\n    color=True,            # Color or grayscale\n    starttime=0,           # Start time\n    endtime=0,             # End time\n    target_name=None,      # Output name\n    overwrite=False\n)\n</code></pre>"},{"location":"user-guide/core-classes/#flow-analysis-methods","title":"Flow Analysis Methods","text":"<pre><code># Dense optical flow\ndense_flow = flow.dense(\n    save_plot=True,         # Save flow visualization\n    save_data=True,         # Save flow data\n    save_video=True         # Save flow video\n)\n\n# Sparse optical flow\nsparse_flow = flow.sparse(\n    corners_max=100,\n    quality_level=0.3,\n    min_distance=7,\n    block_size=7,\n    save_plot=True,\n    save_data=True,\n    save_video=True\n)\n</code></pre>"},{"location":"user-guide/core-classes/#utility-classes","title":"Utility Classes","text":""},{"location":"user-guide/core-classes/#mgfigure","title":"MgFigure","text":"<p>Handle plot creation and customization:</p> <pre><code>from musicalgestures._utils import MgFigure\n\nfig = MgFigure(\n    figure=plt.figure(),    # Matplotlib figure\n    figure_type='plot',     # Type of figure\n    data=data_array,        # Associated data\n    layers=[],             # Plot layers\n    title='My Plot'        # Figure title\n)\n</code></pre>"},{"location":"user-guide/core-classes/#mgimage","title":"MgImage","text":"<p>Handle image operations:</p> <pre><code>from musicalgestures._utils import MgImage\n\nimg = MgImage(\n    image=image_array,      # Image data\n    title='My Image',       # Image title\n    xlabel='X Label',       # X-axis label\n    ylabel='Y Label'        # Y-axis label\n)\n</code></pre>"},{"location":"user-guide/core-classes/#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"user-guide/core-classes/#complete-analysis-workflow","title":"Complete Analysis Workflow","text":"<pre><code>import musicalgestures as mg\n\n# Load video with preprocessing\nmv = mg.MgVideo(\n    'performance.mp4',\n    starttime=10,           # Skip intro\n    endtime=120,            # First 2 minutes\n    color=False,            # Grayscale for motion\n    filtertype='Regular',   # Motion filter\n    thresh=0.1             # Motion threshold\n)\n\n# Perform all analyses\nmotion = mv.motion()\nmotiongrams = mv.motiongrams()\naverage = mv.average()\nhistory = mv.history()\n\n# Audio analysis\nspectrogram = mv.audio.spectrogram()\ndescriptors = mv.audio.descriptors()\ntempogram = mv.audio.tempogram()\n\nprint(\"Analysis complete!\")\n</code></pre>"},{"location":"user-guide/core-classes/#error-handling","title":"Error Handling","text":"<pre><code>import musicalgestures as mg\n\ntry:\n    mv = mg.MgVideo('video.mp4')\n    motion = mv.motion()\n    print(\"Success!\")\n\nexcept FileNotFoundError:\n    print(\"Video file not found\")\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"user-guide/core-classes/#next-steps","title":"Next Steps","text":"<ul> <li>Video Processing Guide - Advanced video techniques</li> <li>Audio Analysis Guide - Detailed audio processing</li> <li>Motion Analysis Guide - Motion detection techniques</li> <li>API Reference - Complete method documentation</li> </ul>"}]}